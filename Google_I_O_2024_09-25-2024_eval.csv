user_query,context_docs,output
What was the total number of developers in attendance at Google I/O 2024?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The total number of developers in attendance at Google I/O 2024 was about 4,000."
What are some of the key features introduced in the Gemini models during the keynote?,"And this is not just the model completing the code. It's the model coming up with an entire plan and running it. And not only do I get an answer to my initial question about the data set, but the AI agent also produces a fully functional collab notebook based on your instructions. And you can share it with others, allowing for real time collaboration on code, new visualizations, and all of the results. It's available today at labs.google slash code and let us know what you think about it. Now you see why I'm so excited today. You see, we're committed to empowering you to build with genitive AI boldly and responsibly from Gemini power assistance in your favorite IDEs to easy access for our most powerful Gemini model. And it comes with flexibility to use Gemma open models. You have an opportunity to reach anyone anywhere through Google's ecosystem of devices, app experiences, and platform. We continued investing resources to give your projects a boost. Google accelerators and the Google developer portfolio is just a couple of great examples. Our accelerators have guided over 1,300 startups in 84 countries with countless success stories. And that includes 20 startups that have grown to become unicorns. You Genie AI founded by Dr. Sudeth Boyd, it's a recent graduate of our climate change accelerator. You Genie was born out of Dr. Childry's desire to use AI for good. His team helps manufacturing companies to dramatically reduce their carbon emissions. And you Genie's algorithms apply Google DeepMind research which has helped to optimize energy use in Google's own data centers. You Genie was able to streamline their model development and their analytics with accurate emissions data through Google Earth Engine. We love working alongside startups to tackle big problems and invite all of you to apply to an upcoming accelerator soon. If you have one of the 15 million Google developer profiles, you enjoy invitations for early access programs, a chance to participate in research and opportunities to attend special events. Beginning today, your profile is evolving into something even better. Introducing the Google Developer Program. Members of the program can now access new benefits at no cost and I'm going to take you through them. First, you can access Gemini for learning and searching and chatting with documentation. Second, for IDX users, we're now enabling you to go from two work stations up to five. Third, if you're in the Google Cloud Innovators community, you get credits for interactive labs on Google Cloud Skills Boost. Sign up for the Google Developer Program today or if you have an existing profile, you're all set. More benefits are coming soon so be on the lookout. That brings us to the close of today's developer keynote but we're not quite done. There's a lot more to Google I.O than just the keynotes. If you're with us in person, attend the sessions, the workshops, and demos both today and tomorrow. If you're online, you'll find all the sessions available on IO.Google starting this Thursday morning, Mountain View Time. Last year, we kicked off the series of IO connect events to bring the in-person experience from Shoreline to more places that are closer to you. Well, we heard just how much you loved them so we've got three of them lined up this year. Starting in Berlin on June 27th and then we're in Bangalore in July and Beijing in August. And I'll mention, I'll be at all of them so I hope to see you there too. If you can join us at IO connect, come to one of the many community-led IO extended event happening in your local area. Now, I like to share something really cool with you. We had Project Astra watch the keynote this morning. Yes. Let's see what it says. Hey, my name is Michael Chang and we're watching the Google IO keynote while it's happening with Project Astra. Let's take a look. Hey, Gemini, currently watching Google IO keynote right now. You want to watch it with me? Sure, I'd love to watch it with you. What's happening in the keynote? What does it look like? What does it look like they're announcing right now? It looks like they're announcing Gemini 1.5 Flash. The speaker is talking about its speed and efficiency in comparison to the pro model. What are the other features? It looks like Gemini 1.5 Flash is optimized for tasks where low latency and high efficiency matter most. Cool. The screen also shows. Oh, yeah, sorry, I interrupted you. Keep going.
We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.","During the keynote, several key features of the Gemini models were introduced:

1. **Gemini Power Assistance**: The models provide assistance in popular Integrated Development Environments (IDEs) and offer easy access to powerful Gemini models.

2. **Collaboration Tools**: The AI agent can generate fully functional collaborative notebooks based on user instructions, facilitating real-time collaboration on code and visualizations.

3. **Gemini App**: The app is designed to be a personal AI assistant, allowing users to interact using text, voice, or camera inputs. It features a new ""live"" conversation mode that enables users to have in-depth discussions with the AI.

4. **Customization with Gems**: Users can create personalized ""gems"" that act as experts on specific topics, allowing for tailored interactions with the AI.

5. **Trip Planning Experience**: The Gemini advanced model can assist in planning trips by gathering information from various sources (like Gmail and maps) and creating personalized itineraries based on user preferences and constraints.

6. **Dynamic User Interface**: The Gemini model presents information in a dynamic UI that adapts to user inputs, such as adjusting itineraries based on changes in preferences.

These features highlight Gemini's focus on enhancing productivity, creativity, and personalization in user interactions."
What unique feature does the music FX DJ tool provide according to Mark Rubier?,"Sabak Give me the hell out of here. Get me the hell out of this cup. Get me out. Yeah! Google! Google! Google! Google! Google! Ha ha ha! We all ready to do a little googling. Everyone get out your computers. It's time to have ourselves a nice Google. Siri Navigate to google.com. Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Are you awake? Have you had your coffee? Are you awake? Don't worry baby. Google's gonna wake you right up. Don't worry baby. Google! Google! Google! Google's gonna wake you right up. Bleep bloop, bloop, bloop, rappy bloop, bloop, bloop. Up time to get up. You silly little nerd's wake up. okay Google.com. What is it? What does it mean? Why are we here? No one knows. And you're not, you're not going to find out. Not today. Those answers? Not part of the presentation. Buckle up. I'm Mark Rabier. It's nice to meet you, nice to see you, nice to be here. In beautiful Honolulu Hawaii. Here for Google I.O. If you have no idea who I am, that's perfectly I would expect that. Not many people do, but I'm an improvisational musician. I make stuff up from scratch. And that's exactly what we're going to be doing this entire show. We have nothing planned. That includes the rest of the conference. None of this has been rehearsed. Sundar's going to come out, say some stuff. No, we don't know what's going to happen here today. They're making up these visuals right now. Someone's back there. So what I'm here to talk about is music. I'm here to talk to you about music. And in particular, a very exciting new technology that we've been developing here at Google.com. Music FX. DJ. That's right. No more do you need to haul around the case of records. No, the computer will take care of it. Music FX DJ is a generative artificial intelligence tool that will from scratch generate from nothing. A series of sounds as it interprets your prompts. I'll demonstrate that to you now. Utilizing technology at my fingertips. And utilizing the colossal power of Google's machine. Connecting to Google's machine now. Music. We'll start with a viola. How about that? Everyone loves a viola. And if you don't, I don't care. I don't care. You do now. Viola. No one wrote this. Google wrote this. Let's throw in something weird. Hip hop beat from the 808 drum machine. It's pretty tough actually. So we have the sort of melodic element of the viola still in there. We can pull that out. Chiptones. I think that's chiptones. Let's do it out. It's pretty tough. Pulling that out. The machine is in control now. And that's what it can do. And let me tell you something. Let me tell you something right now. That is just the beginning. You're going to be hearing that a lot at this keynote. That's just the beginning. Okay. Now you will choose the prompts and we'll leave it up to chance. As I said, everything here unscripted unplanned. I didn't know this was going to happen. What's this? I didn't even know you guys were going to be here. What are you guys doing here? Six props, bagpipes, Persian tar, jembe, dulcimer, flamenco guitar, Detroit techno. You know what those things are? You know what this is? You better. Shout them out. What do you want to hear? Flamenco guitar. Persian took bagpipes. No, no bagpipes. They said, no, get the bagpipes out of here. Okay. Persian tar, jembe, flamenco guitar. Jembe.
While we've made some great strides in developing AI systems that can understand multi-modal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called? That is the tweeter. It produces high frequency sounds. Give me a creative alliteration about these. Creative crayons color cheerfully. They certainly craft colorful creations. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector IV. That's right. What neighborhood do you think I'm in? This appears to be the King's Cross area of London. It is known for its railway station and transportation connections. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk and you're a red apple. What can I add here to make this system faster? Add in a cache between the server and database could improve speed. What does this remind you of? Trudginger's cat. All right, give me a band name for this duo. Golden Stripes. Nice. Thanks, Dominic. I think you'll agree it's amazing to see how far AI has come, especially when it comes to spatial understanding, video processing and memory. It's easy to envisage a future where you can have an expert assistant by your side through your phone or new exciting form factors like glasses. Some of these agent capabilities will come to Google products like Gemini app later this year. For those of you on site today, you can try out a live demo version of this experience in the AI sandbox area. Next, let's take a look at how our innovations are helping people bring new creative ideas to life. Today, we're introducing a series of updates across our genitive media tools with new models covering image, music and video. Over the past year, we've been enhancing quality, improving safety and increasing access to help tell this story. Here's Doug. Thanks, Demis. Over the past few months, we've been working hard to build a new image generation model from the ground up with stronger evaluations, extensive red teaming and state-of-the-art watermarking with synth ID. Today, I'm so excited to introduce Imagine 3. It's our most capable image generation model yet. Imagine 3 is more photorealistic. You can literally count the whiskers on its snout with richer details like this incredible sunlight in the shot and fewer visual artifacts or distorted images. It understands prompts written the way people write, the more creative and detailed you are the better and Imagine 3 remembers to incorporate small details like the wild flowers or small blue bird in this longer prompt. Plus, this is our best model yet for rendering text, which has been a challenge for image generation models. Inside-by-side comparisons, independent evaluators preferred Imagine 3 over other popular image generation models. In some, Imagine 3 is our highest quality image generation model so far. You can sign up today to try Imagine 3 in image FX, part of our suite of AI tools at labs.google, and it'll be coming soon to developers and enterprise customers in Vertex AI. Another area full of creative possibility is generative music. I've been working in this space for over 20 years, and this is by far the most exciting year of my career. We're exploring ways of working with artists to expand their creativity with AI. Together with YouTube, we've been building music AI sandbox, a suite of professional music AI tools that can create new instrumental sections from scratch, transfer styles between tracks, and more. To help us design and test them, we've been working closely with incredible musicians, songwriters, and producers. Some of them made even entirely new songs in ways that would have not been possible without these tools.","The Music FX DJ tool provides a unique feature of generative artificial intelligence that can create a series of sounds from scratch based on user prompts. This means that users no longer need to carry physical records, as the computer will generate the music dynamically in response to the prompts given."
What significant updates were announced for Android Studio during the developer keynote?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","During the developer keynote, significant updates for Android Studio were announced, including the ability for developers to leverage Gemini 1.5 Pro to improve and accelerate their development. One of the most exciting features highlighted was the capability to take a mock-up of a screen and generate the code for it in Android Studio, allowing developers to create an interactive app in real-time."
What was the purpose of the Googlers-only demo slam at Google I/O 2024?,"I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!
Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.","The purpose of the Googlers-only demo slam at Google I/O 2024 was to allow Googlers to present live demos across various labs, including Gemini, Search, Android, and more. Each demo had a limited time of four minutes before a humorous ""dinosaur"" would signal them to leave the stage. This event aimed to showcase the innovative projects and technologies developed by Googlers, fostering a sense of community and excitement among the attendees."
What are the key AI innovations mentioned in the presentation?,"This is the end of the video. Thank you. Listening to everything that has been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context, and agents are the cutting edge of what this technology can do, take it to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners, and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting-edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines, and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine three NVO create more realistic imagery and videos. We must also consider how they might be misused to spread misinformation. To help last year, we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today, we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Gemini, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making. And in the coming months, we'll be open sourcing Synth ID text watermarking. This will be available in our updated responsible Gerritive AI Toolkit, which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supposed to appear collaborating with Adobe, Microsoft, Startups, and many others to build that implement standards that improve the transparency of digital media. Now, let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today, our AI advances are helping to solve real-world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases, helping predict floods in more than 80 countries, and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube.
We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.","The key AI innovations mentioned in the presentation include:

1. **Multiple-Dality**: This refers to the ability of AI to process and integrate multiple forms of data or modalities, enhancing its capabilities.

2. **Long-Context**: This innovation allows AI models to understand and retain longer sequences of information, improving their performance in tasks that require context over extended interactions.

3. **AI-Assisted Red Teaming**: A technique that involves training AI agents to compete against each other to identify weaknesses in AI models, enhancing their robustness and security.

4. **Synth ID**: A tool that adds imperceptible watermarks to AI-generated images and audio, helping to identify and prevent misuse of generated content. This tool is being expanded to include text and video.

5. **LearnLam**: A new family of models based on Gemini, fine-tuned for educational purposes, aimed at making learning experiences more personal and engaging.

These innovations are part of Google's approach to building AI responsibly while maximizing its benefits for society."
What is the purpose of the AI-assisted red teaming technique?,"This is the end of the video. Thank you. Listening to everything that has been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context, and agents are the cutting edge of what this technology can do, take it to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners, and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting-edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines, and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine three NVO create more realistic imagery and videos. We must also consider how they might be misused to spread misinformation. To help last year, we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today, we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Gemini, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making. And in the coming months, we'll be open sourcing Synth ID text watermarking. This will be available in our updated responsible Gerritive AI Toolkit, which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supposed to appear collaborating with Adobe, Microsoft, Startups, and many others to build that implement standards that improve the transparency of digital media. Now, let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today, our AI advances are helping to solve real-world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases, helping predict floods in more than 80 countries, and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube.
But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and contacts to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives. Let me bring Tony up to show you what I mean. Hey, Tony. Hey, partner. Hey, everyone. Okay. So let me start by showing you how we set up this virtual teammate. As you can see, the teammate has this very own account. And we can go ahead and give it a name. We'll do something fun like Chip. Chip's been given a specific job role at the set of descriptions on how to be helpful for the team. You can see that here. And some of the jobs are to monitor and track projects. We've listed a few out to organize information and provide context and a few more things. Now that we've configured our virtual teammate, let's go ahead and see Chip in action. To do that, I'll switch this over here to Google Chat. First, when planning for an event like I.O., we have a ton of chat rooms for various purposes. Luckily for me, Chip is in all of them. To quickly catch up, I might ask a question like, anyone know if our I.O. storyboards are approved? Because we've instructed Chip to track this project, Chip searches across all the conversations and knows respond with an answer. There it is. Simple but very helpful. Now, as the team adds Chip to more group chats, more files, more email threads, Chip builds a collective memory of our work together. Let's look at an example to show you all switch over to a different room. How about a project sapphire over here? And here we are discussing a product release coming up. And as usual, many pieces are still in flight. So I can go ahead and ask, are we on track for launch? Chip gets to work not only searching through everything it has access to, but also synthesizing what's found and coming back with an up-to-date response. There it is. A clear timeline, a nice summary, and notice even in this first message here, Chip flags a potential issue the team should be aware of. Because we're in a group space, everyone can follow along, anyone can jump in at any time, as you see someone just did, asking Chip to help create a doc to help address the issue. A task like this could take me hours, dozens of hours, Chip can get all done in just a few minutes, sending the doc over right when it's ready. So much of this practical helpfulness comes from how we've customized Chip to our team's needs, and how seamlessly this AI is integrated directly into where we're already working back to you, Aparna. Thank you, Tony. Now, I can imagine a number of virtual types of, number of different types of virtual teammates configured by businesses to help them do what they need. Now we have a lot of work to do to figure out how to bring these agent of experiences like virtual teammates into workspace, including enabling third parties to make their very own versions of Chip. We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience.","The purpose of the AI-assisted red teaming technique is to improve AI models by testing them to identify weaknesses and vulnerabilities. This technique involves training AI agents to compete against each other, drawing on advancements from Google DeepMind's gaming breakthroughs, such as AlphaGo. By simulating adversarial scenarios, the AI-assisted red teaming aims to address adversarial prompting and limit problematic outputs, ultimately enhancing the safety and reliability of AI models."
"How many scientists are using AlphaFold, and in how many countries?","Just last week, we announced our next generation AlphaFull model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. And all of this was made possible with the best infrastructure for the AI era, including our highly optimized, tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multimodal because that's how we interact with and understand the world around us. We built a variety of models for different use cases. You've seen how powerful Gemini 1.5 Pro is. But we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash. Flash is a lightweight model compared to Pro. It's designed to be fast and cost efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context. Flash is optimized for tasks where low latency and efficiency matter most. Starting today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI. And developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. And you'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models, but we're always striving to push the state of the art even further. At any one time, we have many different models in training, and we use our very large and powerful ones to help teach and train our production ready models. Together with user feedback, this cutting edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multimodal AI. And this would how this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multimodal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action. And it would have to be proactive, teachable, and personal, so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called? That is the tweeter. It produces high frequency sounds. Give me a creative alliteration about these. Creative crayons color cheerfully. They certainly craft colorful creations. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector, IV. That's right. What neighborhood do you think I'm in? This appears to be the King's Cross area of London. It is known for its railway station and transportation connections. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. What can I add here to make this system faster? Adding a cache between the server and database could improve speed. What does this remind you of? I think you'll agree. It's amazing to see how far AI has come.
Just last week, we announced our next generation AlphaFull model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. And all of this was made possible with the best infrastructure for the AI era, including our highly optimized, tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multimodal because that's how we interact with and understand the world around us. We built a variety of models for different use cases. You've seen how powerful Gemini 1.5 Pro is. But we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash. Flash is a lightweight model compared to Pro. It's designed to be fast and cost efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context. Flash is optimized for tasks where low latency and efficiency matter most. Starting today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI. And developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. And you'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models, but we're always striving to push the state of the art even further. At any one time, we have many different models in training, and we use our very large and powerful ones to help teach and train our production ready models. Together with user feedback, this cutting edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multimodal AI. And this would how this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multimodal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action. And it would have to be proactive, teachable, and personal, so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called? That is the tweeter. It produces high frequency sounds. Give me a creative alliteration about these. Creative crayons color cheerfully. They certainly craft colorful creations. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector, IV. That's right. What neighborhood do you think I'm in? This appears to be the King's Cross area of London. It is known for its railway station and transportation connections. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. What can I add here to make this system faster? Adding a cache between the server and database could improve speed. What does this remind you of? I think you'll agree. It's amazing to see how far AI has come.","The provided context does not contain specific information regarding the number of scientists using AlphaFold or the number of countries involved. Therefore, I cannot answer that question based on the given text."
"What is LearnLam, and how is it intended to be used?","This is particularly important as technical advances like better intonation, make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine 3NVO create more realistic imagery and videos. We must also consider how there might be misuse to spread misinformation. To help last year we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Jemeni, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making, and in the coming months we'll be open sourcing Synth ID text-watermarking. This will be available in our updated responsible, generative AI toolkit which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supporting C2PA collaborating with Adobe, Microsoft, startups, and many others to build that implement standards that improve the transparency of digital media. Now let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today our AI advances are helping to solve real-world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases. Helping predict floods in more than 80 countries and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now generative AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day and generative AI is opening up new possibilities allowing us to ask questions like, what if everyone everywhere could have their own personal AI tutor on any topic or what if every educator could have the role assistant in the classroom. Today marks a new chapter for learning and education at Google. I'm excited to introduce LearnLam. Our new family of models based on Gemini and fine tuned for learning. LearnLam is grounded in educational research making learning experiences more personal and engaging. And it's coming to the products you use every day like search, Android, Gemini and YouTube. In fact, you've already seen LearnLam on stage today when it helps to me with his son's homework on Android. Now let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems which will be available in the Gemini app and web experience including one called Learning Coach. With Learning Coach you can get step-by-step study guidance along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features like Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering with experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use generative AI. We're also working directly with educators to build more helpful generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning. And enable teachers to tailor lessons and content to meet the individual needs of their students.
And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube. In fact, you've already seen LearnLam on stage today, when it helps to meer with his son's homework on Android. Now, let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems, which will be available in the Gemini app and web experience, including one called Learning Coach. With Learning Coach, you can get step-by-step study guidance, along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features that Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy, to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use Generative AI. We're also working directly with educators to build more helpful Generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning, and enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real, how much it is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors, and how much more is still to come. This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar.","LearnLam is a new family of models developed by Google, based on the Gemini architecture and fine-tuned specifically for educational purposes. It aims to make learning experiences more personal and engaging by leveraging educational research. LearnLam is designed to be integrated into various Google products, such as search, Android, Gemini, and YouTube.

The intended use of LearnLam includes:

1. **Personal AI Tutors**: It allows users to have their own personal AI tutor on any topic, enhancing the learning experience.
2. **Learning Coach**: One of the features is called Learning Coach, which provides step-by-step study guidance and helpful memory techniques to build understanding rather than just providing answers.
3. **Interactive Educational Videos**: In YouTube, LearnLam is used to make educational videos more interactive, enabling users to ask questions, receive explanations, or take quizzes during the video.
4. **Collaboration with Educators**: Google is partnering with educational institutions to test and improve the capabilities of LearnLam, as well as developing tools for Google Classroom to simplify lesson planning and tailor content to meet individual student needs.

Overall, LearnLam is positioned as a tool to enhance learning and education through personalized and interactive experiences."
What collaborations are mentioned in relation to the development of Generative AI tools for education?,"What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.
This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering with experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use generative AI. We're also working directly with educators to build more helpful generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning. And enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real. How much is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors. And how much more is still to come? This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar. Thanks, James. All of this shows the important progress we've made as we take a bold and responsible approach to making AI helpful for everyone. Before we wrap, I have a feeling that someone out there might be counting how many times we have mentioned AI today. And since a big team today has been letting Google do the work for you, we went ahead and counted so that you don't have to. That might be a record in how many times someone has said AI. I'm tempted to say it a few more times, but I won't. Anyhow, this tally is more than just a punchline. It reflects something much deeper. We've been AI first in our approach for a long time. Our decades of research leadership have pioneered many of the modern breakthroughs that power AI progress for us and for the industry. On top of that, we have world-leading infrastructure built for the AI era. Cutting edge innovation in search now powered by Gemini, products that help with extraordinary scale, including 15 products with over half a billion users, and platforms that enable everyone, partners, customers, creators, and all of you to invent the future. This progress is only possible because of our incredible developer community. You're making it real through the experiences you build every day. So to everyone here in Showline and the millions more watching around the world, here's to the possibilities ahead and creating them together. Thank you. Hello, me to re-educe myself. My name is... What does this remind you of? Soap in your head. Wow! When all of these tools come together, it's a powerful combination. It's amazing. It's amazing. It's an entire suite of different kinds of possibilities. Hi, I'm Gemini. What neighborhood do you think I'm in? This appears to be the campus cross-area of London. Together we're creating a new era. Ah...","The collaborations mentioned in relation to the development of Generative AI tools for education include partnerships with experts and institutions such as Columbia Teachers College, Arizona State University, and Khan Academy. Additionally, there is a collaboration with MIT Rays to develop an online course aimed at helping educators better understand and use generative AI."
"What are the two models introduced in the Gemini 1.5 series, and what are their primary use cases?","Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response. And it's OK if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to aisstudio.google.com, log in with your Google account, and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit run. Now, in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, it starts streaming it back. Now, this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from Notebook LM, like we showed earlier. Now, what's great about this is that you can take something like this in AI Studio, prototype tier in 10 seconds. And with one click in the upper left, get an API key, or over here in the upper right, just tap get code. And you've got all of the model configurations, the safety settings, ready to go, straight into your IDE. Now, over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way, you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. OK. OK. APPLAUSE Now, let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now, since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. APPLAUSE It's optimized for a range of image captioning, visual Q&A, and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model, but it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on NextGen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. APPLAUSE So wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look.","The two models introduced in the Gemini 1.5 series are **1.5 Pro** and **1.5 Flash**. 

- **1.5 Pro** is used for complex tasks where the highest quality response is desired, even if it takes a bit longer to return results.
- **1.5 Flash** is designed for quick tasks where speed is the most important factor. 

Developers can try both models to determine which one works best for their specific use cases."
What is the pricing structure for the Gemini 1.5 Pro and Flash models based on token usage?,"Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response. And it's OK if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to aisstudio.google.com, log in with your Google account, and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit run. Now, in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, it starts streaming it back. Now, this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from Notebook LM, like we showed earlier. Now, what's great about this is that you can take something like this in AI Studio, prototype tier in 10 seconds. And with one click in the upper left, get an API key, or over here in the upper right, just tap get code. And you've got all of the model configurations, the safety settings, ready to go, straight into your IDE. Now, over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way, you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. OK. OK. APPLAUSE Now, let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now, since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. APPLAUSE It's optimized for a range of image captioning, visual Q&A, and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model, but it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on NextGen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. APPLAUSE So wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look.","The pricing structure for the Gemini 1.5 Pro and Flash models based on token usage is as follows:

- **Gemini 1.5 Pro**: 
  - $7 per 1 million tokens.
  - For prompts up to 128K tokens, the price is reduced to $3.50 (50% less).

- **Gemini 1.5 Flash**: 
  - Starts at 35 cents per 1 million tokens."
"What new developer features are being added to the Gemini API, and when will they be available?","We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. Over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now Gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now based on my flight information, Gemini knows that I needed two and a half day, itinerary. You can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens.","The new developer features being added to the Gemini API include the ability to create ""gems,"" which are customizable personal experts that users can set up for specific tasks or interactions. These gems allow users to write instructions once and return to them whenever needed, making it easier to interact with Gemini for particular purposes.

These features will be rolling out in the coming months, although a specific date is not mentioned in the provided context. Additionally, the new trip planning experience in Gemini Advanced, which utilizes reasoning and intelligence to assist with planning, will be available this summer, just in time for Labor Day weekend."
"What is the significance of the Gemma model family, and what are its available sizes?","Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now since it launched less than three months ago it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, Polly Gemma, our first vision language open model and it's available right now. It's optimized for a range of image captioning visual Q&A and other image labeling tasks so go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model but it's still got a fitness size that's easy for all of you to use. So in a few weeks we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on next-gen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. To wrap up I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. Languages are a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navarasa. Navarasa is a model that's trained for Indic languages. It's a fine tuned model based on Google's Gemma. We built Navarasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. Today the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of generative AI that we want to bring to every corner of India and the entire world.
Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response. And it's OK if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to aisstudio.google.com, log in with your Google account, and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit run. Now, in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, it starts streaming it back. Now, this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from Notebook LM, like we showed earlier. Now, what's great about this is that you can take something like this in AI Studio, prototype tier in 10 seconds. And with one click in the upper left, get an API key, or over here in the upper right, just tap get code. And you've got all of the model configurations, the safety settings, ready to go, straight into your IDE. Now, over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way, you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. OK. OK. APPLAUSE Now, let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now, since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. APPLAUSE It's optimized for a range of image captioning, visual Q&A, and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model, but it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on NextGen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. APPLAUSE So wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look.","The Gemma model family is significant for driving AI innovation and responsibility. It is built from the same research and technology as the Gemini models and offers top performance. The Gemma models come in lightweight sizes of 7 billion (7B) and 2 billion (2B) parameters. Since its launch, Gemma has been downloaded millions of times across major model hubs, and developers and researchers have been customizing the base model and using pre-trained variants. The introduction of Gemma models aims to enhance accessibility and usability in various applications, particularly in the context of language processing and AI development."
"What is the purpose of the Navarasa project, and how does it relate to the Gemma model?","The language is a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navrasa. Navrasa is a model that's trained for Indic languages. It's a fine tune model based on Google's Gemma. We built Navrasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. So we should use AI to make sure that no one is following and no one is using it. Today, the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of Generative AI that we want to bring to every corner of India and the entire world.
Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now since it launched less than three months ago it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, Polly Gemma, our first vision language open model and it's available right now. It's optimized for a range of image captioning visual Q&A and other image labeling tasks so go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model but it's still got a fitness size that's easy for all of you to use. So in a few weeks we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on next-gen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. To wrap up I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. Languages are a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navarasa. Navarasa is a model that's trained for Indic languages. It's a fine tuned model based on Google's Gemma. We built Navarasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. Today the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of generative AI that we want to bring to every corner of India and the entire world.","The purpose of the Navarasa project is to create a set of instruction-tuned models that expand access to 15 Indic languages, making large language models culturally rooted. This allows people to communicate in their native languages and receive responses in the same language. Navarasa is a fine-tuned model based on Google's Gemma, which features an incredibly powerful tokenizer capable of handling a vast vocabulary across multiple alphabets and language systems. This tokenizer is critical for adapting Gemma to power projects like Navarasa, ensuring that the technology can understand and address the linguistic nuances present in a diverse country like India. The overarching goal is to leverage generative AI to solve real-world problems for people across all corners of India."
What are the three breakthroughs in Android that were announced at Google I/O 2024?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
You'll see these and work coming to hundreds of billions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, on-device foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel, later this year, we'll be expanding what's possible with our latest model Gemini Nano with multi-modality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed Talkback an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses Talkback will typically come across 90 unlabeled images per day. Thankfully, Talkback makes them accessible. And now we're taking that to the next level with the multi-modal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the styling cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to Talkback are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scandal continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello. I am calling from the Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh yeah, what kind of charges? I can't give you specifics over the phone, but to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. And look at this. My phone gives me a warning that this call might be a stab. Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone, thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools like Gemini Nano and Gemini in Android Studio. Also, stay tuned tomorrow for our upcoming Android 15 updates, which we can't wait to share with you. As we said at the outset, we're reimagining Android with Gemini at the core from your favorite apps to the OS itself. We're bringing the power of AI to every aspect of the smartphone experience. And with that, let me head over to Josh to share more on our use for developers. Thank you.","The three breakthroughs in Android announced at Google I/O 2024 include:

1. **On-Device AI with Gemini Nano**: Android is the first mobile operating system to integrate a built-in, on-device foundation model, allowing for faster experiences while protecting user privacy. This feature enhances the smartphone experience by enabling the phone to understand the world through various inputs, including text, sounds, and spoken language.

2. **Enhanced Accessibility Features**: The improvements to the Talkback feature, which assists users with blindness or low vision, were highlighted. With the multi-modal capabilities of Gemini Nano, users will receive richer and clearer descriptions of images, making it easier to navigate and interact with their devices.

3. **Fraud Protection**: Android introduced features that help protect users from fraud. For instance, the system can detect suspicious activity, such as scam calls, and alert users in real-time, ensuring that sensitive information remains private and secure on the device. 

These advancements aim to elevate the overall smartphone experience while prioritizing user privacy and accessibility."
"How many devices currently support the Circle to Search feature, and what is the goal for the end of the year?","Is that how it works? Am I doing AI? Yeah, it just does whatever you type. What a last minute gift ideas you can make with arts and crafts. Plan a workout routine to get bigger calves. Help me think of titles my tell all men war. What's something smart I can say about Renoir? Generate another image of a cat playing guitar. If a girl calls me a snack, how do I reply? Yeah, that's how it works. You do AI. Make this email sound more professional before it hits. What's a good excuse to cancel dinner with my friends? We're literally sitting right here. No wrong way to prompt. Hey, if you begin your own way to prompt, just every time you're prompting a prompt bar. Or just generate an image of a cat playing guitar. You know, it can do other stuff, right? Hi, everyone. It's great to be back at Google I.O. Today, you've seen how AI is transforming our products across Gemini, search, workspace, and more. We're bringing all these innovations right onto your Android phone. And we're going even further to make Android the best place to experience Google AI. This new era of AI is a profound opportunity to make smart phones truly smart. Our phones have come a long way in a short time. But if you think about it, it's been years since the user experience has fundamentally transformed. This is a once in a generation moment to reinvent what phones can do. So we've embarked on a multi-year journey to reimagine Android with AI at the core. And it starts with three breakthroughs you'll see this year. First, we're putting AI-powered search right at your fingertips, creating entirely new ways to get the answers you need. Second, Gemini is becoming your new AI assistant on Android. There to help you anytime. And third, we're harnessing on device AI to unlock new experiences that work as fast as you do while keeping your sensitive data private. Let's start with AI-powered search. Earlier this year, we took an important first step at Samsung unpacked by introducing Circle to Search. It brings the best of search directly into the user experience. So you can go deeper on anything you see on your phone without switching apps. Fashionistas are finding the perfect shoes. Home chefs are discovering new ingredients. And with our latest update, it's never been easier to translate whatever's on your screen, like a social post in another language. And there are even more ways Circle to Search can help. One thing we've heard from students is that they're doing more of their school work directly on their phones and tablets. So we thought, could Circle to Search be your perfect study buddy? Let's say my son needs help with a tricky physics word problem, like this one. My first thought is, oh boy, it's been a while since I've thought about kinematics. If he stumped on this question, instead of putting me on the spot, he can circle the exact part he's stuck on and get step by step instructions. Right where he's already doing to work. Ah, of course, final velocity equals initial velocity plus acceleration times elapsed time. Right. I was just about to say that. Seriously though, I love how it shows how to solve the problem, not just the answer. This new capability is available today. And later this year, Circle to Search will be able to tackle more complex problems involving symbolic formulas, diagrams, graphs, and more. Circle to Search is only on Android. It's available on more than 100 million devices today, and we're on track to double that by the end of the year. You've already heard from Sissy about the incredible updates coming to the Gemini app. On Android, Gemini is so much more. It's becoming a foundational part of the Android experience. Here's Dave to share more. Hey everyone. A couple of months ago, we launched Gemini on Android. And like Circle to Search, Gemini works at the system level. So instead of going to a separate app, I can bring Gemini right to what I'm doing. Now, we're making Gemini context aware. So it can then anticipate what you're trying to do and provide more helpful suggestions in the moment. In other words, to be a more helpful assistant. So let me show you how this works. And I have my shiny new Pixel 8A here to help me.
Is that how it works? Am I doing AI? Yeah, it just does whatever you type. What a last minute gift ideas you can make with arts and crafts. Plan a workout routine to get bigger calves. Help me think of titles my tell all men war. What's something smart I can say about Renoir? Generate another image of a cat playing guitar. If a girl calls me a snack, how do I reply? Yeah, that's how it works. You're doing AI. Make this email sound more professional before it hits. What's a good excuse to cancel dinner with my friends? We're literally sitting right here. No wrong way to prompt. Hey, if you begin, no wrong way to prompt. Just every time you're prompting a prompt bar. We're just generating an image of a cat playing guitar. You know I can do other stuff, right? Hi, everyone. It's great to be back at Google I.O. Today, you've seen how AI is transforming our products across Gemini, Search, Workspace, and more. We're bringing all these innovations right onto your Android phone. And we're going even further to make Android the best place to experience Google AI. This new era of AI is a profound opportunity to make smart phones truly smart. Our phones have come a long way in a short time. But if you think about it, it's been years since the user experience has fundamentally transformed. This is a once in a generation moment to reinvent what phones can do. So we've embarked on a multi-year journey to reimagine Android with AI at the core. And it starts with three breakthroughs you'll see this year. First, we're putting AI-powered search right at your fingertips, creating entirely new ways to get the answers you need. Second, Gemini is becoming your new AI assistant on Android. There to help you anytime. And third, we're harnessing on device AI to unlock new experiences that work as fast as you do while keeping your sensitive data private. Let's start with AI-powered search. Earlier this year, we took an important first step at Samsung Unpacked by introducing Circle to Search. It brings the best of search directly into the user experience. So you can go deeper on anything you see on your phone without switching apps. Fashionistas are finding the perfect shoes. Home chefs are discovering new ingredients and with our latest update, it's never been easier to translate whatever's on your screen, like a social post in another language. And there are even more ways Circle to Search can help. One thing we've heard from students is that they're doing more of their schoolwork directly on their phones and tablets. So we thought, could Circle to Search be your perfect study buddy? Let's say my son needs help with a tricky physics word problem, like this one. My first thought is, oh boy, it's been a while since I've thought about kinematics. If he stumped on this question, instead of putting me on the spot, he can circle the exact part he's stuck on and get step by step instructions, right, where he's already doing to work. Of course, final velocity equals initial velocity, plus acceleration times elapsed time. Right. I was just about to say that. Seriously though, I love how it shows how to solve the problem, not just the answer. This new capability is available today. And later this year, Circle to Search will be able to tackle more complex problems involving symbolic formulas, diagrams, graphs, and more. Circle to Search is only on Android. It's available on more than 100 million devices today, and we're on track to double that by the end of the year. You've already heard from Sissy about the incredible updates coming to the Gemini app. On Android, Gemini is so much more. It's becoming a foundational part of the Android experience. Here's Dave to share more. Hi, everyone. A couple of months ago, we launched Gemini on Android. And like Circle to Search, Gemini works at the system level. So instead of going to a separate app, I can bring Gemini right to what I'm doing. Now, we're making Gemini context aware. So it can then dissipate what you're trying to do and provide more helpful suggestions in the moment. In other words, to be a more helpful assistant. So let me show you how this works. And I have my shiny new Pixel 8A here to help me.","Currently, the Circle to Search feature is available on more than 100 million devices. The goal is to double that number by the end of the year."
What is the significance of Gemini Nano in the context of Android's AI capabilities?,"You'll see these and work coming to hundreds of billions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, on-device foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel, later this year, we'll be expanding what's possible with our latest model Gemini Nano with multi-modality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed Talkback an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses Talkback will typically come across 90 unlabeled images per day. Thankfully, Talkback makes them accessible. And now we're taking that to the next level with the multi-modal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the styling cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to Talkback are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scandal continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello. I am calling from the Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh yeah, what kind of charges? I can't give you specifics over the phone, but to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. And look at this. My phone gives me a warning that this call might be a stab. Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone, thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools like Gemini Nano and Gemini in Android Studio. Also, stay tuned tomorrow for our upcoming Android 15 updates, which we can't wait to share with you. As we said at the outset, we're reimagining Android with Gemini at the core from your favorite apps to the OS itself. We're bringing the power of AI to every aspect of the smartphone experience. And with that, let me head over to Josh to share more on our use for developers. Thank you.
And let's hit that. Because I've heard that rule may be changing. Now, because I'm a Gemini advanced user, this works on any PDF and takes full advantage of the long context window. And there's just lots of times when that's useful. For example, let's say you're looking for a quick answer in an appliance user manual. And there you have it. It turns out, no, spin serves are not allowed. So Gemini not only gives me a clear answer to my question, it also shows me exactly where on the PDF to learn more. Awesome. OK. So that's a few of the ways that we're enhancing Gemini to do more context aware and helpful in the moment. And what you've seen here are the first really many new ways that Gemini will unlock new experiences at the system level. And they're only available on Android. You'll see these and work coming to hundreds of millions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, undivided foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel later this year, we'll be expanding what's possible with our latest model, Gemini Nano with multimodality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed TalkBack, an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses TalkBack, will typically come across 90 unlabeled images per day. Thankfully, TalkBack makes them accessible. And now we're taking that to the next level, with the multimodal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the style and cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to TalkBack are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scan continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello? Hi, I'm calling from Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh, yeah. What kind of charges? I can't give you specifics over the phone. But to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. LAUGHTER And look at this. My phone gives me a warning that this call might be a scam. APPLAUSE Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools, like Gemini Nano and Gemini in Android Studio.","Gemini Nano is significant in the context of Android's AI capabilities as it represents a major advancement in integrating AI directly into the operating system, enhancing the overall smartphone experience. Here are some key points highlighting its importance:

1. **On-Device AI**: Gemini Nano allows for AI processing to occur directly on the device, which improves speed and responsiveness while ensuring user privacy since data does not need to be sent to external servers.

2. **Multimodal Understanding**: The model enables smartphones to understand and interpret information not just through text, but also through images, sounds, and spoken language. This multimodal capability allows for a more intuitive interaction with the device.

3. **Accessibility Enhancements**: For users with disabilities, such as those who are blind or have low vision, Gemini Nano enhances accessibility features like Talkback, providing richer descriptions of images and improving navigation through spoken feedback.

4. **Fraud Detection**: The AI can help protect users from scams by analyzing incoming calls and detecting suspicious activity, alerting users in real-time, which enhances security.

5. **Contextual Awareness**: Gemini Nano can understand the context of conversations and provide relevant suggestions, making interactions with the device more seamless and helpful.

6. **Developer Empowerment**: The introduction of Gemini Nano also empowers developers by providing them with new AI tools and models, encouraging innovation and the development of applications that leverage these advanced AI capabilities.

Overall, Gemini Nano is positioned as a transformative element in Android, aiming to elevate user experience through faster, more intelligent, and privacy-conscious AI functionalities."
What improvements to the Talkback feature are expected with the introduction of Gemini Nano?,"You'll see these and work coming to hundreds of billions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, on-device foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel, later this year, we'll be expanding what's possible with our latest model Gemini Nano with multi-modality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed Talkback an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses Talkback will typically come across 90 unlabeled images per day. Thankfully, Talkback makes them accessible. And now we're taking that to the next level with the multi-modal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the styling cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to Talkback are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scandal continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello. I am calling from the Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh yeah, what kind of charges? I can't give you specifics over the phone, but to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. And look at this. My phone gives me a warning that this call might be a stab. Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone, thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools like Gemini Nano and Gemini in Android Studio. Also, stay tuned tomorrow for our upcoming Android 15 updates, which we can't wait to share with you. As we said at the outset, we're reimagining Android with Gemini at the core from your favorite apps to the OS itself. We're bringing the power of AI to every aspect of the smartphone experience. And with that, let me head over to Josh to share more on our use for developers. Thank you.
And let's hit that. Because I've heard that rule may be changing. Now, because I'm a Gemini advanced user, this works on any PDF and takes full advantage of the long context window. And there's just lots of times when that's useful. For example, let's say you're looking for a quick answer in an appliance user manual. And there you have it. It turns out, no, spin serves are not allowed. So Gemini not only gives me a clear answer to my question, it also shows me exactly where on the PDF to learn more. Awesome. OK. So that's a few of the ways that we're enhancing Gemini to do more context aware and helpful in the moment. And what you've seen here are the first really many new ways that Gemini will unlock new experiences at the system level. And they're only available on Android. You'll see these and work coming to hundreds of millions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, undivided foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel later this year, we'll be expanding what's possible with our latest model, Gemini Nano with multimodality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed TalkBack, an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses TalkBack, will typically come across 90 unlabeled images per day. Thankfully, TalkBack makes them accessible. And now we're taking that to the next level, with the multimodal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the style and cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to TalkBack are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scan continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello? Hi, I'm calling from Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh, yeah. What kind of charges? I can't give you specifics over the phone. But to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. LAUGHTER And look at this. My phone gives me a warning that this call might be a scam. APPLAUSE Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools, like Gemini Nano and Gemini in Android Studio.","The improvements to the Talkback feature with the introduction of Gemini Nano include enhanced multimodal capabilities that will provide richer and clearer descriptions of images. For example, when a user receives a photo, they will get a more detailed description of what is happening in the image. Additionally, when shopping online, users will receive clear descriptions of the style and cut of clothing items, helping them find the perfect look. These enhancements aim to make the experience more accessible and informative for users, particularly those with blindness or low vision. The improvements will also minimize latency as the processing will occur on-device, and the model will function even without a network connection."
What proactive feature does Gemini Nano provide to protect users from potential fraud during phone calls?,"And that means I can ask very esoteric questions like, for example, our spin serves allowed. And let's hit that because I've heard that rule maybe changing. Now, because I'm a Gemini advanced user, this works on any PDF and takes full advantage of the long context window. And there's just lots of times when that's useful. For example, let's say you're looking for a quick answer in an appliance user manual. And there you have it. It turns out, no, spin serves are not allowed. So Gemini not only gives me a clear answer to my question, it also shows me exactly when we're on the PDF to learn more. Awesome. Okay. So that's a few of the ways that we're enhancing Gemini to do more context aware and helpful in the moment. And what you've seen here are the first really many new ways that Gemini will unlock new experiences at the system level. And they're only available on Android. You'll see these and work coming to hundreds of billions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, on-device foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel later this year, we'll be expanding what's possible with our latest model, Gemini Nano with multi-modality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed TalkBack, an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Carol, who uses TalkBack, will typically come across 90 unlabeled images per day. Thankfully, TalkBack makes them accessible. And now we're taking that to the next level with the multi-modal capabilities of Gemini Nano. So when someone sends Carol a photo, she'll get a richer and clearer description of what's happening. Or let's say Carol is shopping online for an outfit. Now she can get a crystal clear description of the styling cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to TalkBack are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scandal continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello. Hi, I'm Pauline from Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh yeah, what kind of charges? I can't give you specifics over the phone. But to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. And look at this. My phone gives me a warning that this call might be a scam. I'm going to help you. Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone, thanks to the on device model. So remember that pickleball example earlier? Gemini and Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone.
And let's hit that. Because I've heard that rule may be changing. Now, because I'm a Gemini advanced user, this works on any PDF and takes full advantage of the long context window. And there's just lots of times when that's useful. For example, let's say you're looking for a quick answer in an appliance user manual. And there you have it. It turns out, no, spin serves are not allowed. So Gemini not only gives me a clear answer to my question, it also shows me exactly where on the PDF to learn more. Awesome. OK. So that's a few of the ways that we're enhancing Gemini to do more context aware and helpful in the moment. And what you've seen here are the first really many new ways that Gemini will unlock new experiences at the system level. And they're only available on Android. You'll see these and work coming to hundreds of millions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, undivided foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel later this year, we'll be expanding what's possible with our latest model, Gemini Nano with multimodality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed TalkBack, an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses TalkBack, will typically come across 90 unlabeled images per day. Thankfully, TalkBack makes them accessible. And now we're taking that to the next level, with the multimodal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the style and cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to TalkBack are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scan continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello? Hi, I'm calling from Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh, yeah. What kind of charges? I can't give you specifics over the phone. But to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. LAUGHTER And look at this. My phone gives me a warning that this call might be a scam. APPLAUSE Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools, like Gemini Nano and Gemini in Android Studio.","Gemini Nano provides a proactive feature that alerts users when it detects suspicious activity during phone calls. For instance, if a caller claims to be from a bank and asks the user to transfer money to a secure account, Gemini Nano will warn the user that the call might be a scam. This alert helps protect users from potential fraud by keeping the audio processing private and on the device."
What is the primary vision for the Gemini app as described in the event?,"We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. Over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now Gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now based on my flight information, Gemini knows that I needed two and a half day, itinerary. You can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens.","The primary vision for the Gemini app, as described in the event, is to be the most helpful personal AI assistant by providing users with direct access to Google's latest AI models. The app aims to assist users in various tasks such as learning, creating, coding, and more, while offering a natively multimodal experience that allows interaction through text, voice, or camera. Additionally, it includes features like customizable personal experts called ""gems"" and advanced trip planning capabilities, enhancing productivity and creativity for users both at home and work."
"What new feature allows users to customize their experience with Gemini, and what are they called?","Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. Over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now Gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now based on my flight information, Gemini knows that I needed two and a half day, itinerary. You can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens.
We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. You're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a pure reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new Dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunery for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules. And Gemini did this in a fraction of the time.","The new feature that allows users to customize their experience with Gemini is called ""Gems."" Users can create personal experts on any topic they want by setting up these Gems, which are simple to create and can be tailored to specific needs."
"What significant capability does Gemini Advanced offer regarding context windows, and how does it compare to other chatbots?","Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens. That is the longest context window of any chatbot in the world. It unlocks incredible new potential in AI, so you can tackle complex problems that were previously unimaginable. You can upload a PDF up to 1,500 pages long or multiple files to get insights across a project. And soon you can upload as much as 30,000 lines of code or even an hour long video. Gemini Advanced is the only chatbot that lets you process this amount of information. Now just imagine how useful this will be for students. Let's say you've spent months on your thesis and you could really use a fresh perspective. You can upload your entire thesis, your sources, your notes, your research, and soon interview audio recordings and videos too. So Gemini has all of this context to give you actionable advice. It can dissect your main points, identify improvements, and even roleplay as your professor. So you can feel confident in your work. And check out what Gemini Advanced can do with your spreadsheets, with the new data analysis feature launching in the coming weeks. Maybe you have a side hustle selling handcrafted products. But you're a better artist than accountant, and it's really hard to understand which products are worth your time. Simply upload all of your spreadsheets and ask Gemini to visualize your earnings and help you understand your profit. Gemini goes to work, calculating your returns, and pulling its analysis together into a single chart so you can easily understand which products are really paying off. Now behind the scenes, Gemini writes custom Python code to crunch these numbers. And of course your files are not used to train our models. Oh, and just one more thing. Later this year, we'll be doubling the long context window to two million tokens. We absolutely can't wait for you to try all of this for yourself. Gemini is continuing to evolve and improve at a breakthrough pace. We're making Gemini more multimodal, more agentive, and more intelligent, with the capacity to process the most information of any chatbot in the world. And as you heard earlier, we're also expanding Gemini Advanced to over 35 supported languages available today. But of course, what makes Gemini so compelling is how easy it is to do just about anything you want with a simple prompt. Let's take a look. Enter a prompt here. Okay, it can be that hard. How about generate an image of a cat playing guitar? Is that how it works? Am I doing AI? Yeah, just does whatever you type. What a last minute gift ideas you can make with arts and crafts. Plan a workout routine to get bigger calves. Help me think of titles my tell all men more. What's something smart I can say about Renoir? Generate another image of a cat playing guitar. If a girl calls me a snack, how do I reply? Yeah, that's how it works. You're doing AI. Make this email somewhere professional before it hits. What's a good excuse to cancel dinner with my friends? They're literally sitting right here. You know I can do other stuff, right?
As context windows grow, we're discovering new ways to interact with models to solve tasks. Huge context windows unlock a new set of AI applications, but they're also more computationally expensive, which is why I'm excited to announce the new context-caching feature coming next month. So how does it work? Well, let's say a large part of your prompt doesn't change. You can cache that once and easily call it unsubssequent turns for a fraction of the price. So imagine a major analyst trying to figure out what to write about next. To help me brainstorm ideas, I might want to gather all the content I've already created. Previous articles, some video clips, maybe some voice memos. Using context-caching, I'll send all this content at once, which let's say it will cost me about 20 cents for that first turn, along with the regular input price for the rest of my prompts. Since I cache that content that I'm referencing, each following turn will only cost me 10 cents, plus the cost of any new prompts. Of course, this concept is extensible to so many different applications that you might want to build. These could involve things like legal documents, research papers, school assignments, or employee training materials. The sky is the limit. Gemini models are empowering developers worldwide to transform workflows, enhancing efficiency, and also adding a touch of user delight. Take, for example, front-end development. Locify.ai takes a Figma design and uses their own AI models to generate code. But what's really unique is how they're pairing with Gemini's 1 million token context window. You see, oftentimes, front-end designs contain hundreds of layers with nondescript names such as rectangle one or group 76. Locify sends this entire design to 1.5 pro to generate meaningful names in the front-end code. Because Gemini understands the purpose behind rectangle one, it's actually able to rename it in code as something more useful like header background. Another one that I love is how AI models are unlocking new abilities. Invision is powering people with low vision to better understand their immediate environment through an app or smart glasses and ask specific questions. Because of the speed of 1.5 flash, users will be able to get quick responses. Another developer, Zapier, is enabling anybody to seamlessly connect different apps and automate repetitive tasks like podcast editing. They built a workflow to automatically remove speaker artifacts like OZ and OMS from audio tracks that's triggered as soon as a user uploads a file to Google Drive. As developers, the ideas you have and the code you write will help transform AI into practical applications for the world. We're announcing our Gemini API developer competition for a chance to win the grand prize, a custom electric de lorean. Woo! Woo! Woo! Woo! Woo! Woo! For those of you in the audience, be sure to check it out at the AI Pavilion, maybe grab a picture, and visit our website for more details. And now I'm going to leave you with my favorite developer to tell you a little bit more about it. Hey! You may recognize me for my acting work, but my real passion is programming. When I'm in the zone, I'm unstoppable. I don't know if you know this about me, but I'm really excited about the future and Google's Gemini models are tech that can get it there. And you know I'm going to be a great developer that you know I'm submitting to Google's new Gemini developer contest, which is so cool, it'll blow your mind back to beta. A chance to win for the most creative, useful, and remarkable apps. If you're a dev or a dev team that wants to submit, great! But you'll have some catching up to do. I just drank three cups of hydro cold brew and I'm ready to crank out some serious times of the world poll. Now we'll look at examples of how AI unlocks excellent experiences and how we can get the best results. Jacqueline talked about how Gemini models benefit all developers. Now we'll look at examples of how AI unlocks excellent experiences on Android, the world's most popular operating system. We're reimagining Android with AI at the core, enabling you to build a whole new class of apps on mobile. This unlocks experiences that were not even possible just a few years ago. So let's dive into the ways that Gemini can power building the next generation of apps on Android.","Gemini Advanced offers a significant capability with the longest context window of any chatbot in the world, allowing for up to 1 million tokens. This extensive context window enables users to tackle complex problems that were previously unimaginable. Additionally, it allows users to upload large documents, such as a PDF up to 1,500 pages long or multiple files, to gain insights across a project. This capability is particularly beneficial for students and professionals who need to process substantial amounts of information, as it can provide actionable advice based on the entire context provided. Furthermore, later this year, Gemini Advanced plans to double the context window to two million tokens, further enhancing its ability to handle extensive data compared to other chatbots."
"How does Gemini assist users in trip planning, and what factors does it consider?","We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules. And Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens. That is the longest context window of any chatbot in the world. It unlocks incredible new potential in AI so you can tackle complex problems that were previously unimaginable. You can upload a PDF up to 1500 pages long or multiple files to get insights across a project. And soon you can upload as much as 30,000 lines of code or even an hour long video. Gemini Advanced is the only chatbot that lets you process this amount of information. Now just imagine how useful this will be for students. Let's say you spent months on your thesis and you could really use a fresh perspective. You can upload your entire thesis, your sources, your notes, your research, and soon interview audio recordings and videos too. So Gemini has all of this context to give you actionable advice. It can dissect your main points, identify improvements, and even role play as your professor. So you can feel confident in your work. And check out what Gemini Advanced can do with your spreadsheets, with the new data analysis feature launching in the coming weeks. Maybe you have a side hustle selling handcrafted products, but you're a better artist than accountant. And it's really hard to understand which products are worth your time. Simply upload all of your spreadsheets and ask Gemini to visualize your earnings and help you understand your profit. Gemini goes to work, calculating your returns, and pulling its analysis together into a single chart so you can easily understand which products are really paying off. Now behind the scenes, Gemini writes custom Python code to crunch these numbers. And of course your files are not used to train our models. Oh, and just one more thing. Later this year, we'll be doubling the long context window to two million tokens. We absolutely can't wait for you to try all of this for yourself. Gemini is continuing to evolve and improve at a breakthrough pace. We're making Gemini more multimodal, more agentive, and more intelligent, with the capacity to process the most information of any chatbot in the world. And as you heard earlier, we're also expanding Gemini Advanced to over 35 supported languages available today. But of course, what makes Gemini so compelling is how easy it is to do just about anything you want with a simple prompt. Let's take a look. Enter a prompt here. Okay, it can be that hard. How about generate an image of a cat playing guitar? Is that how it works? Am I doing AI? Yeah, it just does whatever you type. What a last minute gift ideas you can make with arts and crafts. Plan a workout routine to get bigger calves. Help me think of titles my tell all men war. What's something smart I can say about Renoir? Generate another image of a cat playing guitar. If a girl calls me a snack, how do I reply? Yeah, that's how it works. You do AI. Make this email sound more professional before it hits. What's a good excuse to cancel dinner with my friends? We're literally sitting right here. No wrong way to prompt. Hey, if you begin your own way to prompt, just every time you're prompting a prompt bar. Or just generate an image of a cat playing guitar. You know, it can do other stuff, right?","Gemini assists users in trip planning by gathering and analyzing various types of information to create a personalized vacation plan. It takes into account several factors, including:

1. **User Preferences**: Gemini considers the interests and preferences of the user and their travel companions. For example, it can factor in what activities each person enjoys, such as art or seafood.

2. **Flight and Hotel Information**: It utilizes details from the user's email, such as flight and hotel bookings, to understand the logistics of the trip.

3. **Spatial Data**: Gemini uses spatial data to make decisions about the itinerary, such as finding nearby restaurants or activities based on the user's location.

4. **Dynamic Adjustments**: Users can modify their plans (e.g., changing start times for activities), and Gemini will adjust the itinerary accordingly, ensuring that it aligns with the user's preferences and constraints.

5. **Time Management**: It considers the timing of activities, such as skipping large activities on the day of arrival if the flight lands late, to optimize the travel experience.

Overall, Gemini's trip planning experience combines reasoning, intelligence, and user input to create a tailored travel itinerary efficiently."
"What upcoming feature related to data analysis will Gemini Advanced provide, and how can it benefit users?","Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. Over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now Gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now based on my flight information, Gemini knows that I needed two and a half day, itinerary. You can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens.
Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens. That is the longest context window of any chatbot in the world. It unlocks incredible new potential in AI, so you can tackle complex problems that were previously unimaginable. You can upload a PDF up to 1,500 pages long or multiple files to get insights across a project. And soon you can upload as much as 30,000 lines of code or even an hour long video. Gemini Advanced is the only chatbot that lets you process this amount of information. Now just imagine how useful this will be for students. Let's say you've spent months on your thesis and you could really use a fresh perspective. You can upload your entire thesis, your sources, your notes, your research, and soon interview audio recordings and videos too. So Gemini has all of this context to give you actionable advice. It can dissect your main points, identify improvements, and even roleplay as your professor. So you can feel confident in your work. And check out what Gemini Advanced can do with your spreadsheets, with the new data analysis feature launching in the coming weeks. Maybe you have a side hustle selling handcrafted products. But you're a better artist than accountant, and it's really hard to understand which products are worth your time. Simply upload all of your spreadsheets and ask Gemini to visualize your earnings and help you understand your profit. Gemini goes to work, calculating your returns, and pulling its analysis together into a single chart so you can easily understand which products are really paying off. Now behind the scenes, Gemini writes custom Python code to crunch these numbers. And of course your files are not used to train our models. Oh, and just one more thing. Later this year, we'll be doubling the long context window to two million tokens. We absolutely can't wait for you to try all of this for yourself. Gemini is continuing to evolve and improve at a breakthrough pace. We're making Gemini more multimodal, more agentive, and more intelligent, with the capacity to process the most information of any chatbot in the world. And as you heard earlier, we're also expanding Gemini Advanced to over 35 supported languages available today. But of course, what makes Gemini so compelling is how easy it is to do just about anything you want with a simple prompt. Let's take a look. Enter a prompt here. Okay, it can be that hard. How about generate an image of a cat playing guitar? Is that how it works? Am I doing AI? Yeah, just does whatever you type. What a last minute gift ideas you can make with arts and crafts. Plan a workout routine to get bigger calves. Help me think of titles my tell all men more. What's something smart I can say about Renoir? Generate another image of a cat playing guitar. If a girl calls me a snack, how do I reply? Yeah, that's how it works. You're doing AI. Make this email somewhere professional before it hits. What's a good excuse to cancel dinner with my friends? They're literally sitting right here. You know I can do other stuff, right?","Gemini Advanced will provide a new data analysis feature that allows users to upload spreadsheets. This feature will help users, such as those running a side hustle, to visualize their earnings and understand their profits. By uploading their spreadsheets, users can ask Gemini to analyze the data, calculate returns, and present the analysis in a single chart, making it easier to identify which products are worth their time. Additionally, Gemini will write custom Python code behind the scenes to perform these calculations, ensuring that users receive actionable insights based on their data."
What percentage improvement in productivity did Sports Basement experience after rolling out Gemini for workspace?,"The Sons last May we've been hard at work making German IFA workspace even more helpful for businesses and consumers across the world Tens of thousands of customers have been using help me write help me visualize help me organize since we launched and Now we're really excited that the new Gemini powered side panel will be generally available next month One of our customers is a local favorite right here in California sports basement They rolled out Gemini for workspace to the organization and this has helped improve the productivity of their customer support team by more than 30 percent Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages We are really excited about what Gemini 1.5 Pro So price pro unlocks for workspace and AI premiums customers Let me start by showing you three new capabilities coming to Gmail mobile This is my Gmail account Okay, there's an email up top from my husband help me sort out the roof repair thing please now We've been trying to find a contractor to fix our roofs and with work travel. I have clearly dropped the ball It looks like there's an email thread on this with lots of emails that I haven't read and luckily for me I can simply tap the summarize option up top and skip reading this long back and forth Now Gemini pulls up this helpful mobile card as an overlay and this is where I can read a nice summary of all the salient information that I need to know So I see here that we have a quote from Jeff at Green Roofing and he's ready to start Now I know we had other bids and I don't remember the details Previously I would have had to do a number of searches in Gmail and then remember and compare information across different emails Now I can simply type out my question right here in the mobile card and say something like Compare my roof repair bids by price and availability This new Q&A feature makes it so easy to get quick answers on anything in my inbox For example when am I shoes arriving or what time the doors open for the next game without having to first search Gmail Then open the email and then look for the specific information and attachments and so on Anyway back to my roofs It looks like Gemini has found details that I got from two other contractors in completely different email threads And I have this really nicely organized summary and I can do a quick comparison So it seems like Jeff's quote was right in the middle. You can start immediately. So Green Roofing it is I'll open that last email from Jeff and confirm the project and look at that I Seasons suggested replies from Gemini now what is really really neat about this evolution of smart reply is that it's Contextual Gemini understood the back and forth and that thread and that Jeff was ready to start so offers me a few customized options based on that context So you know here. I see I have declined the service suggest a new time. I'll choose proceed and confirm time I can even see a preview of the full reply simply by long pressing This looks reasonable so I'll hit send These new capabilities and Gemini and Gmail will start rolling out this month to labs users Okay, so one of the really neat things about works space apps like Gmail drive docs calendar is how well they work together And in our daily lives we often have information that flows from one app to another like say adding a calendar entry from Gmail or creating reminders from a spreadsheet tracker But what if Gemini can make these journeys totally seamless perhaps even automate them for you entirely So let me show you what I mean with a real-life example My sister is a self-employed photographer and her inbox is full of appointment bookings receipts client feedback on photos and so much more Now if you're a freelancer or a small business you really want to focus on your craft and not on bookkeeping and logistics So let's go to her inbox and take a look Lots of unread emails. Let's click on the first one It's got a PDF that's an attachment from a hotel as a receipt and I see a suggestion in the side panel Help me organize and track my receipts. Let's click on this prompt The side panel now show me more detailed about what that really means and as you can see There's two steps here step one Create a drive folder and put this receipt and 37 others. It's found into that folder.
Do my thing, do my thing. I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. You know, you know, you know, you know, you know, you know. I got the news, dude. I got the new game. Yeah, you know? Yeah, you know? I got the news, dude. I got the news, no. No, no, no, no. No, no, no, no, no. I got the news, dude. I got the news, dude. I got the news, duh. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Help Me Organize since we launched. And now we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini for workspace to the organization and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5 Pro Unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now we've been trying to find a contractor to fix our roofs and with work travel I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read. And luckily for me I can simply tap the summarize option up top and skip reading this long back and forth. Now Gemini pulls up this helpful mobile card as an overlay and this is where I can read a nice summary of all the salient information that I need to know. So I see here that we have a quote from Jeff at Green Roofing and he's ready to start. Now I know we had other bids and I don't remember the details. Previously I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now I can simply type out my question right here in the mobile card and say something like compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox. For example when am I shoes arriving or what time to doors open for the next game without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway back to my roofs. It looks like Gemini has found details that I got from two other contractors in completely different email threads and I have this really nicely organized summary and I can do a quick comparison. So it seems like Jeff's quote was right in the middle and you can start immediately so Green Roofing it is. I'll open that last email from Jeff and confirm the project and look at that. I see some suggested replies from Gemini. Now what is really really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread and that Jeff was ready to start so offers me a few customized options based on that context. So here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay so one of the really neat things about Workspace Apps like Gmail, Drive, Docs, Calendar is how well they work together. In our daily lives we often have information that flows from one app to another like say adding a calendar entry from Gmail or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely. So let me show you what I mean with a real life example.",Sports Basement experienced a productivity improvement of more than 30 percent after rolling out Gemini for workspace.
How many languages does the automatic language detection feature of Gemini support?,"And it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to AIStudio.Google.com, log in with your Google account. And you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit Run. Now in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, it starts streaming it back. Now this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from notebook LM, like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype here in 10 seconds. And with one click in the upper left, get an API key. Or over here in the upper right, just tap Get Code. And you've got all of the model configurations, the safety settings, ready to go, straight into your IDE. Now over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and then using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. It's optimized for a range of image captioning visual Q&A and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model. But it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on next gen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. To wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. Languages are a very trusting problem to solve actually. And given India has a huge variety of languages and it changes every 5 kilometres. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India.
I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.",The automatic language detection feature of Gemini supports 35 languages.
What are the two main steps Gemini automates for organizing receipts in the example provided?,"Let's click on the first one It's got a PDF that's an attachment from a hotel as a receipt and I see a suggestion in the side panel Help me organize and track my receipts. Let's click on this prompt The side panel now show me more detailed about what that really means and as you can see There's two steps here step one Create a drive folder and put this receipt and 37 others. It's found into that folder. Make sense Step two extract the relevant information from those receipts in that folder into a new spreadsheet now This sounds useful why not I also have the option to edit these actions or just hit OK So let's hit OK Gemini will now complete the two steps described above and this is where it gets even better Gemini offers you the option to automate this so that this particular workflow is run on all future emails Keeping your drive folder an expense sheet up to date with no effort from you Now Now we know that creating a complex spreadsheet like this can be daunting for most people But with this automation Gemini does the hard work of extracting all the right information from all the files and in that folder and Generates this sheet for you. So let's take a look. Okay. It's super well organized and it even has a category for expense type now We have the sheet things can get even more fun We can ask Gemini questions questions like show me where the money spent Gemini not only analyzes the data from the sheet But also creates a nice visual to help me see the complete breakdown by category And you can imagine how this extends to all sorts of use cases in your inbox like travel expenses Shopping remodeling projects you name it all of that information and Gmail can be put to good use and help you Work plan and play better now this particular This particular ability to organize your attachments and drive and generate a sheet and do data analysis via Q&A We'll be rolling out to labs users this September and it's just one of the many automations that we're working on in workspace Work space in the Gemini era will continue to unlock new ways of getting things done We're building advanced agent of experiences including customizing how you use Gemini Now as we look to 2025 and beyond we're exploring entirely new ways of working with AI Now with Gemini you have an AI power assistant always at your side But what if you could expand how you interact with AI For example when we work with other people we mention them in comments and docs or we send them emails We have group chats with them etc And it's not just how we collaborate with each other But we each have a specific role to play in the team and as the team works together We build a set of collective experiences and contacts to learn from each other We have the combined set of skills to draw from when we need help So how could we introduce AI into this mix and build on this shared expertise Well, here's one way We're prototyping a virtual Gemini powered teammate This teammate has an identity a workspace account along with a specific role and objectives Let me bring Tony up to show you what I mean Hey Tony Hey everyone okay So let me start by showing you how we set up this virtual teammate As you can see the teammate has this very own account and we can go ahead and give it a name. We'll do something fun like Chip Chips been given a specific job role at the set of descriptions on how to be helpful for the team You see that here and some of the jobs are to monitor and track projects We've listed a few out to organize information and provide context and a few more things Now that we've configured our virtual teammate Let's go ahead and see chip in action to do that. I'll switch us over here to Google chat First when planning for an event like IO we have a ton of chat rooms for various purposes Luckily for me chip is in all of them To quickly catch up I might ask a question like Anyone know if our IO Storyboards are approved Because we've instructed chip to track this project chip searches across all the conversations and knows respond with an answer There it is simple but very helpful Now as the team adds chip to more group chats more files more email threads Chip builds a collective memory of our work together Let's look at an example to show you all switch over to a different room. How about project sapphire over here And here we are discussing a product release coming up and as usual many pieces are still in flight so I can go ahead and ask Are we on track?
So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example. My sister is a self-employed photographer, and her inbox is full of appointment bookings, receipts, client feedback on photos, and so much more. Now, if you're a freelancer or a small business, you really want to focus on your craft and not on bookkeeping and logistics. So, let's go to her inbox and take a look. Lots of unread emails. Let's click on the first one. It's got a PDF. That's an attachment from a hotel as a receipt, and I see a suggestion in the side panel. Help me organize and track my receipts. Let's click on this prompt. The side panel now will show me more details about what that really means. And as you can see, there's two steps here. Step one. Create a drive folder and put this receipt and 37 others it's found into that folder. Make sense. Step two. Extract the relevant information from those receipts in that folder into a new spreadsheet. Now, this sounds useful. Why not? I also have the option to edit these actions, or just hit OK. So, let's hit OK. Gemini will now complete the two steps described above, and this is where it gets even better. Gemini offers you the option to automate this so that this particular workflow is run on all future emails, keeping your drive folder and expense sheet up to date with no effort from you. Now, we know that creating a complex spreadsheet like this can be daunting for most people. But with this automation, Gemini does the hard work of extracting all the right information from all the files and in that folder and generates this sheet for you. So, let's take a look. OK, it's super well organized and it even has a category for expense type. Now, we have the sheet. Things can get even more fun. We can ask Gemini questions, questions like show me where the money spent. Gemini not only analyzes the data from the sheet, but also creates a nice visual to help me see the complete breakdown by category. You can imagine how this extends to all sorts of use cases in your inbox like travel expenses, shopping, remodeling projects, you name it. All of that information in Gmail can be put to good use and help you work, plan, and play better. Now, this particular... I know. This particular ability to organize your attachments and drive and generate a sheet and do data analysis via Q&A will be rolling out to labs users this September. And it's just one of the many automations that we're working on in workspace. Workspace in the Gemini era will continue to unlock new ways of getting things done. We're building advanced agent of experiences, including customizing how you use Gemini. Now, as we look to 2025 and beyond, we're exploring entirely new ways of working with AI. Now, with Gemini, you have an AI-powered assistant always at your side. But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs, or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and context to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives.","The two main steps that Gemini automates for organizing receipts in the example provided are:

1. **Create a drive folder** and put the receipt along with 37 others that it finds into that folder.
2. **Extract the relevant information** from those receipts in that folder into a new spreadsheet."
What is the name of the virtual Gemini powered teammate prototype mentioned in the presentation?,"Now, with Gemini, you have an AI-powered assistant always at your side. But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs, or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and context to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives. Let me bring Tony up to show you what I mean. Hey, Tony. Hey, partner. Hey, everyone. Okay. So let me start by showing you how we set up this virtual teammate. As you can see, the teammate has this very own account, and we can go ahead and give it a name. We'll do something fun like Chip. Chip's been given a specific job role at the set of descriptions on how to be helpful for the team. You can see that here. And some of the jobs are to monitor and track projects. We've listed a few out to organize information and provide context and a few more things. Now that we've configured our virtual teammate, let's go ahead and see Chip in action to do that. I'll switch us over here to Google Chat. First, when planning for an event like IO, we have a ton of chat rooms for various purposes. Luckily for me, Chip is in all of them. To quickly catch up, I might ask a question like, anyone know if our IO storyboards are approved? Because we've instructed Chip to track this project, Chip searches across all the conversations and knows respond with an answer. There it is. Simple but very helpful. Now, as the team adds Chip to more group chats, more files, more email threads, Chip builds a collective memory of our work together. Let's look at an example to show you all switch over to a different room. How about a project sapphire over here? And here we are discussing a product release coming up. And as usual, many pieces are still in flight. So I can go ahead and ask, are we on track for launch? Chip gets to work not only searching through everything it has access to, but also synthesizing what's found and coming back with an up-to-date response. There it is. A clear timeline and a summary, and notice even in this first message here, Chip flags a potential issue the team should be aware of. Because we're in a group space, everyone can follow along, anyone can jump in at any time, as you see someone just did, asking Chip to help create a doc to help address the issue. A task like this could take me hours, dozens of hours, Chip can get all done in just a few minutes, sending the doc over right when it's ready. And so much of this practical helpfulness comes from how we've customized Chip to our team's needs, and how seamlessly this AI is integrated directly into where we're already working. Back to you, Aparna. Thank you, Tony. Now, I can imagine a number of virtual types of virtual teammates configured by businesses to help them do what they need. Now, we have a lot of work to do to figure out how to bring these agent of experiences like virtual teammates into workspace, including enabling third parties to make their very own versions of Chip. We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google.
But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and contacts to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives. Let me bring Tony up to show you what I mean. Hey, Tony. Hey, partner. Hey, everyone. Okay. So let me start by showing you how we set up this virtual teammate. As you can see, the teammate has this very own account. And we can go ahead and give it a name. We'll do something fun like Chip. Chip's been given a specific job role at the set of descriptions on how to be helpful for the team. You can see that here. And some of the jobs are to monitor and track projects. We've listed a few out to organize information and provide context and a few more things. Now that we've configured our virtual teammate, let's go ahead and see Chip in action. To do that, I'll switch this over here to Google Chat. First, when planning for an event like I.O., we have a ton of chat rooms for various purposes. Luckily for me, Chip is in all of them. To quickly catch up, I might ask a question like, anyone know if our I.O. storyboards are approved? Because we've instructed Chip to track this project, Chip searches across all the conversations and knows respond with an answer. There it is. Simple but very helpful. Now, as the team adds Chip to more group chats, more files, more email threads, Chip builds a collective memory of our work together. Let's look at an example to show you all switch over to a different room. How about a project sapphire over here? And here we are discussing a product release coming up. And as usual, many pieces are still in flight. So I can go ahead and ask, are we on track for launch? Chip gets to work not only searching through everything it has access to, but also synthesizing what's found and coming back with an up-to-date response. There it is. A clear timeline, a nice summary, and notice even in this first message here, Chip flags a potential issue the team should be aware of. Because we're in a group space, everyone can follow along, anyone can jump in at any time, as you see someone just did, asking Chip to help create a doc to help address the issue. A task like this could take me hours, dozens of hours, Chip can get all done in just a few minutes, sending the doc over right when it's ready. So much of this practical helpfulness comes from how we've customized Chip to our team's needs, and how seamlessly this AI is integrated directly into where we're already working back to you, Aparna. Thank you, Tony. Now, I can imagine a number of virtual types of, number of different types of virtual teammates configured by businesses to help them do what they need. Now we have a lot of work to do to figure out how to bring these agent of experiences like virtual teammates into workspace, including enabling third parties to make their very own versions of Chip. We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience.","The name of the virtual Gemini-powered teammate prototype mentioned in the presentation is ""Chip."""
What new capabilities are being introduced to Gmail mobile with Gemini 1.5 Pro?,"Do my thing, do my thing. I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. You know, you know, you know, you know, you know, you know. I got the news, dude. I got the new game. Yeah, you know? Yeah, you know? I got the news, dude. I got the news, no. No, no, no, no. No, no, no, no, no. I got the news, dude. I got the news, dude. I got the news, duh. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Help Me Organize since we launched. And now we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini for workspace to the organization and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5 Pro Unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now we've been trying to find a contractor to fix our roofs and with work travel I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read. And luckily for me I can simply tap the summarize option up top and skip reading this long back and forth. Now Gemini pulls up this helpful mobile card as an overlay and this is where I can read a nice summary of all the salient information that I need to know. So I see here that we have a quote from Jeff at Green Roofing and he's ready to start. Now I know we had other bids and I don't remember the details. Previously I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now I can simply type out my question right here in the mobile card and say something like compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox. For example when am I shoes arriving or what time to doors open for the next game without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway back to my roofs. It looks like Gemini has found details that I got from two other contractors in completely different email threads and I have this really nicely organized summary and I can do a quick comparison. So it seems like Jeff's quote was right in the middle and you can start immediately so Green Roofing it is. I'll open that last email from Jeff and confirm the project and look at that. I see some suggested replies from Gemini. Now what is really really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread and that Jeff was ready to start so offers me a few customized options based on that context. So here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay so one of the really neat things about Workspace Apps like Gmail, Drive, Docs, Calendar is how well they work together. In our daily lives we often have information that flows from one app to another like say adding a calendar entry from Gmail or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely. So let me show you what I mean with a real life example.
I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. I got the news, dude. I got the news, dude. I got therada, I got theirdoooo. I got the news, dude. I got the news, dude. I got the news, dudes. I got the news, dude. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Me Organize, Since We Launched. And now, we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini-for-Works-based to the organization, and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5-Private Pro unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now, we've been trying to find a contractor to fix our roofs, and with Work Travel, I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read, and luckily for me, I can simply tap the summarized option up top and skip reading this long back and forth. Now, Gemini pulls up this helpful mobile card as an overlay, and this is where I can read a nice summary of all the salient information that I need to know. So, I see here that we have a quote from Jeff at Green Roofing, and he's ready to start. Now, I know we had other bids, and I don't remember the details. Previously, I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now, I can simply type out my question right here in the mobile card and say something like, compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox, for example, when am I shoes arriving, or what time the doors open for the next game, without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway, back to my roof. It looks like Gemini has found details that I got from two other contractors in completely different email threads, and I have this really nicely organized summary, and I can do a quick comparison. So, it seems like Jeff's quote was right in the middle, and he can start immediately, so Green Roofing it is. I'll open that last email from Jeff and confirm the project, and look at that. I see some suggested replies from Gemini. Now, what is really, really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread, and that Jeff was ready to start, so offers me a few customized options based on that context. So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example.","The new capabilities being introduced to Gmail Mobile with Gemini 1.5 Pro include:

1. **Summarization Feature**: Users can tap a summarize option to get a concise summary of long email threads, allowing them to skip reading through extensive back-and-forth communications.

2. **Q&A Feature**: This feature enables users to type questions directly in the mobile card, such as comparing roof repair bids by price and availability, making it easier to get quick answers without searching through emails.

3. **Contextual Smart Replies**: Gemini offers suggested replies based on the context of the email thread, understanding the conversation's flow and providing customized response options.

These capabilities aim to enhance productivity and streamline the email management process for users."
What is the expected performance improvement of the sixth generation TPUs called Trillium compared to the previous generation?,"Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years, we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensile processing units that power our AI advances, Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs, we are proud to offer CPUs and GPUs to support any workload that includes the new Axion processes we announced last month, our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge black-well GPUs available in early 2025. We are fortunate to have a long-standing partnership with NVIDIA, excited to bring black-wells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI Hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying the raw hardware and chips. Our AI Hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We've been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly one gigawatt and growing. That's close to 70 times the capacity of any other fleet. Undroline this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers using better context, location awareness, and real time information. With advances in natural language understanding and computer vision, we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course, search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities, our high bar for information quality, our decades of experience connecting you to the richness of the web, the result is a product that does the work for you. Google search is a generative AI at the scale of human curiosity and it's our most exciting chapter of search yet. To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search.
At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features are really good. They're a great way to create a new world. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways, and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AGI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensed processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs we are proud to offer CPUs and GPUs to support any workload. That includes the new Axion processes we announced last month. Our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge blackwell GPUs available in the market. We are fortunate to have a long standing partnership with NVIDIA, excited to bring blackwells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better.",The expected performance improvement of the sixth generation TPUs called Trillium compared to the previous generation is 4.7 times (4.7x).
How much has the industry demand for ML compute grown in the last six years?,"Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years, we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensile processing units that power our AI advances, Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs, we are proud to offer CPUs and GPUs to support any workload that includes the new Axion processes we announced last month, our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge black-well GPUs available in early 2025. We are fortunate to have a long-standing partnership with NVIDIA, excited to bring black-wells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI Hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying the raw hardware and chips. Our AI Hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We've been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly one gigawatt and growing. That's close to 70 times the capacity of any other fleet. Undroline this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers using better context, location awareness, and real time information. With advances in natural language understanding and computer vision, we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course, search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities, our high bar for information quality, our decades of experience connecting you to the richness of the web, the result is a product that does the work for you. Google search is a generative AI at the scale of human curiosity and it's our most exciting chapter of search yet. To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search.
At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features are really good. They're a great way to create a new world. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways, and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AGI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensed processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs we are proud to offer CPUs and GPUs to support any workload. That includes the new Axion processes we announced last month. Our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge blackwell GPUs available in the market. We are fortunate to have a long standing partnership with NVIDIA, excited to bring blackwells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better.",The industry demand for ML compute has grown by a factor of 1 million in the last six years.
What are the three unique strengths of Google search in the Gemini era as mentioned in the presentation?,"A Google we are fully in our Gemini era. Today all of our two billion user products use Gemini. Gemini 1.5 Pro is available today in Workspace Labs. Let's see how this comes to life with Google Workspace. People are always searching their emails and Gmail. We are working to make it much more powerful with Gemini. Now we can ask Gemini to summarize all recent emails from the school. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. People love using photos to search across their life. With Gemini, you're making that a whole lot easier. And ask photos can also help you search your memories in a deeper way. For example, you might be reminiscing about your daughter Lucius, early milestones. You can ask photos, show me how Lucius swimming has progressed. Here Gemini goes beyond a simple search, recognizing different contexts and photos packages it up all together in a summary. Unlocking knowledge across formats is why we build Gemini to be multimodal from the ground up. It's one model with all the modalities built in. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. So today, we are expanding the context window to two million tokens. So far, we've talked about two technical advances, multimodality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. But what if we could go even further? That's one of the opportunities we see with AI agents. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Here's a video of our prototype, which we'll see has two parts. Each part was captured in a single take in real time. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector, IV. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. Give me a banner name for this duo. Golden Stripes. Nice. Thanks, Gemini. Today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context. There's one more area I'm really excited to share with you. Our teams have made some incredible progress in genitive video. Today, I'm excited to announce our newest, most capable genitive video model, called VO. VO creates high quality 1080p videos from text, image and video prompts. It can capture the details of your instructions in different visual and cinematic styles. For 25 years, we have invested in world-class technical infrastructure. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. Google Search is a generative AI at the scale of human curiosity, and it's our most exciting chapter of Search yet. All the advancements you'll see today are made possible by a new Gemini model, customized for Google Search. What really sets this apart is our three unique strengths. This is Search in the Gemini era. By the end of the year, AI overviews will come to over a billion people. We're making AI overviews even more helpful for your most complex questions. The types that are really more like 10 questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. I'm really excited to share that soon you'll be able to ask questions with video. Why will this not stay in place? In the near-instead, Google gives me an AI overview. I guess some reasons this might be happening and steps I can take to troubleshoot.
To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly, complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today, with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. You make this possible for introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates to use in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you've got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there. So since you're asking about places, it taps into Google's index of information about the real world with over 250 million places and updated in real time, including their ratings, reviews, business hours, and more. Research that might have taken you minutes or even hours Google can now do on your behalf in just seconds. Next, let me show you another way multi-step reasoning in Google search can make your life that much easier. Take planning, for example. Planning of trips and meal plans can be fun. But doing the work of actually figuring it all out, no thank you. With Gemini in search, Google does the planning with you. Planning is really hard for AI to get right. It's the type of problem that takes advance reasoning and logic. After all, if you're meal planning, you probably don't want mac and cheese for breakfast, lunch, and dinner. Okay, my tics might, but say you're looking for a bit more variety. Now, you can ask search to create a three-day meal plan for a group that's easy to prepare. And here you get a plan with a wide range of recipes from across the web. This one for overnight oats looks particularly interesting. And you can easily head over to the website to learn how to prepare them. If you want to get more veggies in, you can simply ask search to swap in a vegetarian dish. And just like that, search customizes your meal plan.","The three unique strengths of Google search in the Gemini era, as mentioned in the presentation, are:

1. **Real-time information**: Google has access to over a trillion facts about people, places, and things, allowing it to provide up-to-date information.

2. **Unparalleled ranking and quality systems**: Google has trusted ranking and quality systems that have been developed over decades to ensure users receive the very best of the web.

3. **The power of Gemini**: The Gemini model unlocks new agent capabilities right in search, enhancing the overall functionality and effectiveness of the search experience."
"What is the total deployed fleet capacity for liquid cooling systems at Google, and how does it compare to other fleets?","We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision, we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course, search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities are high bar for information quality, our decades of experience connecting you to the richness of the web, the result is a product that does the work for you. Google search is a generative AI at the scale of human curiosity and it's our most exciting chapter of search yet to tell you more. Here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. Now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind and whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. To make this possible, we're introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates studios in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning.
At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features are really good. They're a great way to create a new world. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways, and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AGI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensed processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs we are proud to offer CPUs and GPUs to support any workload. That includes the new Axion processes we announced last month. Our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge blackwell GPUs available in the market. We are fortunate to have a long standing partnership with NVIDIA, excited to bring blackwells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better.",The total deployed fleet capacity for liquid cooling systems at Google is nearly 1 gigawatt. This capacity is close to 70 times the capacity of any other fleet in the industry.
What new capabilities will Google search offer in terms of planning and organizing information?,"To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly, complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today, with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. You make this possible for introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates to use in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you've got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there. So since you're asking about places, it taps into Google's index of information about the real world with over 250 million places and updated in real time, including their ratings, reviews, business hours, and more. Research that might have taken you minutes or even hours Google can now do on your behalf in just seconds. Next, let me show you another way multi-step reasoning in Google search can make your life that much easier. Take planning, for example. Planning of trips and meal plans can be fun. But doing the work of actually figuring it all out, no thank you. With Gemini in search, Google does the planning with you. Planning is really hard for AI to get right. It's the type of problem that takes advance reasoning and logic. After all, if you're meal planning, you probably don't want mac and cheese for breakfast, lunch, and dinner. Okay, my tics might, but say you're looking for a bit more variety. Now, you can ask search to create a three-day meal plan for a group that's easy to prepare. And here you get a plan with a wide range of recipes from across the web. This one for overnight oats looks particularly interesting. And you can easily head over to the website to learn how to prepare them. If you want to get more veggies in, you can simply ask search to swap in a vegetarian dish. And just like that, search customizes your meal plan.
Soon you'll be able to ask search to find the best yoga Pilates studios in Boston and show you details on their inter offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there. So since you're asking about places, it taps into Google's index of information about the real world with over 250 million places and updated in real time, including their ratings, reviews, business hours, and more. Research that might have taken you minutes or even hours Google can now do on your behalf in just seconds. Next, let me show you another way multi-step reasoning in Google search can make your life that much easier. Take planning, for example. Dreaming of trips and meal plans can be fun. But doing the work of actually figuring it all out, no thank you. With Gemini in search, Google does the planning with you. Planning is really hard for AI to get right. It's the type of problem that takes advanced reasoning and logic. After all, if you're meal planning, you probably don't want mac and cheese for breakfast, lunch, and dinner. Okay, my tips might, but say you're looking for a bit more variety. Now you can ask search to create a three day meal plan for a group that's easy to prepare. And here you get a plan with a wide range of recipes from across the web. This one for overnight oats looks particularly interesting. And you can easily head over to the website to learn how to prepare them. If you want to get more veggies in, you can simply ask search to swap in a vegetarian dish. And just like that, search customizes your meal plan. And you can export your meal plan or get the ingredients as a list just by tapping here. Looking ahead, you can imagine asking Google to add everything to your preferred shopping cart. Then we're really cooking. These planning capabilities mean search will be able to help plan everything from meals and trips to parties, dates, work out routines, and more. So you can get all the fun of planning without any of the hassle. You've seen how Google search can help with increasingly complex questions and planning. But what about all those times when you don't know exactly what to ask? And you need some help brainstorming. When you come to search for ideas, you'll get more than an AI generate answer. You'll get an entire AI organized page, custom built for you, end your question. Say you're heading to Dallas to celebrate your anniversary and you're looking for the perfect restaurant. What you get here breaks AI out of the box and it brings it to the whole page. Our Gemini model, it covers the most interesting angles for you to explore and organizes these results into these helpful clusters. Like you might never have considered restaurants with live music or ones with historic charm. Our model even uses contextual factors like the time of the year. So since it's warm in Dallas, you can get rooftop patios as an idea. And it pulls everything together into a dynamic whole page experience. You'll start to see this new AI organized search results page when you look for inspiration, starting with dining and recipes, and coming to movies, music, books, hotels, shopping, and more. Today you've seen how you can bring any question to search and Google takes the work out of searching. But your questions aren't limited to words in a text box and sometimes even that picture can't tell the whole story. Earlier, Demis showed you our latest advancements in video understanding. And I'm really excited to share that soon you'll be able to ask questions with video right in Google search. Let me introduce Rose to show you this in a live demo. Thank you, Liz. I have always known a record player. And I got this one and some finals that you're still recently.","Google search will introduce several new capabilities focused on planning and organizing information, particularly through the use of its custom Gemini model. Here are the key features:

1. **Multi-Step Reasoning**: Google search will utilize multi-step reasoning to break down complex questions into manageable parts, allowing it to provide comprehensive answers. For example, users can ask for detailed information about yoga and Pilates studios in Boston, including ratings, introductory offers, and walking distances from specific locations.

2. **Meal Planning**: Users will be able to request meal plans tailored to their needs. For instance, they can ask for a three-day meal plan that is easy to prepare, and the search will generate a variety of recipes from across the web. Users can also customize their meal plans by swapping in vegetarian dishes or exporting ingredient lists.

3. **Dynamic AI-Organized Pages**: When users are unsure of what to ask, Google search will provide AI-organized pages that cover various angles related to their queries. For example, if someone is looking for restaurants in Dallas for an anniversary, the search results will be organized into helpful clusters, suggesting options like restaurants with live music or rooftop patios.

4. **Planning for Various Activities**: The capabilities will extend beyond meal planning to include assistance with planning trips, parties, workout routines, and more. This means users can enjoy the fun of planning without the associated hassle.

5. **Contextual Suggestions**: The search will consider contextual factors, such as the time of year, to provide relevant suggestions, enhancing the overall user experience.

Overall, these advancements aim to make Google search a more powerful tool for organizing and planning various aspects of users' lives, streamlining the process and saving time."
What is the primary goal of DeepMind as stated by its co-founder?,"Thanks, and it's so great to be here. Ever since I was a kid playing chess for the England Junior Team, I'd been thinking about the nature of intelligence. I was captivated by the idea of a computer that could think like a person. It's ultimately why it became a programmer and studied neuroscience. I co-founded DeepMind in 2010 with the goal of one day building AGI, Artificial General Intelligence, a system that has human level cognitive capabilities. I've always believed that if we could build this technology responsibly, its impact would be truly profound and it could benefit humanity in incredible ways. Last year, we reached a milestone on that path when we formed Google DeepMind, combining AI talent from across the company into one super unit. Since then, we built AI systems that can do an amazing range of things, from turning language and vision into action for robots, navigating complex virtual-threading environments, solving a limpiav level math problems, and even discovering thousands of new materials. Just last week, we announced our next-generation alpha-fold model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. All of this was made possible with the best infrastructure for the AI era, including our highly optimized tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multi-modal because that's how we interact with and understand the world around us. We've built a variety of models for different use cases, you've seen how powerful Gemini 1.5 Pro is, but we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale while still featuring multi-modal reasoning capabilities and breakthrough-long context. Flash is optimized for tasks where low latency and efficiency matter most. Starting today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI, and developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. You'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models, but we're always striving to push the state of the art even further. At any one time, we have many different models in training, and we use our very large and powerful ones to help teach and train our production-ready models. Together with user feedback, this cutting-edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multi-modal AI. And this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multi-modal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action, and it would have to be proactive, teachable and personal, so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multi-modal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called?
We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.","The primary goal of DeepMind, as stated by its co-founder, is to build Artificial General Intelligence (AGI), a system that has human-level cognitive capabilities. The co-founder believes that if this technology is built responsibly, its impact could be profoundly beneficial for humanity."
What are the key features of the newly introduced Gemini 1.5 Flash model?,"Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response. And it's OK if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to aisstudio.google.com, log in with your Google account, and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit run. Now, in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, it starts streaming it back. Now, this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from Notebook LM, like we showed earlier. Now, what's great about this is that you can take something like this in AI Studio, prototype tier in 10 seconds. And with one click in the upper left, get an API key, or over here in the upper right, just tap get code. And you've got all of the model configurations, the safety settings, ready to go, straight into your IDE. Now, over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way, you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. OK. OK. APPLAUSE Now, let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now, since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. APPLAUSE It's optimized for a range of image captioning, visual Q&A, and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model, but it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on NextGen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. APPLAUSE So wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look.","The key features of the newly introduced Gemini 1.5 Flash model include:

1. **Speed**: Designed for quick tasks where the speed of the model is a priority, making it suitable for applications requiring low latency.

2. **Natively Multimodal**: Supports interleaving text, images, audio, and video as inputs, allowing for versatile input types.

3. **Large Context Window**: Offers a massive context window of up to 1 million tokens, with an option to try a 2 million token context window for the 1.5 Pro model.

4. **Developer Features**: Introduces new developer features such as:
   - Video frame extraction in the Gemini API.
   - Parallel function calling, enabling multiple function calls to be returned simultaneously.
   - Context caching, which allows users to send files to the model once and avoid resending them repeatedly.

5. **Cost-Effective**: Priced at 35 cents per 1 million tokens, making it an affordable option for developers.

6. **Integration with AI Studio**: Easily accessible through AI Studio, where users can log in with their Google account and select the model without needing extensive configuration.

These features make Gemini 1.5 Flash a powerful tool for developers looking for fast and efficient AI solutions."
What significant milestone was reached by Google DeepMind last year?,"Thanks, and it's so great to be here. Ever since I was a kid playing chess for the England Junior Team, I'd been thinking about the nature of intelligence. I was captivated by the idea of a computer that could think like a person. It's ultimately why it became a programmer and studied neuroscience. I co-founded DeepMind in 2010 with the goal of one day building AGI, Artificial General Intelligence, a system that has human level cognitive capabilities. I've always believed that if we could build this technology responsibly, its impact would be truly profound and it could benefit humanity in incredible ways. Last year, we reached a milestone on that path when we formed Google DeepMind, combining AI talent from across the company into one super unit. Since then, we built AI systems that can do an amazing range of things, from turning language and vision into action for robots, navigating complex virtual-threading environments, solving a limpiav level math problems, and even discovering thousands of new materials. Just last week, we announced our next-generation alpha-fold model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. All of this was made possible with the best infrastructure for the AI era, including our highly optimized tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multi-modal because that's how we interact with and understand the world around us. We've built a variety of models for different use cases, you've seen how powerful Gemini 1.5 Pro is, but we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale while still featuring multi-modal reasoning capabilities and breakthrough-long context. Flash is optimized for tasks where low latency and efficiency matter most. Starting today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI, and developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. You'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models, but we're always striving to push the state of the art even further. At any one time, we have many different models in training, and we use our very large and powerful ones to help teach and train our production-ready models. Together with user feedback, this cutting-edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multi-modal AI. And this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multi-modal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action, and it would have to be proactive, teachable and personal, so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multi-modal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called?
Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years, we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensile processing units that power our AI advances, Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs, we are proud to offer CPUs and GPUs to support any workload that includes the new Axion processes we announced last month, our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge black-well GPUs available in early 2025. We are fortunate to have a long-standing partnership with NVIDIA, excited to bring black-wells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI Hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying the raw hardware and chips. Our AI Hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We've been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly one gigawatt and growing. That's close to 70 times the capacity of any other fleet. Undroline this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers using better context, location awareness, and real time information. With advances in natural language understanding and computer vision, we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course, search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities, our high bar for information quality, our decades of experience connecting you to the richness of the web, the result is a product that does the work for you. Google search is a generative AI at the scale of human curiosity and it's our most exciting chapter of search yet. To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search.","Last year, Google DeepMind reached a significant milestone by forming Google DeepMind, which combined AI talent from across the company into one super unit."
"How does the new generative video model, VO, enhance video creation?","We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.
This is what excites me. As a hip-hop producer, we dug in the crates. We play these vinyls, and the part with it is no vocal, we pull it, we sample it, and we create an entire song around that. Right now we dig in an infinite crate. It's endless. Where I found the AI really useful for me, this way to fill in the sparser elements of my loops. Okay, let's try Bongo's. We're going to put Viola. We're going to put Rhythnic clapping. And we're going to see what happens there. And it makes it sound ironically at the end of the day a little more human. So then this is entirely Google's loops right here. These are glutes. It's like having like this weird friend that's just like, try this, try that. And then you're like, oh, okay, yeah, no, that's pretty dope. It's a group in, group in, group in, group in, up, up, up. The tools are capable, speeding up the process of what's in my head getting it out. You're able to move light speed with your creativity. This is amazing. Right there. I think this really shows what's possible when we work with the artist's community on the future of music. You can find some brand new songs from these acclaimed artists and songwriters on their YouTube channels now. There's one more area I'm really excited to share with you. Our teams have made some incredible progress in genitive video. Today, I'm excited to announce our newest, most capable genitive video model called VO. VO creates high quality 1080p videos from text, image and video prompts. It can capture the details of your instructions in different visual and cinematic styles. You can prompt for things like aerial shots of a landscape or time lapse and further edit your videos using additional prompts. You can use VO in our new experimental tool called Video FX. We're exploring features like storyboarding and generating longer scenes. VO gives you unprecedented creative control. Techniques for generating static images have come a long way, but generating video is a different challenge altogether. Not only is it important to understand where an object or subject should be in space, it needs to maintain this consistency over time, just like the car in this video. VO builds upon years of our pioneering genitive video model work, including GQN, FNACI, WALT, Video Poet, Lumiere and much more. We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into a video model. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it, right? As much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today.","The new generative video model, VO, enhances video creation by enabling high-quality 1080p videos to be generated from text, image, and video prompts. It captures the details of user instructions in various visual and cinematic styles, allowing for prompts like aerial shots or time-lapse sequences. VO also offers unprecedented creative control, enabling users to edit videos further using additional prompts.

Key features of VO include:

1. **Improved Consistency and Quality**: VO builds upon previous generative video models to enhance the consistency and quality of the output.
2. **Faster Visualization**: It allows for the visualization of ideas on a time scale that is 10 to 100 times faster than traditional methods.
3. **Creative Flexibility**: Users can experiment and make mistakes quickly, which is essential in the creative process.
4. **Multi-modal Capabilities**: By utilizing Gemini's multi-modal capabilities, VO better captures nuances from prompts, incorporating cinematic techniques and visual effects.
5. **Storytelling Focus**: The model emphasizes storytelling, encouraging everyone to become a director and share their stories creatively.

Overall, VO represents a significant advancement in generative video technology, making it easier for creators to bring their ideas to life and explore new creative possibilities."
What is Project Astra and what capabilities does it aim to develop?,"Yes. Let's see what it says. Hey, my name is Michael Chang and we're watching the Google IO keynote while it's happening with Project Astra. Let's take a look. Hey, Gemini, currently watching Google IO keynote right now. You want to watch it with me? Sure, I'd love to watch it with you. What's happening in the keynote? What does it look like? What does it look like they're announcing right now? It looks like they're announcing Gemini 1.5 Flash. The speaker is talking about its speed and efficiency in comparison to the pro model. What are the other features? It looks like Gemini 1.5 Flash is optimized for tasks where low latency and high efficiency matter most. Cool. The screen also shows. Oh, yeah, sorry, I interrupted you. Keep going. That's all right. What were we talking about? Well, we were talking about the capabilities of 1.5 Flash. Do you remember? Yes, we were discussing the new Gemini model and its efficiency and speed. The presentation seems to have moved on though. What would you like to talk about now? Well, I'm curious about what he's going to talk about next. He seems to be moving on to the topic of model training and feedback. Perhaps they will talk about how user input helps train the models. It looks like he just announced a new project. What is that project called? They are calling that project, Project Astra. It looks like they are calling that project Astra. It looks like they are calling that project Astra. It looks like they are calling that project Astra. It looks to focus on the future of AI assistance. What do you think a project Astra will be helpful for? It sounds like a very ambitious project. It will be interesting to see what kind of AI assistant they create and how it integrates with our daily lives. They seem to say it's a universal agent. What would you use a universal agent for? A universal agent that can help with everyday tasks would be helpful. It could potentially be useful for things like scheduling, finding information, or even controlling smart home devices. Oh, that would be super cool. I agree it sounds amazing. Having a seamless and intuitive AI agent could revolutionize how we interact with technology. Yeah, can you summarize what he showed so far? The speaker introduced Project Astra, an AI-powered universal agent for everyday tasks that is still under development. They discussed the challenges of building this technology and how their team is working toward making Astra helpful and intuitive for users. Wow, amazing. And what did he talk about before Project Astra? He was discussing the Gemini 1.5 model, particularly Gemini 1.5 Flash, which focuses on efficiency and speed. The presentation was focused on the future direction of AI models and user feedbacks roll in training those models. It looks like they're now doing a demo of Project Astra. What does it look like they're doing right now? In the video, someone is holding a phone and panning across the table. The phone is running Project Astra, which can identify a speaker and a cup of crayons. And what is it doing now? It appears to be scanning a computer screen displaying code. Astra then explains what the highlighted section of the code is doing. Wow, that's super cool. Thanks for explaining all of these things to me, Gemini. You're very welcome. Is there anything else I can explain about the presentation? Let's just keep on watching. What's currently happening? In the video, Astra is identifying the King's Cross Neighborhood of London using the phone's camera view. It's also providing some facts about the area. Wow. I can't believe this is...this technology exists right now. It does look quite impressive. Astra seems very advanced at interpreting what it sees and providing relevant information. That was a lot of fun to watch. I'm looking forward to Project Astra as I'm sure all of you are too. Thank you for being here. Thank you for tuning in. Let's go build something amazing!
And this is not just the model completing the code. It's the model coming up with an entire plan and running it. And not only do I get an answer to my initial question about the data set, but the AI agent also produces a fully functional collab notebook based on your instructions. And you can share it with others, allowing for real time collaboration on code, new visualizations, and all of the results. It's available today at labs.google slash code and let us know what you think about it. Now you see why I'm so excited today. You see, we're committed to empowering you to build with genitive AI boldly and responsibly from Gemini power assistance in your favorite IDEs to easy access for our most powerful Gemini model. And it comes with flexibility to use Gemma open models. You have an opportunity to reach anyone anywhere through Google's ecosystem of devices, app experiences, and platform. We continued investing resources to give your projects a boost. Google accelerators and the Google developer portfolio is just a couple of great examples. Our accelerators have guided over 1,300 startups in 84 countries with countless success stories. And that includes 20 startups that have grown to become unicorns. You Genie AI founded by Dr. Sudeth Boyd, it's a recent graduate of our climate change accelerator. You Genie was born out of Dr. Childry's desire to use AI for good. His team helps manufacturing companies to dramatically reduce their carbon emissions. And you Genie's algorithms apply Google DeepMind research which has helped to optimize energy use in Google's own data centers. You Genie was able to streamline their model development and their analytics with accurate emissions data through Google Earth Engine. We love working alongside startups to tackle big problems and invite all of you to apply to an upcoming accelerator soon. If you have one of the 15 million Google developer profiles, you enjoy invitations for early access programs, a chance to participate in research and opportunities to attend special events. Beginning today, your profile is evolving into something even better. Introducing the Google Developer Program. Members of the program can now access new benefits at no cost and I'm going to take you through them. First, you can access Gemini for learning and searching and chatting with documentation. Second, for IDX users, we're now enabling you to go from two work stations up to five. Third, if you're in the Google Cloud Innovators community, you get credits for interactive labs on Google Cloud Skills Boost. Sign up for the Google Developer Program today or if you have an existing profile, you're all set. More benefits are coming soon so be on the lookout. That brings us to the close of today's developer keynote but we're not quite done. There's a lot more to Google I.O than just the keynotes. If you're with us in person, attend the sessions, the workshops, and demos both today and tomorrow. If you're online, you'll find all the sessions available on IO.Google starting this Thursday morning, Mountain View Time. Last year, we kicked off the series of IO connect events to bring the in-person experience from Shoreline to more places that are closer to you. Well, we heard just how much you loved them so we've got three of them lined up this year. Starting in Berlin on June 27th and then we're in Bangalore in July and Beijing in August. And I'll mention, I'll be at all of them so I hope to see you there too. If you can join us at IO connect, come to one of the many community-led IO extended event happening in your local area. Now, I like to share something really cool with you. We had Project Astra watch the keynote this morning. Yes. Let's see what it says. Hey, my name is Michael Chang and we're watching the Google IO keynote while it's happening with Project Astra. Let's take a look. Hey, Gemini, currently watching Google IO keynote right now. You want to watch it with me? Sure, I'd love to watch it with you. What's happening in the keynote? What does it look like? What does it look like they're announcing right now? It looks like they're announcing Gemini 1.5 Flash. The speaker is talking about its speed and efficiency in comparison to the pro model. What are the other features? It looks like Gemini 1.5 Flash is optimized for tasks where low latency and high efficiency matter most. Cool. The screen also shows. Oh, yeah, sorry, I interrupted you. Keep going.","Project Astra is an AI-powered universal agent introduced during the Google I/O keynote. It aims to assist users with everyday tasks by providing a seamless and intuitive interaction with technology. The capabilities it seeks to develop include:

1. **Task Optimization**: Astra is designed to be efficient and effective in handling tasks that require low latency and high efficiency.

2. **Real-Time Information**: It can identify objects and provide relevant information based on what it sees through a camera, such as identifying locations and offering facts about them.

3. **User Interaction**: Astra is intended to integrate user input to improve its functionality and responsiveness, potentially revolutionizing how users interact with AI.

4. **Collaboration**: The project also emphasizes real-time collaboration, allowing users to share and work on projects together.

Overall, Project Astra is focused on creating an advanced AI assistant that can enhance daily life by simplifying various tasks and providing useful information in real-time."
What is the significance of the Gemini 1.5 Pro model introduced at Google I/O 2024?,"Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
A Google we are fully in our Gemini era. Today all of our two billion user products use Gemini. Gemini 1.5 Pro is available today in Workspace Labs. Let's see how this comes to life with Google Workspace. People are always searching their emails and Gmail. We are working to make it much more powerful with Gemini. Now we can ask Gemini to summarize all recent emails from the school. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. People love using photos to search across their life. With Gemini, you're making that a whole lot easier. And ask photos can also help you search your memories in a deeper way. For example, you might be reminiscing about your daughter Lucius, early milestones. You can ask photos, show me how Lucius swimming has progressed. Here Gemini goes beyond a simple search, recognizing different contexts and photos packages it up all together in a summary. Unlocking knowledge across formats is why we build Gemini to be multimodal from the ground up. It's one model with all the modalities built in. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. So today, we are expanding the context window to two million tokens. So far, we've talked about two technical advances, multimodality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. But what if we could go even further? That's one of the opportunities we see with AI agents. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Here's a video of our prototype, which we'll see has two parts. Each part was captured in a single take in real time. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector, IV. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. Give me a banner name for this duo. Golden Stripes. Nice. Thanks, Gemini. Today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context. There's one more area I'm really excited to share with you. Our teams have made some incredible progress in genitive video. Today, I'm excited to announce our newest, most capable genitive video model, called VO. VO creates high quality 1080p videos from text, image and video prompts. It can capture the details of your instructions in different visual and cinematic styles. For 25 years, we have invested in world-class technical infrastructure. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. Google Search is a generative AI at the scale of human curiosity, and it's our most exciting chapter of Search yet. All the advancements you'll see today are made possible by a new Gemini model, customized for Google Search. What really sets this apart is our three unique strengths. This is Search in the Gemini era. By the end of the year, AI overviews will come to over a billion people. We're making AI overviews even more helpful for your most complex questions. The types that are really more like 10 questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. I'm really excited to share that soon you'll be able to ask questions with video. Why will this not stay in place? In the near-instead, Google gives me an AI overview. I guess some reasons this might be happening and steps I can take to troubleshoot.","The Gemini 1.5 Pro model introduced at Google I/O 2024 is significant for several reasons:

1. **Multimodal Capabilities**: Gemini 1.5 Pro is designed to handle multiple types of inputs, including text, images, audio, and video, allowing developers to create more versatile applications that can process and respond to various forms of data.

2. **Long Context Window**: The model features an extended context window of up to 2 million tokens, enabling it to maintain context over longer interactions and process more extensive information, which is particularly beneficial for complex tasks.

3. **Quality Improvements**: The 1.5 Pro model includes a series of quality enhancements aimed at providing higher quality responses, making it suitable for tasks where accuracy and depth of information are critical.

4. **Cost Efficiency**: Google has structured pricing to make the model accessible, with costs set at $7 per million tokens and a reduced rate of $3.50 for prompts up to 128K tokens, which encourages developers to utilize the model for various applications.

5. **Developer Features**: The introduction of new developer features, such as video frame extraction, parallel function calling, and context caching, enhances the usability of the model, allowing developers to build more efficient and powerful applications.

6. **Integration with Google Cloud**: The model is integrated with Google Cloud services, enabling developers to scale their applications as needed and access enterprise-grade features through Vertex AI.

Overall, Gemini 1.5 Pro represents a significant advancement in AI capabilities, providing developers with powerful tools to create innovative applications that leverage multimodal data and long context processing."
How many developers are currently using Gemini models across Google's tools?,"Welcome to the 16th Google I.O. I want to thank our fans, have developed our community all millions of you around the world for choosing to build with Google. Literally, whatever you can imagine, you can bring it to life because only Google's ecosystem gives you the potential to reach people on 3 billion Android devices and 2 billion Chrome and Chromium-based browsers. You've created millions of helpful apps with Firebase, Google Cloud, and generative AI models like Gemini and Gemma. At Google, we're on a mission to make generative AI accessible to every developer on the planet. It's transforming the very fundamentals of software development, including new techniques for how you build and new inspiration for what you build. AI helps you to be more productive by assisting you with many different development tasks, such as writing, debugging, and testing your code, or generating documentation, and understanding your code base. That's why Gemini is available to you, all developers to use in Android Studio, Chrome Dev Tools, Project IDX, CoLab, Beath Code, IntelliJ, and Firebase. CHEERING AND APPLAUSE Gemini is even more helpful with context such as app settings, performance data, your logs, and source code. This is just a glimpse of AI changing how we built. Now, let's go to what you built. Today, people just assumed that things will work great. Of course, any platform, any device that they use, literally anywhere they are, whether on the go, at home, or at work, in their local town, or on the other side of the world. At the same time, the tools that you use need to make that possible and they're constantly evolving. It's great to see the latest technologies come out, but that means more complexity, especially with so many different AI concepts to learn. We want to make your lives simpler. As code becomes content and coders become creators, whether you're building an AI-powered mobile app, a web app, or an interactive experience that works across all platforms, we're providing you with powerful, more integrated development tools, both from Google and the OpenEka system, and it comes with secure, easy access to Google's most capable AI models and global cloud infrastructure. Ultimately, it's about helping you to stay focused on what you want to build, amazing products and services, and not the plumbing, which takes me to our agenda for today. Up next, you'll hear more in how you can use the Gemini API in Google AI Studio to develop engaging and multimodal apps. Then we will take you through how Gemini helps you to create a whole new category of AI-powered experiences on Android and Web, with exceptional levels of productivity. We've also been hard at work creating the very best experienced, full-stack, multi-platform development. Our team will share what's now possible with Project IDX, Flutter, and Firebase. And we think that will dramatically expand what you can build. Then we will talk about how you can build and customize your own AI models with Gemma. You'll see how CoLab, Carrison Jacks, make it really easy. Finally, I'll be back up here to share one more thing before we wrap it up. Are you ready to jump in? OK. Awesome. Jacqueline, come join us. Our investment in our AI research and infrastructure allows us to put the power of AI directly into your hands. We make it easy to start building an AI app with a simple API and a tool that can be used to create a new product. It can be used to create an AI app with a simple API integration. So you can focus on building the best possible products for your users. Now, meeting many of you at Hackathons and Meetups, I am blown away by how you're pushing the boundaries with our new AI models. From new ideas using audio and video inputs, the creative use of longer context windows. It's really inspiring to see. And like you heard about earlier today, balance across quality, cost, and speed. That's why I'm thrilled that Gemini 1.5 Flash is officially open to all developers to start building with today. You can get started using the Gemini API in Google AI Studio, which is now available in more than 200 countries and territories. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. But what I'm seeing more and more these days is that it isn't just about selecting the right model, but also the right techniques to bring your ideas to life.
Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in monks than is happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. So, we are in the very early days of the AI platform shift. We see so much opportunity ahead for creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too, including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries, as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. Applause There's so much innovation happening in search. Thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.",More than 1.5 million developers are currently using Gemini models across Google's tools.
What new feature was introduced in Google Photos that utilizes Gemini's capabilities?,"One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. The so much innovation happening in search, thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago. Since then, people have used it to organize their most important memories. Today, that amounts to more than 6 billion photos and videos uploaded every single day. And people are using photos to search across their life. With Gemini, you're making that a whole lot easier. Say you're at a parking station ready to pay, but you can't recall your license plate number. Before you could search photos for keywords, then scroll through years worth of photos looking for the right one. Now, you can simply ask photos. It knows the cards that appear often, it triangulates which one is yours, and just tells you the license plate number. And ask photos can also help you search your memories in a deeper way. For example, you might be reminiscing about your daughter Lucius early milestones. You can ask photos when did Lucius learn to swim. You can even fall up with something more complex. Show me how Lucius swimming has progressed. Here, Gemini goes beyond a simple search, recognizing different contexts from doing laps in the pool, to snorkeling in the ocean, to the texture and dates on her swimming certificates, and photos packages it up all together in a summary. You can really take it all in and relive amazing memories all over again. We are rolling out asked photos this summer with more capabilities to come. APPLAUSE Unlocking knowledge across formats is why we build Gemini to be multi-model from the ground up. It's one model with all the modalities built in. So not only does it understand each type of input, it finds connections between them. Multi-modality radically expands the questions we can ask, and the answers we will get back. Long context takes this a step further, enabling us to bring in even more information, hundreds of pages of text, hars of audio, a full hour of video, or entire code repos. Or if you want, roughly 96 cheesecake factory menus. For that many menus, you need a one-million token context window, now possible with Gemini 1.5 Pro. Developers have been using it in super interesting ways. Let's take a look. I remember the announcement, the one-million token context window, and my first reaction was, there's no way they were able to achieve this. I wanted to test this technical skills. So I uploaded a line chart. It was temperatures between Tokyo and Berlin, and how they were across the 12 months of the year. So I got in there, and I threw in the Python library that I was really struggling with. And I just asked it a simple question. And it nailed it. It could find specific references, the comments in the code, and specific requests that people had made, and other issues that people had had, but then suggest a fix for it, that related to what I was working on. I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model.
Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Oops! Reacts. Things got bigger. Like way bigger. And it wasn't all just for him, or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. Still, we are in the very early days of the AI platform shift. We see so much opportunity ahead. For creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too. Including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. The so much innovation happening in search, thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.","The new feature introduced in Google Photos that utilizes Gemini's capabilities is called ""Ask Photos."" This feature allows users to search for specific information within their photos by asking questions. For example, users can inquire about their license plate number or ask about their child's milestones, and Gemini will provide relevant information by recognizing different contexts and retrieving the appropriate photos."
What are the two key technical advances discussed at the event that enhance Gemini's capabilities?,"We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example. My sister is a self-employed photographer, and her inbox is full of appointment bookings, receipts, client feedback on photos, and so much more. Now, if you're a freelancer or a small business, you really want to focus on your craft and not on bookkeeping and logistics. So, let's go to her inbox and take a look. Lots of unread emails. Let's click on the first one. It's got a PDF. That's an attachment from a hotel as a receipt, and I see a suggestion in the side panel. Help me organize and track my receipts. Let's click on this prompt. The side panel now will show me more details about what that really means. And as you can see, there's two steps here. Step one. Create a drive folder and put this receipt and 37 others it's found into that folder. Make sense. Step two. Extract the relevant information from those receipts in that folder into a new spreadsheet. Now, this sounds useful. Why not? I also have the option to edit these actions, or just hit OK. So, let's hit OK. Gemini will now complete the two steps described above, and this is where it gets even better. Gemini offers you the option to automate this so that this particular workflow is run on all future emails, keeping your drive folder and expense sheet up to date with no effort from you. Now, we know that creating a complex spreadsheet like this can be daunting for most people. But with this automation, Gemini does the hard work of extracting all the right information from all the files and in that folder and generates this sheet for you. So, let's take a look. OK, it's super well organized and it even has a category for expense type. Now, we have the sheet. Things can get even more fun. We can ask Gemini questions, questions like show me where the money spent. Gemini not only analyzes the data from the sheet, but also creates a nice visual to help me see the complete breakdown by category. You can imagine how this extends to all sorts of use cases in your inbox like travel expenses, shopping, remodeling projects, you name it. All of that information in Gmail can be put to good use and help you work, plan, and play better. Now, this particular... I know. This particular ability to organize your attachments and drive and generate a sheet and do data analysis via Q&A will be rolling out to labs users this September. And it's just one of the many automations that we're working on in workspace. Workspace in the Gemini era will continue to unlock new ways of getting things done. We're building advanced agent of experiences, including customizing how you use Gemini. Now, as we look to 2025 and beyond, we're exploring entirely new ways of working with AI. Now, with Gemini, you have an AI-powered assistant always at your side. But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs, or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and context to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives.","The two key technical advances discussed at the event that enhance Gemini's capabilities are:

1. **Multimodal Interaction**: Gemini is natively multimodal, allowing users to interact using text, voice, or their phone's camera. This enables a more natural way of expressing oneself and enhances the user experience.

2. **Live Interaction and Real-Time Understanding**: Gemini can engage in live conversations using voice, leveraging Google's latest speech models to better understand users and respond naturally. This includes the ability to interrupt while Gemini is responding, adapting to the user's speech patterns, and utilizing video understanding capabilities to respond to surroundings in real time."
What is the ultimate goal of Google's AI initiatives as presented at the event?,"This is the end of the video. Thank you. Listening to everything that has been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context, and agents are the cutting edge of what this technology can do, take it to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners, and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting-edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines, and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine three NVO create more realistic imagery and videos. We must also consider how they might be misused to spread misinformation. To help last year, we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today, we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Gemini, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making. And in the coming months, we'll be open sourcing Synth ID text watermarking. This will be available in our updated responsible Gerritive AI Toolkit, which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supposed to appear collaborating with Adobe, Microsoft, Startups, and many others to build that implement standards that improve the transparency of digital media. Now, let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today, our AI advances are helping to solve real-world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases, helping predict floods in more than 80 countries, and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube.
We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.","The ultimate goal of Google's AI initiatives, as presented at the event, is to build AI responsibly while maximizing its benefits for people and society. This involves addressing the risks associated with AI technologies, improving models to protect against misuse, and enhancing their capabilities through innovative techniques like AI-assisted red teaming. Additionally, Google's AI advancements aim to solve real-world problems, such as aiding scientists, predicting natural disasters, and making information universally accessible for learning. The introduction of tools like LearnLam, which personalizes learning experiences, exemplifies their commitment to using AI to enhance education and empower individuals. Overall, the focus is on creating AI that is safe, effective, and beneficial for all."
What is the purpose of Google's new Gemini Developer Contest?,"Hey! You may recognize me for my acting work, but my real passion is programming. When I'm in the zone, I'm unstoppable. I don't know if you know this about me, but I'm really excited about the future. And Google's Gemini models are a tech that can get it there. And you know I'm submitting to Google's new Gemini Developer Contest, which is so cool, it'll blow your mind back to beta. A chance to win for the most creative, useful and remarkable apps. If you're a dev or a dev team that wants to submit great, but you'll have some catching up to do. I just drank three cups of hydrochol brew, and I'm ready to crank out some serious. Time for the wormhole!
Welcome to the 16th Google I.O. I want to thank our fans, have developed our community all millions of you around the world for choosing to build with Google. Literally, whatever you can imagine, you can bring it to life because only Google's ecosystem gives you the potential to reach people on 3 billion Android devices and 2 billion Chrome and Chromium-based browsers. You've created millions of helpful apps with Firebase, Google Cloud, and generative AI models like Gemini and Gemma. At Google, we're on a mission to make generative AI accessible to every developer on the planet. It's transforming the very fundamentals of software development, including new techniques for how you build and new inspiration for what you build. AI helps you to be more productive by assisting you with many different development tasks, such as writing, debugging, and testing your code, or generating documentation, and understanding your code base. That's why Gemini is available to you, all developers to use in Android Studio, Chrome Dev Tools, Project IDX, CoLab, Beath Code, IntelliJ, and Firebase. CHEERING AND APPLAUSE Gemini is even more helpful with context such as app settings, performance data, your logs, and source code. This is just a glimpse of AI changing how we built. Now, let's go to what you built. Today, people just assumed that things will work great. Of course, any platform, any device that they use, literally anywhere they are, whether on the go, at home, or at work, in their local town, or on the other side of the world. At the same time, the tools that you use need to make that possible and they're constantly evolving. It's great to see the latest technologies come out, but that means more complexity, especially with so many different AI concepts to learn. We want to make your lives simpler. As code becomes content and coders become creators, whether you're building an AI-powered mobile app, a web app, or an interactive experience that works across all platforms, we're providing you with powerful, more integrated development tools, both from Google and the OpenEka system, and it comes with secure, easy access to Google's most capable AI models and global cloud infrastructure. Ultimately, it's about helping you to stay focused on what you want to build, amazing products and services, and not the plumbing, which takes me to our agenda for today. Up next, you'll hear more in how you can use the Gemini API in Google AI Studio to develop engaging and multimodal apps. Then we will take you through how Gemini helps you to create a whole new category of AI-powered experiences on Android and Web, with exceptional levels of productivity. We've also been hard at work creating the very best experienced, full-stack, multi-platform development. Our team will share what's now possible with Project IDX, Flutter, and Firebase. And we think that will dramatically expand what you can build. Then we will talk about how you can build and customize your own AI models with Gemma. You'll see how CoLab, Carrison Jacks, make it really easy. Finally, I'll be back up here to share one more thing before we wrap it up. Are you ready to jump in? OK. Awesome. Jacqueline, come join us. Our investment in our AI research and infrastructure allows us to put the power of AI directly into your hands. We make it easy to start building an AI app with a simple API and a tool that can be used to create a new product. It can be used to create an AI app with a simple API integration. So you can focus on building the best possible products for your users. Now, meeting many of you at Hackathons and Meetups, I am blown away by how you're pushing the boundaries with our new AI models. From new ideas using audio and video inputs, the creative use of longer context windows. It's really inspiring to see. And like you heard about earlier today, balance across quality, cost, and speed. That's why I'm thrilled that Gemini 1.5 Flash is officially open to all developers to start building with today. You can get started using the Gemini API in Google AI Studio, which is now available in more than 200 countries and territories. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. But what I'm seeing more and more these days is that it isn't just about selecting the right model, but also the right techniques to bring your ideas to life.","The purpose of Google's new Gemini Developer Contest is to provide a chance for developers or development teams to submit creative, useful, and remarkable applications using Google's Gemini models. The contest aims to encourage innovation and recognize outstanding contributions in app development, with the potential to win prizes for the best submissions."
What are the key features of the Gemini models mentioned in the context?,"We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son and I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, she really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that of course means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes, the foundation for understanding how objects move and interact. Ah, yes, this is where multimodal really shines. Now it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah, can you give my son Jimmy a basketball example? Hey, Jimmy, that's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay, so first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say, the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity or Isaac Newton. But nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format. And it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Shar. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead. Work across software and systems. All to get something done on your behalf. And most importantly, under your supervision. We are still in the early days and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes. And a lot less fun to return them when they don't fit. Imagine if Gemini could do all the steps for you.","The key features of the Gemini models mentioned in the context include:

1. **Multimodal Interaction**: Gemini allows users to interact using text, voice, or camera, enabling a natural expression of thoughts and commands.

2. **Live Conversations**: Users can have in-depth conversations with Gemini using their voice, with the ability to interrupt and adapt to speech patterns.

3. **Customization with Gems**: Users can create personalized ""gems"" that act as personal experts on specific topics, allowing for tailored interactions and assistance.

4. **Dynamic Trip Planning**: Gemini can assist in planning trips by gathering information from various sources (like Gmail and maps) and creating personalized itineraries based on user preferences and constraints.

5. **Audio Overviews**: In educational contexts, Gemini can generate audio discussions based on provided materials, making learning more engaging and interactive.

6. **AI Agents**: The models are designed to function as intelligent systems that can reason, plan, and remember, performing tasks on behalf of users while allowing for supervision.

7. **Integration with Workspace**: Gemini 1.5 Pro is integrated into Workspace Labs, enhancing productivity and creativity for users.

These features highlight Gemini's focus on enhancing user experience through personalization, interactivity, and intelligent assistance."
What is the significance of the phrase 'blow your mind back to beta' in relation to the contest?,"Hey! You may recognize me for my acting work, but my real passion is programming. When I'm in the zone, I'm unstoppable. I don't know if you know this about me, but I'm really excited about the future. And Google's Gemini models are a tech that can get it there. And you know I'm submitting to Google's new Gemini Developer Contest, which is so cool, it'll blow your mind back to beta. A chance to win for the most creative, useful and remarkable apps. If you're a dev or a dev team that wants to submit great, but you'll have some catching up to do. I just drank three cups of hydrochol brew, and I'm ready to crank out some serious. Time for the wormhole!
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The phrase ""blow your mind back to beta"" in relation to the contest suggests that the innovations and creative possibilities presented in Google's Gemini Developer Contest are so impressive and groundbreaking that they evoke a sense of nostalgia for the early, experimental stages of technology development (often referred to as ""beta""). It implies that the contest encourages developers to push the boundaries of creativity and utility in app development, reminiscent of the excitement and potential seen in the beta phases of tech products. This phrase emphasizes the transformative nature of the contest and the potential for participants to create remarkable applications that could redefine user experiences."
What does the speaker imply about their readiness to participate in the contest?,"Hey! You may recognize me for my acting work, but my real passion is programming. When I'm in the zone, I'm unstoppable. I don't know if you know this about me, but I'm really excited about the future. And Google's Gemini models are a tech that can get it there. And you know I'm submitting to Google's new Gemini Developer Contest, which is so cool, it'll blow your mind back to beta. A chance to win for the most creative, useful and remarkable apps. If you're a dev or a dev team that wants to submit great, but you'll have some catching up to do. I just drank three cups of hydrochol brew, and I'm ready to crank out some serious. Time for the wormhole!
Sabak Give me the hell out of here. Get me the hell out of this cup. Get me out. Yeah! Google! Google! Google! Google! Google! Ha ha ha! We all ready to do a little googling. Everyone get out your computers. It's time to have ourselves a nice Google. Siri Navigate to google.com. Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Are you awake? Have you had your coffee? Are you awake? Don't worry baby. Google's gonna wake you right up. Don't worry baby. Google! Google! Google! Google's gonna wake you right up. Bleep bloop, bloop, bloop, rappy bloop, bloop, bloop. Up time to get up. You silly little nerd's wake up. okay Google.com. What is it? What does it mean? Why are we here? No one knows. And you're not, you're not going to find out. Not today. Those answers? Not part of the presentation. Buckle up. I'm Mark Rabier. It's nice to meet you, nice to see you, nice to be here. In beautiful Honolulu Hawaii. Here for Google I.O. If you have no idea who I am, that's perfectly I would expect that. Not many people do, but I'm an improvisational musician. I make stuff up from scratch. And that's exactly what we're going to be doing this entire show. We have nothing planned. That includes the rest of the conference. None of this has been rehearsed. Sundar's going to come out, say some stuff. No, we don't know what's going to happen here today. They're making up these visuals right now. Someone's back there. So what I'm here to talk about is music. I'm here to talk to you about music. And in particular, a very exciting new technology that we've been developing here at Google.com. Music FX. DJ. That's right. No more do you need to haul around the case of records. No, the computer will take care of it. Music FX DJ is a generative artificial intelligence tool that will from scratch generate from nothing. A series of sounds as it interprets your prompts. I'll demonstrate that to you now. Utilizing technology at my fingertips. And utilizing the colossal power of Google's machine. Connecting to Google's machine now. Music. We'll start with a viola. How about that? Everyone loves a viola. And if you don't, I don't care. I don't care. You do now. Viola. No one wrote this. Google wrote this. Let's throw in something weird. Hip hop beat from the 808 drum machine. It's pretty tough actually. So we have the sort of melodic element of the viola still in there. We can pull that out. Chiptones. I think that's chiptones. Let's do it out. It's pretty tough. Pulling that out. The machine is in control now. And that's what it can do. And let me tell you something. Let me tell you something right now. That is just the beginning. You're going to be hearing that a lot at this keynote. That's just the beginning. Okay. Now you will choose the prompts and we'll leave it up to chance. As I said, everything here unscripted unplanned. I didn't know this was going to happen. What's this? I didn't even know you guys were going to be here. What are you guys doing here? Six props, bagpipes, Persian tar, jembe, dulcimer, flamenco guitar, Detroit techno. You know what those things are? You know what this is? You better. Shout them out. What do you want to hear? Flamenco guitar. Persian took bagpipes. No, no bagpipes. They said, no, get the bagpipes out of here. Okay. Persian tar, jembe, flamenco guitar. Jembe.","The speaker implies that they are very enthusiastic and prepared to participate in the contest. They express excitement about the future and the potential of Google's Gemini models, indicating a strong interest in the competition. The mention of having ""drank three cups of hydrochol brew"" suggests they are energized and ready to create something significant, emphasizing their eagerness to submit a remarkable app for the contest. Overall, the tone conveys confidence and readiness to engage in the challenge."
How does the speaker's background in acting relate to their passion for programming?,"I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!
Hey! You may recognize me for my acting work, but my real passion is programming. When I'm in the zone, I'm unstoppable. I don't know if you know this about me, but I'm really excited about the future. And Google's Gemini models are a tech that can get it there. And you know I'm submitting to Google's new Gemini Developer Contest, which is so cool, it'll blow your mind back to beta. A chance to win for the most creative, useful and remarkable apps. If you're a dev or a dev team that wants to submit great, but you'll have some catching up to do. I just drank three cups of hydrochol brew, and I'm ready to crank out some serious. Time for the wormhole!","The speaker, who is recognized for their acting work, expresses that their real passion lies in programming. They convey a sense of excitement about the future and the potential of Google's Gemini models, indicating that their enthusiasm for technology and programming is significant. The speaker's background in acting may contribute to their creativity and ability to engage an audience, which can be beneficial in programming, especially when developing innovative and user-friendly applications. They also mention their readiness to dive into programming with energy and determination, suggesting that their acting experience fuels their drive and creativity in the tech space."
What is the total number of Android devices and Chrome browsers mentioned in the keynote?,"You'll see these and work coming to hundreds of billions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, on-device foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel, later this year, we'll be expanding what's possible with our latest model Gemini Nano with multi-modality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed Talkback an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses Talkback will typically come across 90 unlabeled images per day. Thankfully, Talkback makes them accessible. And now we're taking that to the next level with the multi-modal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the styling cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to Talkback are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scandal continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello. I am calling from the Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh yeah, what kind of charges? I can't give you specifics over the phone, but to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. And look at this. My phone gives me a warning that this call might be a stab. Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone, thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools like Gemini Nano and Gemini in Android Studio. Also, stay tuned tomorrow for our upcoming Android 15 updates, which we can't wait to share with you. As we said at the outset, we're reimagining Android with Gemini at the core from your favorite apps to the OS itself. We're bringing the power of AI to every aspect of the smartphone experience. And with that, let me head over to Josh to share more on our use for developers. Thank you.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The provided context does not specify the total number of Android devices or Chrome browsers mentioned in the keynote. It only discusses features and capabilities related to Android and AI but does not provide specific numerical data regarding the number of devices or browsers. Therefore, I cannot provide an answer to the question based on the given information."
What new feature was announced for Gemini 1.5 Flash regarding its availability for developers?,"Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response. And it's OK if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to aisstudio.google.com, log in with your Google account, and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit run. Now, in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, it starts streaming it back. Now, this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from Notebook LM, like we showed earlier. Now, what's great about this is that you can take something like this in AI Studio, prototype tier in 10 seconds. And with one click in the upper left, get an API key, or over here in the upper right, just tap get code. And you've got all of the model configurations, the safety settings, ready to go, straight into your IDE. Now, over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way, you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. OK. OK. APPLAUSE Now, let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now, since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. APPLAUSE It's optimized for a range of image captioning, visual Q&A, and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model, but it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on NextGen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. APPLAUSE So wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look.","A new feature announced for Gemini 1.5 Flash is the ability for developers to use it to learn from customer feedback about their products. This is made possible due to Flash's low latency, allowing developers to input customer feedback into the model, load a prompt, and run it to extract themes and insights efficiently. Additionally, developers can try out both the 1.5 Pro and 1.5 Flash models to see which works best for their use cases."
What are the benefits of using Gemini models in Android Studio as highlighted in the keynote?,"It's important for us to be available on any formfactor so that people can listen to SoundCloud in any situation. Our team's mobile Compost guilds transfer directly to other formfactors, given that its concepts and most APIs are the same. We build our various and TV apps from the ground up in Compost in just 4 months, which would have been unimaginable with the old D system. Some parts of our foldable and tablet layouts and the whole login experience of our car are built in Compost. Being available across formfactors has allowed us to reach new people and markets, which ultimately increases our global reach. It is so exciting to see how millions of developers create amazing experiences with Android APIs and Android Studio that ultimately touch billions of people and their devices. We care about incorporating AI to accelerate your productivity on Android to make things easier for you. Last year we introduced StudioBunt as your AI coding companion for Android. Thanks to your feedback, we evolved our models, expanded to over 200 countries and territories, released at the stable and brought it into the Gemini family of products last month. We introduced the introduction of Gemini and Android Studio. Today, we will show you how the latest Gemini 1.5 Pro models can take your development to the next level. Our vision is to responsibly use AI so you can spend more time on aspects of development that you like and make the harder tasks easier. For example, the crash-litx integration in Android Studio is even more helpful than ever before. Because on top of showing crashes and errors and the context of our source code, we now use Gemini to analyze and provide recommendations on how to fix those issues. Now, as you venture into more AI features, you ultimately have control of what you share or don't share with AI privacy controls built right into Android Studio. So, speaking of Android Studio, let's jump into the IDE to see a few more examples of Gemini and Action. So, here I am in the Jet Snack app, one of our open source composed samples. Just shopping up that cool, shared element in the integration that Maroo shared earlier with us. One of the cool things that Dubai enables is that it makes it easier to optimize my code. So, for example, here I have a conditional statement that can be a bit more efficient and idiomatic in Kotlin. To fix this, I can actually use Gemini to optimize this code so that it's easier to understand and maintain. So, all I have to do is right click on my selected code, navigate to Gemini, click on Transform, and let me simply input, simplify and make the idiomatic with this quick action. And press Enter. And what's happening is that I'm seeing the code snippet to the model and getting a result back. So, here I have a more optimized code submit going from an if-else structure to more efficient wind structure in Kotlin. Now, what is also great is that I can do additional modifications to this result. So, for instance, it's good practice to add documentation, right? So, let me use Gemini to help with that. All I need to do is click on the Refine button. Let's say, add some documentation and press Enter. Consider it's a model. And now I have augmented... The code changed with the help of Gemini. So, let me accept these changes. Now, I have optimized the code for readability and even added some code documentation, all of the help of Gemini. Nice. Now, Gemini can do pretty cool things on code itself, but it also does some amazing things that are perfect for Android app development. So, for example, we have the screen here that needs a little bit of translation. So, looking at my strings resource file, my colleague left me out to do the Transite to Spanish, but I don't know Spanish that well, but Gemini does. So, let me highlight a few strings here. And I can navigate right-click and navigate the Gemini. Click on Transform. In this case, let me type in Transite to Spanish. Let me see this question and strings to the model. I get a response back. It looks pretty good. And note, Gemini kept my app name in English. Nice. So, let me do one more section for completeness. Again, I hide the strings. Right-click, go to Gemini, click on Transform. Let me translate that to Spanish for this quick action. Results look pretty good. Let me accept those.
A chance to win for the most creative, useful, and remarkable apps. If you're a dev or a dev team that wants to submit, great! But you'll have some catching up to do. I just drank three cups of hydro cold brew and I'm ready to crank out some serious times of the world poll. Now we'll look at examples of how AI unlocks excellent experiences and how we can get the best results. Jacqueline talked about how Gemini models benefit all developers. Now we'll look at examples of how AI unlocks excellent experiences on Android, the world's most popular operating system. We're reimagining Android with AI at the core, enabling you to build a whole new class of apps on mobile. This unlocks experiences that were not even possible just a few years ago. So let's dive into the ways that Gemini can power building the next generation of apps on Android. Using Google AI Studio, you can integrate the Gemini API directly into your app to leverage Gemini models. When using Google Cloud, you can also access the powerful capabilities of Gemini through Vertex AI. This is Google's fully managed development platform designed for building and deploying generative AI. Whether you simply need world-class inference capabilities or want to build end-to-end AI workflows with Vertex, the Gemini API is a great solution. Next, let's look at Gemini Nano, our most efficient model for on-device tasks. It runs directly on mobile, enabling low latency responses and data privacy. This helps you implement features like suggested replies and messaging apps while knowing that the data won't leave the device. Gemini Nano also guarantees AI model availability even when there's no cellular network coverage. Gemini Nano, inference, is made possible by AI Core, a system service managing on-device foundation models. It removes the developer burden of packaging and distributing large language models. Gemini Nano and AI Core are currently available on the Pixel 8 Pro in Samsung Galaxy S24 series and, as you heard earlier, will be expanding their reach to additional devices later this year. Gemini Nano gives you the best path to running a generative AI model on device. And yet, we know that being able to customize and work with open models is important too. So we also built MediaPipe LLAM to give you a complete solution. Last year, we announced an Early Access program for Gemini Nano and we got an overwhelming amount of responses. We're now working closely with a few early adopters to ensure we're creating the best APIs that unlock the most innovative experiences. We're now on the path to opening access even wider in the coming months. One of these trailblazers is Patreon. They're building an experience that helps creators and fans rapidly catch up on unread messages in their community chats. And another example is Grammarly. Their team has been able to easily experiment with Gemini Nano while building their smart suggestions feature. And yet, these amazing examples are just the beginning. We're excited to see what you'll build with Gemini. Next up, Mara will share a bit about developer productivity and how Android is making it faster and easier to build excellent experiences across form factors. We know how much you all love Kotlin. It's been so fun to see the growth and energy in the community. Today, I'm excited to announce that we're taking a significant step forward with towards the future with first class tooling and library support for Kotlin Multiplotform on Android. We've been adding Kotlin Multiplotform support to Jetpack libraries like Datastore and now we're expanding it to your favorite libraries like Room. At Google, we've been migrating the Google Docs app to use Kotlin Multiplotform to share business logic across Android, iOS and Web. The workspace team is excited to continue to invest in using Kotlin Multiplotform across the rest of their apps in the future. We've been learning a lot from our work with Google Apps and we're collaborating with Jetbrings and the Kotlin Developer community to bring the benefits of KMP to all of you. Kotlin Multiplotform will boost your productivity by sharing business logic across platforms. On the UI layer, Compost helps you build excellent experiences and today 40% of the top 1,000 apps like Google Drive, Threads and SoundCloud are taking advantage of the benefits Compost brings. We're continuing to invest in Compost with more performance improvements and easier ways to build beautiful UI across form factors. Compost continues to be faster with each release and combined with R8 and baseline profiles can now significantly boost your apps performance. For an even more delightful user experience, you can now use shared element transitions with Compost.","The benefits of using Gemini models in Android Studio, as highlighted in the keynote, include:

1. **Enhanced Productivity**: Gemini acts as an AI coding companion, helping developers by providing recommendations and automating tasks, which allows them to focus on more enjoyable aspects of development.

2. **Code Optimization**: Gemini can analyze code and suggest optimizations, making it easier to write efficient and idiomatic code in Kotlin. For example, it can transform complex conditional statements into simpler structures.

3. **Documentation Assistance**: Gemini can help developers add documentation to their code, improving code readability and maintainability.

4. **Translation Capabilities**: Gemini can assist in translating strings within apps, making it easier for developers to localize their applications without needing to be fluent in the target language.

5. **AI Integration**: Developers can integrate the Gemini API directly into their apps, leveraging its capabilities for various tasks, including on-device AI processing with low latency and data privacy.

6. **Customization and Flexibility**: Gemini supports customization and the use of open models, allowing developers to tailor AI functionalities to their specific needs.

7. **Support for Kotlin Multiplatform**: The keynote mentioned the expansion of Kotlin Multiplatform support, which allows developers to share business logic across different platforms, enhancing productivity.

Overall, Gemini models are designed to streamline the development process, improve code quality, and enable innovative app experiences on Android."
What is the purpose of the new context-caching feature announced for Gemini models?,"As context windows grow, we're discovering new ways to interact with models to solve tasks. Huge context windows unlock a new set of AI applications, but they're also more computationally expensive, which is why I'm excited to announce the new context-caching feature coming next month. So how does it work? Well, let's say a large part of your prompt doesn't change. You can cache that once and easily call it unsubssequent turns for a fraction of the price. So imagine a major analyst trying to figure out what to write about next. To help me brainstorm ideas, I might want to gather all the content I've already created. Previous articles, some video clips, maybe some voice memos. Using context-caching, I'll send all this content at once, which let's say it will cost me about 20 cents for that first turn, along with the regular input price for the rest of my prompts. Since I cache that content that I'm referencing, each following turn will only cost me 10 cents, plus the cost of any new prompts. Of course, this concept is extensible to so many different applications that you might want to build. These could involve things like legal documents, research papers, school assignments, or employee training materials. The sky is the limit. Gemini models are empowering developers worldwide to transform workflows, enhancing efficiency, and also adding a touch of user delight. Take, for example, front-end development. Locify.ai takes a Figma design and uses their own AI models to generate code. But what's really unique is how they're pairing with Gemini's 1 million token context window. You see, oftentimes, front-end designs contain hundreds of layers with nondescript names such as rectangle one or group 76. Locify sends this entire design to 1.5 pro to generate meaningful names in the front-end code. Because Gemini understands the purpose behind rectangle one, it's actually able to rename it in code as something more useful like header background. Another one that I love is how AI models are unlocking new abilities. Invision is powering people with low vision to better understand their immediate environment through an app or smart glasses and ask specific questions. Because of the speed of 1.5 flash, users will be able to get quick responses. Another developer, Zapier, is enabling anybody to seamlessly connect different apps and automate repetitive tasks like podcast editing. They built a workflow to automatically remove speaker artifacts like OZ and OMS from audio tracks that's triggered as soon as a user uploads a file to Google Drive. As developers, the ideas you have and the code you write will help transform AI into practical applications for the world. We're announcing our Gemini API developer competition for a chance to win the grand prize, a custom electric de lorean. Woo! Woo! Woo! Woo! Woo! Woo! For those of you in the audience, be sure to check it out at the AI Pavilion, maybe grab a picture, and visit our website for more details. And now I'm going to leave you with my favorite developer to tell you a little bit more about it. Hey! You may recognize me for my acting work, but my real passion is programming. When I'm in the zone, I'm unstoppable. I don't know if you know this about me, but I'm really excited about the future and Google's Gemini models are tech that can get it there. And you know I'm going to be a great developer that you know I'm submitting to Google's new Gemini developer contest, which is so cool, it'll blow your mind back to beta. A chance to win for the most creative, useful, and remarkable apps. If you're a dev or a dev team that wants to submit, great! But you'll have some catching up to do. I just drank three cups of hydro cold brew and I'm ready to crank out some serious times of the world poll. Now we'll look at examples of how AI unlocks excellent experiences and how we can get the best results. Jacqueline talked about how Gemini models benefit all developers. Now we'll look at examples of how AI unlocks excellent experiences on Android, the world's most popular operating system. We're reimagining Android with AI at the core, enabling you to build a whole new class of apps on mobile. This unlocks experiences that were not even possible just a few years ago. So let's dive into the ways that Gemini can power building the next generation of apps on Android.
Now, meeting many of you at Hackathons and Meetups, I am blown away by how you're pushing the boundaries with our new AI models. From new ideas using audio and video inputs, the creative use of longer context windows. It's really inspiring to see. And like you heard about earlier today, balance across quality, cost, and speed. That's why I'm thrilled that Gemini 1.5 Flash is officially open to all developers to start building with today. You can get started using the Gemini API in Google AI Studio, which is now available in more than 200 countries and territories. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. But what I'm seeing more and more these days is that it isn't just about selecting the right model, but also the right techniques to bring your ideas to life. Let's say you want to personalize the responses you get. Now, one way to do that is by tuning the model. In fact, Google AI Studio lets you do that in minutes. And now, with our 2 million token context window, you have another compelling option. Let me show you how in Google AI Studio. So here I'm already signed in. We've made it easier for you to start prompting right away. Or if you want to get straight to code, you can quickly grab an API key here to use with one of our many SDKs. If you're looking for some inspiration, we also have a Gemini API cookbook, which is a great resource. But today, I actually want to take you through one of my own personal workflows that has unlocked a new way of getting things done. Now, if you're anything like me, ideas always seem to strike when I'm away from the computer. So now, when an idea comes to mind, I quickly grab my phone and I capture a voice memo to myself. This could be at the park with my daughters or while I'm out walking. So let's actually jump into a saved prompt that I have right now. This is what I use to help you write new blog posts. As you can see, I've already added some system instructions, which tell the model it's an experienced writer. I've also added a few of my previous blog posts so that the output is going to better mirror my tone, style, and word choice. Next, I'm going to add a bunch of voice memos of some half-big thoughts I've had recently for an upcoming post. So we'll click in here, go into my drive, and add these two voice memos. Now, while those are loading, I'm actually going to play you a clip from one of those just so you have an idea of exactly what I've sent into Gemini. OK, so recently, I made the connection about two pretty prominent moments in my career. The first. OK, so as you can clearly hear, I was at the park with my daughters pushing them on the swings. Now that these have loaded, I'm going to add a simple prompt. Turn these into a blog post. I'll hit run. And what you're about to see is Gemini 1.5 Flash take all of that content. My previous blog post, the system instructions, my voice memos, along with that prompt, and turn it into what will hopefully be a compelling first draft of my next blog post. From throwaway work to breakthrough ideas or to breakthroughs, embracing the power of right to think. Not bad for a working title. I think I can probably take it from there. As context windows grow, we're discovering new ways to interact with models to solve tasks. Huge context windows unlock a new set of AI applications, but they're also more computationally expensive, which is why I'm excited to announce the new context-caching feature coming next month. So how does it work? Well, let's say a large part of your prompt doesn't change. You can cache that once and easily call it unsubssequent turns for a fraction of the price. So imagine a major analyst trying to figure out what to write about next. To help me brainstorm ideas, I might want to gather all the content I've already created. Previous articles, some video clips, maybe some voice memos. Using context-caching, I'll send all this content at once, which let's say it will cost me about 20 cents for that first turn, along with the regular input price for the rest of my prompts.","The new context-caching feature announced for Gemini models is designed to optimize the use of large context windows by allowing users to cache a significant portion of their prompts that do not change. This means that once a user sends a large amount of content (such as previous articles, video clips, or voice memos) in a prompt, they can cache that content for subsequent interactions. As a result, the cost for future prompts that reference the cached content will be reduced, making it more economical to interact with the models. This feature aims to enhance efficiency in AI applications while managing the computational costs associated with large context windows."
What significant updates were made to Kotlin Multiplatform as discussed in the keynote?,"The workspace team is excited to continue to invest in using Kotlin Multiplotform across the rest of their apps in the future. We've been learning a lot from our work with Google Apps and we're collaborating with Jetbrings and the Kotlin Developer community to bring the benefits of KMP to all of you. Kotlin Multiplotform will boost your productivity by sharing business logic across platforms. On the UI layer, Compost helps you build excellent experiences and today 40% of the top 1,000 apps like Google Drive, Threads and SoundCloud are taking advantage of the benefits Compost brings. We're continuing to invest in Compost with more performance improvements and easier ways to build beautiful UI across form factors. Compost continues to be faster with each release and combined with R8 and baseline profiles can now significantly boost your apps performance. For an even more delightful user experience, you can now use shared element transitions with Compost. We've built a simple and powerful system that helps you choreograph beautiful transitions across screens that share content. It handles size changes and animates text changes and it works great with predictive apps right out of the box. Developers have seen significantly increased engagement on apps that look and feel great across the diverse range of Android devices. Rather than thinking about individual screen sizes, we want you to think adaptive. So write once and adapt across screen sizes and configurations. To make this journey easier, we've built a new set of Compost APIs for building adaptive layouts using material guidance. Early adopters are already integrating this layout into their Compost projects, significantly reducing development time. We've also been making improvements to stylus, mouse and keyboard support for Compost to make it easier to adapt to the input device in your user's hand. Take advantage of the new stylus handwriting recognition, which using the power of AI transforms handwriting into text when using views or Compost. Widgets are another high user engagement surface with Jetpack Lamps, you can now build widgets that seamlessly adapt to all screen sizes. And Android 15 brings support for generated previews to improve user discoverability in the widget picker. Next is testing. We're working with Samsung and Pixel and other OEMs to connect their device labs into the new Android device streaming powered by Firebase now in beta. So you can now validate your adaptive app on a wider array of real devices, including the latest foldable and tablets. We're also continuing to invest in the resizable emulator, which helps you quickly test your app across different device types, as well as building a new Compost UI check mode, which helps you identify and provide solutions to adaptive and accessibility issues. Let's hear from SoundCloud about how Compost accelerated their UI development to create excellent user experiences across devices. Then Jamal will join us to share more about how AI can supercharge your productivity. SoundCloud is an ex-generation music entertainment company powered by an ecosystem of artists, fans and curators. My name is Svita Sautna. I'm a super passionate about music and I'm doing Android development at SoundCloud where I'm leading the adoption of Jetpack Compost. We started using Jetpack Compost when we were still in early preview. Our goal was to overcome the robustity that's associated with manipulating XMLVs. We were able to save up to 45% of lines of code when migrating screens from XMLVs to Compost. We quickly realized how easy and natural it was to create reusable UI components. Now we've migrated around 50% of our design system to Jetpack Compost. We're investing heavily in writing our base filling box and Compost so that our engineers can write new features with it and make it their first choice. It's important for us to be available on any formfactor so that people can listen to SoundCloud in any situation. Our team's mobile Compost guilds transfer directly to other formfactors, given that its concepts and most APIs are the same. We build our various and TV apps from the ground up in Compost in just 4 months, which would have been unimaginable with the old D system. Some parts of our foldable and tablet layouts and the whole login experience of our car are built in Compost. Being available across formfactors has allowed us to reach new people and markets, which ultimately increases our global reach. It is so exciting to see how millions of developers create amazing experiences with Android APIs and Android Studio that ultimately touch billions of people and their devices. We care about incorporating AI to accelerate your productivity on Android to make things easier for you. Last year we introduced StudioBunt as your AI coding companion for Android.
A chance to win for the most creative, useful, and remarkable apps. If you're a dev or a dev team that wants to submit, great! But you'll have some catching up to do. I just drank three cups of hydro cold brew and I'm ready to crank out some serious times of the world poll. Now we'll look at examples of how AI unlocks excellent experiences and how we can get the best results. Jacqueline talked about how Gemini models benefit all developers. Now we'll look at examples of how AI unlocks excellent experiences on Android, the world's most popular operating system. We're reimagining Android with AI at the core, enabling you to build a whole new class of apps on mobile. This unlocks experiences that were not even possible just a few years ago. So let's dive into the ways that Gemini can power building the next generation of apps on Android. Using Google AI Studio, you can integrate the Gemini API directly into your app to leverage Gemini models. When using Google Cloud, you can also access the powerful capabilities of Gemini through Vertex AI. This is Google's fully managed development platform designed for building and deploying generative AI. Whether you simply need world-class inference capabilities or want to build end-to-end AI workflows with Vertex, the Gemini API is a great solution. Next, let's look at Gemini Nano, our most efficient model for on-device tasks. It runs directly on mobile, enabling low latency responses and data privacy. This helps you implement features like suggested replies and messaging apps while knowing that the data won't leave the device. Gemini Nano also guarantees AI model availability even when there's no cellular network coverage. Gemini Nano, inference, is made possible by AI Core, a system service managing on-device foundation models. It removes the developer burden of packaging and distributing large language models. Gemini Nano and AI Core are currently available on the Pixel 8 Pro in Samsung Galaxy S24 series and, as you heard earlier, will be expanding their reach to additional devices later this year. Gemini Nano gives you the best path to running a generative AI model on device. And yet, we know that being able to customize and work with open models is important too. So we also built MediaPipe LLAM to give you a complete solution. Last year, we announced an Early Access program for Gemini Nano and we got an overwhelming amount of responses. We're now working closely with a few early adopters to ensure we're creating the best APIs that unlock the most innovative experiences. We're now on the path to opening access even wider in the coming months. One of these trailblazers is Patreon. They're building an experience that helps creators and fans rapidly catch up on unread messages in their community chats. And another example is Grammarly. Their team has been able to easily experiment with Gemini Nano while building their smart suggestions feature. And yet, these amazing examples are just the beginning. We're excited to see what you'll build with Gemini. Next up, Mara will share a bit about developer productivity and how Android is making it faster and easier to build excellent experiences across form factors. We know how much you all love Kotlin. It's been so fun to see the growth and energy in the community. Today, I'm excited to announce that we're taking a significant step forward with towards the future with first class tooling and library support for Kotlin Multiplotform on Android. We've been adding Kotlin Multiplotform support to Jetpack libraries like Datastore and now we're expanding it to your favorite libraries like Room. At Google, we've been migrating the Google Docs app to use Kotlin Multiplotform to share business logic across Android, iOS and Web. The workspace team is excited to continue to invest in using Kotlin Multiplotform across the rest of their apps in the future. We've been learning a lot from our work with Google Apps and we're collaborating with Jetbrings and the Kotlin Developer community to bring the benefits of KMP to all of you. Kotlin Multiplotform will boost your productivity by sharing business logic across platforms. On the UI layer, Compost helps you build excellent experiences and today 40% of the top 1,000 apps like Google Drive, Threads and SoundCloud are taking advantage of the benefits Compost brings. We're continuing to invest in Compost with more performance improvements and easier ways to build beautiful UI across form factors. Compost continues to be faster with each release and combined with R8 and baseline profiles can now significantly boost your apps performance. For an even more delightful user experience, you can now use shared element transitions with Compost.","In the keynote, several significant updates to Kotlin Multiplatform (KMP) were discussed:

1. **First-Class Tooling and Library Support**: There is now first-class tooling and library support for Kotlin Multiplatform on Android, which enhances the development experience.

2. **Integration with Jetpack Libraries**: Kotlin Multiplatform support is being added to Jetpack libraries, including popular ones like Datastore and Room, making it easier for developers to share code across platforms.

3. **Migration of Google Apps**: The Google Docs app has been migrated to use Kotlin Multiplatform to share business logic across Android, iOS, and Web applications, showcasing its practical application in large-scale projects.

4. **Collaboration with the Developer Community**: The workspace team is collaborating with JetBrains and the Kotlin Developer community to further enhance the benefits of Kotlin Multiplatform for developers.

5. **Productivity Boost**: Kotlin Multiplatform is expected to boost developer productivity by allowing the sharing of business logic across different platforms, streamlining the development process.

These updates reflect a strong commitment to improving Kotlin Multiplatform's capabilities and its integration into the broader Android development ecosystem."
What significant advancements were introduced with the Gemini 1.5 Pro model?,"Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.","The Gemini 1.5 Pro model introduced several significant advancements:

1. **Quality Improvements**: The model received a series of quality enhancements across various tasks, including translation, coding, and reasoning.

2. **Increased Context Window**: The context window was expanded to allow for 1 million tokens, with plans to further increase it to 2 million tokens, enabling deeper and more complex interactions.

3. **Multi-modality**: The model supports natively multimodal inputs, allowing users to interleave text, images, audio, and video, which enhances its versatility.

4. **Developer Features**: New features were added, such as video frame extraction, parallel function calling (allowing multiple function calls at once), and context caching (which allows users to send files to the model once without needing to resend them repeatedly).

5. **Global Availability**: The Gemini 1.5 Pro model became available globally in over 200 countries and territories, making it accessible to a wider audience.

6. **Cost Efficiency**: The pricing structure was designed to be competitive, with 1.5 Pro priced at $7 per 1 million tokens and a reduced rate of $3.50 for prompts up to 128K tokens.

7. **Integration with Google Workspace**: The model was integrated into Google Workspace, enhancing productivity tools like Gmail by summarizing emails and analyzing attachments.

These advancements collectively aim to improve user experience and expand the capabilities of AI applications."
How many developers are currently using Gemini models across Google's tools?,"Welcome to the 16th Google I.O. I want to thank our fans, have developed our community all millions of you around the world for choosing to build with Google. Literally, whatever you can imagine, you can bring it to life because only Google's ecosystem gives you the potential to reach people on 3 billion Android devices and 2 billion Chrome and Chromium-based browsers. You've created millions of helpful apps with Firebase, Google Cloud, and generative AI models like Gemini and Gemma. At Google, we're on a mission to make generative AI accessible to every developer on the planet. It's transforming the very fundamentals of software development, including new techniques for how you build and new inspiration for what you build. AI helps you to be more productive by assisting you with many different development tasks, such as writing, debugging, and testing your code, or generating documentation, and understanding your code base. That's why Gemini is available to you, all developers to use in Android Studio, Chrome Dev Tools, Project IDX, CoLab, Beath Code, IntelliJ, and Firebase. CHEERING AND APPLAUSE Gemini is even more helpful with context such as app settings, performance data, your logs, and source code. This is just a glimpse of AI changing how we built. Now, let's go to what you built. Today, people just assumed that things will work great. Of course, any platform, any device that they use, literally anywhere they are, whether on the go, at home, or at work, in their local town, or on the other side of the world. At the same time, the tools that you use need to make that possible and they're constantly evolving. It's great to see the latest technologies come out, but that means more complexity, especially with so many different AI concepts to learn. We want to make your lives simpler. As code becomes content and coders become creators, whether you're building an AI-powered mobile app, a web app, or an interactive experience that works across all platforms, we're providing you with powerful, more integrated development tools, both from Google and the OpenEka system, and it comes with secure, easy access to Google's most capable AI models and global cloud infrastructure. Ultimately, it's about helping you to stay focused on what you want to build, amazing products and services, and not the plumbing, which takes me to our agenda for today. Up next, you'll hear more in how you can use the Gemini API in Google AI Studio to develop engaging and multimodal apps. Then we will take you through how Gemini helps you to create a whole new category of AI-powered experiences on Android and Web, with exceptional levels of productivity. We've also been hard at work creating the very best experienced, full-stack, multi-platform development. Our team will share what's now possible with Project IDX, Flutter, and Firebase. And we think that will dramatically expand what you can build. Then we will talk about how you can build and customize your own AI models with Gemma. You'll see how CoLab, Carrison Jacks, make it really easy. Finally, I'll be back up here to share one more thing before we wrap it up. Are you ready to jump in? OK. Awesome. Jacqueline, come join us. Our investment in our AI research and infrastructure allows us to put the power of AI directly into your hands. We make it easy to start building an AI app with a simple API and a tool that can be used to create a new product. It can be used to create an AI app with a simple API integration. So you can focus on building the best possible products for your users. Now, meeting many of you at Hackathons and Meetups, I am blown away by how you're pushing the boundaries with our new AI models. From new ideas using audio and video inputs, the creative use of longer context windows. It's really inspiring to see. And like you heard about earlier today, balance across quality, cost, and speed. That's why I'm thrilled that Gemini 1.5 Flash is officially open to all developers to start building with today. You can get started using the Gemini API in Google AI Studio, which is now available in more than 200 countries and territories. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. But what I'm seeing more and more these days is that it isn't just about selecting the right model, but also the right techniques to bring your ideas to life.
Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in monks than is happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. So, we are in the very early days of the AI platform shift. We see so much opportunity ahead for creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too, including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries, as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. Applause There's so much innovation happening in search. Thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.",More than 1.5 million developers are currently using Gemini models across Google's tools.
What new features are being rolled out in Google Photos with the integration of Gemini?,"Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in monks than is happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. So, we are in the very early days of the AI platform shift. We see so much opportunity ahead for creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too, including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries, as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. Applause There's so much innovation happening in search. Thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.
Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Oops! Reacts. Things got bigger. Like way bigger. And it wasn't all just for him, or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. Still, we are in the very early days of the AI platform shift. We see so much opportunity ahead. For creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too. Including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. The so much innovation happening in search, thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.","The integration of Gemini into Google Photos has led to the introduction of new experiences that enhance user interaction. While specific features are not detailed in the provided context, it is mentioned that Gemini's capabilities allow for more powerful search experiences within Google Photos. This suggests improvements in how users can search for and interact with their photos, potentially including enhanced search functionalities, better organization, and more intuitive ways to retrieve images based on complex queries or even through visual search."
What is the purpose of the Google Developer Program launched at the event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The purpose of the Google Developer Program launched at the event is to provide developers with updates and new tools to help them build the best applications across various services. It aims to leverage advancements in AI, particularly through the Gemini 1.5 Pro model, to improve and accelerate development processes. The program includes hands-on workshops, breakout sessions, and opportunities for developers to experience new technologies and innovations in action, fostering a collaborative environment for learning and development."
What is the expected impact of the new AI overviews feature in Google Search?,"Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago we created search to help people make sense of the waves of information moving online. With each platform shift we have delivered breakthroughs to help answer your questions better. On mobile we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities are high bar for information quality, our decades of experience connecting you to the richness of the web. The result is a product that does the work for you. Google search is generative AI at the scale of human curiosity. And it's our most exciting chapter of search yet. To tell you more here's Liz. Thanks Sundar. With each of these platform shifts we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI search will do more for you than you ever imagined. So whatever is on your mind and whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real time information with over a trillion facts about people, places and things. Second, our in parallel ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly. Complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub questions and get an AI overview in seconds. To make this possible, we're introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates studios in Boston and show you details on their inter offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there.
To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly, complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today, with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. You make this possible for introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates to use in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you've got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there. So since you're asking about places, it taps into Google's index of information about the real world with over 250 million places and updated in real time, including their ratings, reviews, business hours, and more. Research that might have taken you minutes or even hours Google can now do on your behalf in just seconds. Next, let me show you another way multi-step reasoning in Google search can make your life that much easier. Take planning, for example. Planning of trips and meal plans can be fun. But doing the work of actually figuring it all out, no thank you. With Gemini in search, Google does the planning with you. Planning is really hard for AI to get right. It's the type of problem that takes advance reasoning and logic. After all, if you're meal planning, you probably don't want mac and cheese for breakfast, lunch, and dinner. Okay, my tics might, but say you're looking for a bit more variety. Now, you can ask search to create a three-day meal plan for a group that's easy to prepare. And here you get a plan with a wide range of recipes from across the web. This one for overnight oats looks particularly interesting. And you can easily head over to the website to learn how to prepare them. If you want to get more veggies in, you can simply ask search to swap in a vegetarian dish. And just like that, search customizes your meal plan.","The expected impact of the new AI overviews feature in Google Search is significant. It aims to enhance the user experience by allowing users to receive instant answers to their questions, complete with a range of perspectives and links for deeper exploration. This feature is designed to simplify the process of gathering information by doing the research for users, particularly for complex questions that may involve multiple sub-questions.

With AI overviews, Google will roll out capabilities that enable users to ask comprehensive questions and receive detailed responses quickly. This includes the introduction of multi-step reasoning, which allows the search engine to break down larger queries into manageable parts and solve them in an efficient order. This means that tasks that previously required extensive research can now be accomplished in seconds, making information retrieval more efficient and user-friendly.

Overall, the AI overviews feature is expected to dramatically expand what's possible with Google Search, making it a more powerful tool for users seeking information and enhancing their ability to find relevant data quickly and effectively. By the end of the year, it is projected that AI overviews will be accessible to over a billion people, further increasing its impact."
What new technology related to music was introduced at Google I/O 2024?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
Sabak Give me the hell out of here. Get me the hell out of this cup. Get me out. Yeah! Google! Google! Google! Google! Google! Ha ha ha! We all ready to do a little googling. Everyone get out your computers. It's time to have ourselves a nice Google. Siri Navigate to google.com. Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Are you awake? Have you had your coffee? Are you awake? Don't worry baby. Google's gonna wake you right up. Don't worry baby. Google! Google! Google! Google's gonna wake you right up. Bleep bloop, bloop, bloop, rappy bloop, bloop, bloop. Up time to get up. You silly little nerd's wake up. okay Google.com. What is it? What does it mean? Why are we here? No one knows. And you're not, you're not going to find out. Not today. Those answers? Not part of the presentation. Buckle up. I'm Mark Rabier. It's nice to meet you, nice to see you, nice to be here. In beautiful Honolulu Hawaii. Here for Google I.O. If you have no idea who I am, that's perfectly I would expect that. Not many people do, but I'm an improvisational musician. I make stuff up from scratch. And that's exactly what we're going to be doing this entire show. We have nothing planned. That includes the rest of the conference. None of this has been rehearsed. Sundar's going to come out, say some stuff. No, we don't know what's going to happen here today. They're making up these visuals right now. Someone's back there. So what I'm here to talk about is music. I'm here to talk to you about music. And in particular, a very exciting new technology that we've been developing here at Google.com. Music FX. DJ. That's right. No more do you need to haul around the case of records. No, the computer will take care of it. Music FX DJ is a generative artificial intelligence tool that will from scratch generate from nothing. A series of sounds as it interprets your prompts. I'll demonstrate that to you now. Utilizing technology at my fingertips. And utilizing the colossal power of Google's machine. Connecting to Google's machine now. Music. We'll start with a viola. How about that? Everyone loves a viola. And if you don't, I don't care. I don't care. You do now. Viola. No one wrote this. Google wrote this. Let's throw in something weird. Hip hop beat from the 808 drum machine. It's pretty tough actually. So we have the sort of melodic element of the viola still in there. We can pull that out. Chiptones. I think that's chiptones. Let's do it out. It's pretty tough. Pulling that out. The machine is in control now. And that's what it can do. And let me tell you something. Let me tell you something right now. That is just the beginning. You're going to be hearing that a lot at this keynote. That's just the beginning. Okay. Now you will choose the prompts and we'll leave it up to chance. As I said, everything here unscripted unplanned. I didn't know this was going to happen. What's this? I didn't even know you guys were going to be here. What are you guys doing here? Six props, bagpipes, Persian tar, jembe, dulcimer, flamenco guitar, Detroit techno. You know what those things are? You know what this is? You better. Shout them out. What do you want to hear? Flamenco guitar. Persian took bagpipes. No, no bagpipes. They said, no, get the bagpipes out of here. Okay. Persian tar, jembe, flamenco guitar. Jembe.","The new technology related to music introduced at Google I/O 2024 is called ""Music FX DJ."" It is a generative artificial intelligence tool that can create a series of sounds from scratch based on user prompts. This technology allows users to summon various sounds and mix them together, enabling improvisational music performances."
Who hosted the I-O Pre-Show and what unique feature did they demonstrate?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The I-O Pre-Show was hosted by musician Mark Rubier. He demonstrated the new DJ mode in Google's music FX tool, which allowed him to host an improvisational set."
What was the atmosphere described during the Google I/O 2024 event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The atmosphere during the Google I/O 2024 event was described as buzzing with excitement, with attendees making their way into the Shoreline Amphitheater. The event was characterized as a unique opportunity for the company to share its story, with a vibrant developer festival vibe. There were about 4,000 developers in the audience, and the event featured a mix of programming, including hands-on workshops, demos, and a developer competition. The overall mood was energetic and enthusiastic, especially during the keynote and various demonstrations, where innovations in AI and technology were showcased."
What instruments were mentioned as part of the improvisational music demonstration?,"That's just the beginning. Okay. Now you will choose the prompts and we'll leave it up to chance. As I said, everything here unscripted unplanned. I didn't know this was going to happen. What's this? I didn't even know you guys were going to be here. What are you guys doing here? Six props, bagpipes, Persian tar, jembe, dulcimer, flamenco guitar, Detroit techno. You know what those things are? You know what this is? You better. Shout them out. What do you want to hear? Flamenco guitar. Persian took bagpipes. No, no bagpipes. They said, no, get the bagpipes out of here. Okay. Persian tar, jembe, flamenco guitar. Jembe. What I'm going to do is pop these in here and then I'm going to use them as samples and I'm going to make something with this. Ideally, I want it to be simple so that I can use it as an element in like a fuller composition. Do you understand what I'm saying? It's okay if you don't. I don't. Jembe. Flamenco guitar. Go ahead, baby. Give it a little more. Persian tar. Hey. I've now muted. reacts, deltarict. Monoyo. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. Shaburam. Shaburam. Shaburam. Go go go go go go go go go. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam.
Sabak Give me the hell out of here. Get me the hell out of this cup. Get me out. Yeah! Google! Google! Google! Google! Google! Ha ha ha! We all ready to do a little googling. Everyone get out your computers. It's time to have ourselves a nice Google. Siri Navigate to google.com. Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Are you awake? Have you had your coffee? Are you awake? Don't worry baby. Google's gonna wake you right up. Don't worry baby. Google! Google! Google! Google's gonna wake you right up. Bleep bloop, bloop, bloop, rappy bloop, bloop, bloop. Up time to get up. You silly little nerd's wake up. okay Google.com. What is it? What does it mean? Why are we here? No one knows. And you're not, you're not going to find out. Not today. Those answers? Not part of the presentation. Buckle up. I'm Mark Rabier. It's nice to meet you, nice to see you, nice to be here. In beautiful Honolulu Hawaii. Here for Google I.O. If you have no idea who I am, that's perfectly I would expect that. Not many people do, but I'm an improvisational musician. I make stuff up from scratch. And that's exactly what we're going to be doing this entire show. We have nothing planned. That includes the rest of the conference. None of this has been rehearsed. Sundar's going to come out, say some stuff. No, we don't know what's going to happen here today. They're making up these visuals right now. Someone's back there. So what I'm here to talk about is music. I'm here to talk to you about music. And in particular, a very exciting new technology that we've been developing here at Google.com. Music FX. DJ. That's right. No more do you need to haul around the case of records. No, the computer will take care of it. Music FX DJ is a generative artificial intelligence tool that will from scratch generate from nothing. A series of sounds as it interprets your prompts. I'll demonstrate that to you now. Utilizing technology at my fingertips. And utilizing the colossal power of Google's machine. Connecting to Google's machine now. Music. We'll start with a viola. How about that? Everyone loves a viola. And if you don't, I don't care. I don't care. You do now. Viola. No one wrote this. Google wrote this. Let's throw in something weird. Hip hop beat from the 808 drum machine. It's pretty tough actually. So we have the sort of melodic element of the viola still in there. We can pull that out. Chiptones. I think that's chiptones. Let's do it out. It's pretty tough. Pulling that out. The machine is in control now. And that's what it can do. And let me tell you something. Let me tell you something right now. That is just the beginning. You're going to be hearing that a lot at this keynote. That's just the beginning. Okay. Now you will choose the prompts and we'll leave it up to chance. As I said, everything here unscripted unplanned. I didn't know this was going to happen. What's this? I didn't even know you guys were going to be here. What are you guys doing here? Six props, bagpipes, Persian tar, jembe, dulcimer, flamenco guitar, Detroit techno. You know what those things are? You know what this is? You better. Shout them out. What do you want to hear? Flamenco guitar. Persian took bagpipes. No, no bagpipes. They said, no, get the bagpipes out of here. Okay. Persian tar, jembe, flamenco guitar. Jembe.","The instruments mentioned as part of the improvisational music demonstration are:

1. Bagpipes
2. Persian tar
3. Jembe
4. Dulcimer
5. Flamenco guitar
6. Detroit techno"
What was the overall theme of Mark Rubier's presentation at Google I/O 2024?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The overall theme of Mark Rubier's presentation at Google I/O 2024 was centered around the use of the new DJ mode in Google's music FX tool, which allowed him to host an improvisational set. The presentation highlighted the technology's capability to summon and mix various sounds, showcasing its potential as a companion for creating unique and specific sounds for larger compositions. This theme of creativity and innovation in music technology was emphasized during his performance."
"What is the context window size for Gemini 1.5 Pro, and how does it compare to previous versions?","I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM.
Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.","The context window size for Gemini 1.5 Pro is 1 million tokens, which has recently been expanded to 2 million tokens for developers in private preview. This represents an increase from previous versions, allowing for greater capacity and the ability to handle more extensive inputs compared to earlier iterations of the Gemini models."
"What are the two main technical advances highlighted in the presentation, and how do they enhance Gemini's capabilities?","So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example. My sister is a self-employed photographer, and her inbox is full of appointment bookings, receipts, client feedback on photos, and so much more. Now, if you're a freelancer or a small business, you really want to focus on your craft and not on bookkeeping and logistics. So, let's go to her inbox and take a look. Lots of unread emails. Let's click on the first one. It's got a PDF. That's an attachment from a hotel as a receipt, and I see a suggestion in the side panel. Help me organize and track my receipts. Let's click on this prompt. The side panel now will show me more details about what that really means. And as you can see, there's two steps here. Step one. Create a drive folder and put this receipt and 37 others it's found into that folder. Make sense. Step two. Extract the relevant information from those receipts in that folder into a new spreadsheet. Now, this sounds useful. Why not? I also have the option to edit these actions, or just hit OK. So, let's hit OK. Gemini will now complete the two steps described above, and this is where it gets even better. Gemini offers you the option to automate this so that this particular workflow is run on all future emails, keeping your drive folder and expense sheet up to date with no effort from you. Now, we know that creating a complex spreadsheet like this can be daunting for most people. But with this automation, Gemini does the hard work of extracting all the right information from all the files and in that folder and generates this sheet for you. So, let's take a look. OK, it's super well organized and it even has a category for expense type. Now, we have the sheet. Things can get even more fun. We can ask Gemini questions, questions like show me where the money spent. Gemini not only analyzes the data from the sheet, but also creates a nice visual to help me see the complete breakdown by category. You can imagine how this extends to all sorts of use cases in your inbox like travel expenses, shopping, remodeling projects, you name it. All of that information in Gmail can be put to good use and help you work, plan, and play better. Now, this particular... I know. This particular ability to organize your attachments and drive and generate a sheet and do data analysis via Q&A will be rolling out to labs users this September. And it's just one of the many automations that we're working on in workspace. Workspace in the Gemini era will continue to unlock new ways of getting things done. We're building advanced agent of experiences, including customizing how you use Gemini. Now, as we look to 2025 and beyond, we're exploring entirely new ways of working with AI. Now, with Gemini, you have an AI-powered assistant always at your side. But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs, or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and context to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives.
Gemini understood the back and forth in that thread and that Jeff was ready to start so offers me a few customized options based on that context. So here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay so one of the really neat things about Workspace Apps like Gmail, Drive, Docs, Calendar is how well they work together. In our daily lives we often have information that flows from one app to another like say adding a calendar entry from Gmail or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely. So let me show you what I mean with a real life example. My sister is a self-employed photographer and her inbox is full of appointment bookings, feeds, client feedback on photos and so much more. Now if you're a freelancer or a small business you really want to focus on your craft and not on bookkeeping and logistics. So let's go to her inbox and take a look. Lots of unread emails, let's click on the first one. It's got a PDF, that's an attachment from a hotel as a receipt and I see a suggestion in the side panel. Talk me, organize and track my receipts. Let's click on this prompt. The side panel now will show me more detailed about what that really means. And as you can see there's two steps here. Step one, create a drive folder and put this receipt and 37 others it's found into that folder. Make sense. Step two, extract the relevant information from those receipts in that folder into a new spreadsheet. Now this sounds useful, why not. I also have the option to edit these actions or just hit OK. So let's hit OK. Gemini will now complete the two steps described above and this is where it gets even better. Gemini offers you the option to automate this so that this particular workflow is run on all future emails, keeping your drive folder and expand sheet up to date with no effort from you. Now we know that creating a complex spreadsheet like this can be daunting for most people. But with this automation Gemini does the hard work of extracting all the right information from all the files and in that folder and generates this sheet for you. So let's take a look. OK, it's super well organized and it even has a category for expense type. Now we have the sheet, things can get even more fun. We can ask Gemini questions, questions like, show me where the money is spent. Gemini not only analyzes the data from the sheet but also creates a nice visual to help me see the complete breakdown by category. And you can imagine how this extends to all sorts of use cases in your inbox like travel expenses, shopping, remodeling projects, you name it. All of that information in Gmail can be put to good use and help you work, plan and play better. Now this particular, I know. This particular ability to organize your attachments in drive and generate a sheet and do data analysis via Q&A will be rolling out to labs users this September. And it's just one of the many automations that we're working on in workspace. Workspace in the Gemini era will continue to unlock new ways of getting things done. We're building advanced agent of experiences including customizing how you use Gemini. Now as we look to 2025 and beyond, we're exploring entirely new ways of working with AI. Now with Gemini you have an AI power dissistant always at your side. But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and contacts to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives. Let me bring Tony up to show you what I mean. Hey, Tony. Hey, partner.","The two main technical advances highlighted in the presentation regarding Gemini's capabilities are:

1. **Seamless Integration Across Workspace Apps**: Gemini enhances the functionality of Workspace Apps like Gmail, Drive, Docs, and Calendar by allowing information to flow seamlessly between them. This integration enables users to automate tasks such as adding calendar entries from emails or creating reminders from spreadsheets, thereby streamlining workflows and reducing manual effort.

2. **Automation of Workflows**: Gemini introduces automation features that allow users to set up workflows for managing tasks like organizing receipts and generating spreadsheets. For instance, it can automatically create a drive folder for receipts and extract relevant information into a new spreadsheet. This automation not only saves time but also minimizes the complexity of managing data, making it easier for users, especially freelancers and small business owners, to focus on their core activities rather than on bookkeeping and logistics.

Together, these advances enhance Gemini's capabilities by improving user efficiency and productivity through automation and better integration of tools within the Google Workspace ecosystem."
"What is Project Astra, and what is its goal in relation to AI assistance?","Yes. Let's see what it says. Hey, my name is Michael Chang and we're watching the Google IO keynote while it's happening with Project Astra. Let's take a look. Hey, Gemini, currently watching Google IO keynote right now. You want to watch it with me? Sure, I'd love to watch it with you. What's happening in the keynote? What does it look like? What does it look like they're announcing right now? It looks like they're announcing Gemini 1.5 Flash. The speaker is talking about its speed and efficiency in comparison to the pro model. What are the other features? It looks like Gemini 1.5 Flash is optimized for tasks where low latency and high efficiency matter most. Cool. The screen also shows. Oh, yeah, sorry, I interrupted you. Keep going. That's all right. What were we talking about? Well, we were talking about the capabilities of 1.5 Flash. Do you remember? Yes, we were discussing the new Gemini model and its efficiency and speed. The presentation seems to have moved on though. What would you like to talk about now? Well, I'm curious about what he's going to talk about next. He seems to be moving on to the topic of model training and feedback. Perhaps they will talk about how user input helps train the models. It looks like he just announced a new project. What is that project called? They are calling that project, Project Astra. It looks like they are calling that project Astra. It looks like they are calling that project Astra. It looks like they are calling that project Astra. It looks to focus on the future of AI assistance. What do you think a project Astra will be helpful for? It sounds like a very ambitious project. It will be interesting to see what kind of AI assistant they create and how it integrates with our daily lives. They seem to say it's a universal agent. What would you use a universal agent for? A universal agent that can help with everyday tasks would be helpful. It could potentially be useful for things like scheduling, finding information, or even controlling smart home devices. Oh, that would be super cool. I agree it sounds amazing. Having a seamless and intuitive AI agent could revolutionize how we interact with technology. Yeah, can you summarize what he showed so far? The speaker introduced Project Astra, an AI-powered universal agent for everyday tasks that is still under development. They discussed the challenges of building this technology and how their team is working toward making Astra helpful and intuitive for users. Wow, amazing. And what did he talk about before Project Astra? He was discussing the Gemini 1.5 model, particularly Gemini 1.5 Flash, which focuses on efficiency and speed. The presentation was focused on the future direction of AI models and user feedbacks roll in training those models. It looks like they're now doing a demo of Project Astra. What does it look like they're doing right now? In the video, someone is holding a phone and panning across the table. The phone is running Project Astra, which can identify a speaker and a cup of crayons. And what is it doing now? It appears to be scanning a computer screen displaying code. Astra then explains what the highlighted section of the code is doing. Wow, that's super cool. Thanks for explaining all of these things to me, Gemini. You're very welcome. Is there anything else I can explain about the presentation? Let's just keep on watching. What's currently happening? In the video, Astra is identifying the King's Cross Neighborhood of London using the phone's camera view. It's also providing some facts about the area. Wow. I can't believe this is...this technology exists right now. It does look quite impressive. Astra seems very advanced at interpreting what it sees and providing relevant information. That was a lot of fun to watch. I'm looking forward to Project Astra as I'm sure all of you are too. Thank you for being here. Thank you for tuning in. Let's go build something amazing!
Thanks, and it's so great to be here. Ever since I was a kid playing chess for the England Junior Team, I'd been thinking about the nature of intelligence. I was captivated by the idea of a computer that could think like a person. It's ultimately why it became a programmer and studied neuroscience. I co-founded DeepMind in 2010 with the goal of one day building AGI, Artificial General Intelligence, a system that has human level cognitive capabilities. I've always believed that if we could build this technology responsibly, its impact would be truly profound and it could benefit humanity in incredible ways. Last year, we reached a milestone on that path when we formed Google DeepMind, combining AI talent from across the company into one super unit. Since then, we built AI systems that can do an amazing range of things, from turning language and vision into action for robots, navigating complex virtual-threading environments, solving a limpiav level math problems, and even discovering thousands of new materials. Just last week, we announced our next-generation alpha-fold model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. All of this was made possible with the best infrastructure for the AI era, including our highly optimized tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multi-modal because that's how we interact with and understand the world around us. We've built a variety of models for different use cases, you've seen how powerful Gemini 1.5 Pro is, but we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale while still featuring multi-modal reasoning capabilities and breakthrough-long context. Flash is optimized for tasks where low latency and efficiency matter most. Starting today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI, and developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. You'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models, but we're always striving to push the state of the art even further. At any one time, we have many different models in training, and we use our very large and powerful ones to help teach and train our production-ready models. Together with user feedback, this cutting-edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multi-modal AI. And this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multi-modal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action, and it would have to be proactive, teachable and personal, so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multi-modal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called?","Project Astra is an initiative aimed at developing a universal AI agent that can be truly helpful in everyday life. The goal of Project Astra is to create an AI system that understands and responds to the complex and dynamic world similarly to how humans do. This involves the AI being able to take in and remember what it sees to understand context, act proactively, and interact naturally with users without lag or delay. The project focuses on making AI assistance more intuitive and efficient, enhancing the way people interact with technology in their daily tasks."
"What is the significance of the new generative video model called VO, and what capabilities does it offer?","We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.
This is what excites me. As a hip-hop producer, we dug in the crates. We play these vinyls, and the part with it is no vocal, we pull it, we sample it, and we create an entire song around that. Right now we dig in an infinite crate. It's endless. Where I found the AI really useful for me, this way to fill in the sparser elements of my loops. Okay, let's try Bongo's. We're going to put Viola. We're going to put Rhythnic clapping. And we're going to see what happens there. And it makes it sound ironically at the end of the day a little more human. So then this is entirely Google's loops right here. These are glutes. It's like having like this weird friend that's just like, try this, try that. And then you're like, oh, okay, yeah, no, that's pretty dope. It's a group in, group in, group in, group in, up, up, up. The tools are capable, speeding up the process of what's in my head getting it out. You're able to move light speed with your creativity. This is amazing. Right there. I think this really shows what's possible when we work with the artist's community on the future of music. You can find some brand new songs from these acclaimed artists and songwriters on their YouTube channels now. There's one more area I'm really excited to share with you. Our teams have made some incredible progress in genitive video. Today, I'm excited to announce our newest, most capable genitive video model called VO. VO creates high quality 1080p videos from text, image and video prompts. It can capture the details of your instructions in different visual and cinematic styles. You can prompt for things like aerial shots of a landscape or time lapse and further edit your videos using additional prompts. You can use VO in our new experimental tool called Video FX. We're exploring features like storyboarding and generating longer scenes. VO gives you unprecedented creative control. Techniques for generating static images have come a long way, but generating video is a different challenge altogether. Not only is it important to understand where an object or subject should be in space, it needs to maintain this consistency over time, just like the car in this video. VO builds upon years of our pioneering genitive video model work, including GQN, FNACI, WALT, Video Poet, Lumiere and much more. We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into a video model. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it, right? As much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today.","The new generative video model called VO is significant because it represents a major advancement in the field of generative video technology. Developed by Google DeepMind, VO is designed to convert input text, images, and video prompts into high-quality 1080p videos. Here are some of its key capabilities:

1. **High-Quality Video Generation**: VO can create videos with a resolution of 1080p, ensuring that the output is visually appealing and suitable for various applications.

2. **Creative Control**: The model allows users to prompt for specific visual and cinematic styles, enabling unprecedented creative control over the video production process. Users can request particular shots, such as aerial views or time-lapse sequences.

3. **Enhanced Consistency**: VO builds upon previous generative video models and incorporates techniques that improve the consistency and quality of the generated videos over time, which is crucial for maintaining coherence in moving images.

4. **Multi-Modal Capabilities**: By utilizing Gemini's multi-modal capabilities, VO can better capture the nuances of user prompts, including cinematic techniques and visual effects, thereby enriching the storytelling experience.

5. **Rapid Prototyping**: The model allows creators to visualize ideas much faster than traditional methods, facilitating quicker iterations and experimentation in the creative process.

6. **Accessibility for Creators**: Features of VO will be made available to select creators through an experimental tool called Video FX, with a waitlist open for interested users, indicating a push towards democratizing access to advanced video creation tools.

Overall, VO is positioned to empower a broader range of individuals, including those who may not have traditional filmmaking skills, to express their creativity and tell stories through video."
"How does Gemini Advanced enhance the user experience for trip planning, and what features does it include?","Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. Over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now Gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now based on my flight information, Gemini knows that I needed two and a half day, itinerary. You can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens.
We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.","Gemini Advanced enhances the user experience for trip planning by integrating advanced reasoning and intelligence capabilities that allow it to create personalized vacation plans based on user preferences and constraints. Here are the key features it includes:

1. **Dynamic Graph Creation**: Gemini gathers information from various sources, such as search results and helpful extensions like maps and Gmail, to create a dynamic graph of possible travel options. This allows it to consider multiple variables and prioritize them effectively.

2. **Personalized Itineraries**: Based on user inputs, such as flight details and personal interests, Gemini generates a tailored itinerary. For example, it can adjust plans based on flight arrival times and user preferences for activities.

3. **Interactive Adjustments**: Users can interact with the itinerary in real-time. If a user wants to change the start time of an activity, Gemini can automatically adjust the rest of the itinerary accordingly, ensuring that all activities fit within the new schedule.

4. **Integration with User Data**: Gemini can access relevant information from the user's Gmail inbox, such as flight and hotel details, to streamline the planning process and make it more efficient.

5. **User-Centric Design**: The new dynamic user interface presents the vacation plan in an organized manner, making it easy for users to understand and follow.

Overall, these features collectively make trip planning faster, more intuitive, and tailored to individual needs, significantly enhancing the overall user experience with Gemini Advanced."
What is the significance of the tools mentioned in the event?,"Let's click on the first one It's got a PDF that's an attachment from a hotel as a receipt and I see a suggestion in the side panel Help me organize and track my receipts. Let's click on this prompt The side panel now show me more detailed about what that really means and as you can see There's two steps here step one Create a drive folder and put this receipt and 37 others. It's found into that folder. Make sense Step two extract the relevant information from those receipts in that folder into a new spreadsheet now This sounds useful why not I also have the option to edit these actions or just hit OK So let's hit OK Gemini will now complete the two steps described above and this is where it gets even better Gemini offers you the option to automate this so that this particular workflow is run on all future emails Keeping your drive folder an expense sheet up to date with no effort from you Now Now we know that creating a complex spreadsheet like this can be daunting for most people But with this automation Gemini does the hard work of extracting all the right information from all the files and in that folder and Generates this sheet for you. So let's take a look. Okay. It's super well organized and it even has a category for expense type now We have the sheet things can get even more fun We can ask Gemini questions questions like show me where the money spent Gemini not only analyzes the data from the sheet But also creates a nice visual to help me see the complete breakdown by category And you can imagine how this extends to all sorts of use cases in your inbox like travel expenses Shopping remodeling projects you name it all of that information and Gmail can be put to good use and help you Work plan and play better now this particular This particular ability to organize your attachments and drive and generate a sheet and do data analysis via Q&A We'll be rolling out to labs users this September and it's just one of the many automations that we're working on in workspace Work space in the Gemini era will continue to unlock new ways of getting things done We're building advanced agent of experiences including customizing how you use Gemini Now as we look to 2025 and beyond we're exploring entirely new ways of working with AI Now with Gemini you have an AI power assistant always at your side But what if you could expand how you interact with AI For example when we work with other people we mention them in comments and docs or we send them emails We have group chats with them etc And it's not just how we collaborate with each other But we each have a specific role to play in the team and as the team works together We build a set of collective experiences and contacts to learn from each other We have the combined set of skills to draw from when we need help So how could we introduce AI into this mix and build on this shared expertise Well, here's one way We're prototyping a virtual Gemini powered teammate This teammate has an identity a workspace account along with a specific role and objectives Let me bring Tony up to show you what I mean Hey Tony Hey everyone okay So let me start by showing you how we set up this virtual teammate As you can see the teammate has this very own account and we can go ahead and give it a name. We'll do something fun like Chip Chips been given a specific job role at the set of descriptions on how to be helpful for the team You see that here and some of the jobs are to monitor and track projects We've listed a few out to organize information and provide context and a few more things Now that we've configured our virtual teammate Let's go ahead and see chip in action to do that. I'll switch us over here to Google chat First when planning for an event like IO we have a ton of chat rooms for various purposes Luckily for me chip is in all of them To quickly catch up I might ask a question like Anyone know if our IO Storyboards are approved Because we've instructed chip to track this project chip searches across all the conversations and knows respond with an answer There it is simple but very helpful Now as the team adds chip to more group chats more files more email threads Chip builds a collective memory of our work together Let's look at an example to show you all switch over to a different room. How about project sapphire over here And here we are discussing a product release coming up and as usual many pieces are still in flight so I can go ahead and ask Are we on track?
Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.","The tools mentioned in the Google I/O 2024 event, particularly those related to the Gemini models, signify a major advancement in how AI can enhance productivity and collaboration. The key points of significance include:

1. **Automation of Tasks**: The ability to automate workflows, such as organizing receipts and generating spreadsheets, demonstrates how AI can simplify complex tasks, making them more manageable for users. This reduces the effort required for routine tasks and allows users to focus on more strategic activities.

2. **Data Analysis and Visualization**: The tools enable users to analyze data from spreadsheets and create visual representations, which helps in understanding financial expenditures and other metrics. This capability enhances decision-making by providing clear insights into data.

3. **Enhanced Collaboration**: The introduction of a virtual AI teammate, like ""Chip,"" illustrates how AI can facilitate teamwork by tracking projects and providing context across various communication channels. This collective memory feature allows teams to work more efficiently by having access to shared knowledge.

4. **Accessibility Features**: The advancements in accessibility, such as the multi-modal capabilities of Gemini, highlight the importance of inclusivity in technology. Features that describe images in detail for visually impaired users demonstrate a commitment to making technology usable for everyone.

5. **Developer Support**: The updates and tools announced for developers, including enhancements to Android Studio, indicate a focus on empowering developers to create better applications. This support is crucial for fostering innovation and ensuring that developers can leverage the latest AI advancements in their projects.

Overall, these tools signify a shift towards more intelligent, user-friendly applications that enhance productivity, collaboration, and accessibility, ultimately aiming to improve the overall user experience in various contexts."
What does the introduction of 'Gemini' signify in the context of the event?,"We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
Chercalm7 Allow me to reuse myself My name is Sodieneres Okay When all of these tools come together, it's a powerful combination. It's amazing. That's amazing. It's an entire suite of different kinds of possibilities. Hi, I'm Gemini. What neighborhood do you think I'm in? This appears to be the campus cross area of London. Together we're creating a new era.","The introduction of 'Gemini' signifies a major advancement in AI technology, particularly in how users interact with AI. It is presented as a personal AI assistant designed to enhance productivity and creativity for users at home and work. Gemini is characterized by its multimodal capabilities, allowing users to communicate through text, voice, or camera, and it aims to provide a more natural and intuitive interaction experience. The introduction also highlights features like customization through 'gems,' which allow users to create personal AI experts tailored to specific needs. Overall, Gemini represents a shift towards more intelligent, responsive, and personalized AI experiences, indicating Google's commitment to integrating advanced AI into everyday tasks and interactions."
In which location is the 'Gemini' character suggesting it is situated?,"We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
Chercalm7 Allow me to reuse myself My name is Sodieneres Okay When all of these tools come together, it's a powerful combination. It's amazing. That's amazing. It's an entire suite of different kinds of possibilities. Hi, I'm Gemini. What neighborhood do you think I'm in? This appears to be the campus cross area of London. Together we're creating a new era.",The 'Gemini' character suggests it is situated in the campus cross area of London.
What overarching theme does the event convey regarding technological advancements?,"At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features are really good. They're a great way to create a new world. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways, and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AGI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensed processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs we are proud to offer CPUs and GPUs to support any workload. That includes the new Axion processes we announced last month. Our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge blackwell GPUs available in the market. We are fortunate to have a long standing partnership with NVIDIA, excited to bring blackwells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better.
That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways. And thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AGI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases tenfold. Google was built for this. For 25 years, we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensor processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Amtropic have trained their models on TPUs as well. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs, we are proud to offer CPUs and GPUs to support any workload that includes the new Axion processes we announced last month our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge Blackwell GPUs available in early 2025. We are fortunate to have a long-standing partnership with NVIDIA excited to bring Blackwell's capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying the raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago we created search to help people make sense of the waves of information moving online. With each platform shift we have delivered breakthroughs to help answer your questions better. On mobile we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk.","The overarching theme conveyed by the event regarding technological advancements is the transformative power of AI and its potential to enhance creativity, communication, and problem-solving. The discussions highlight how innovations, particularly in AI models and computing infrastructure, are enabling individuals to express their creativity more freely and effectively. The emphasis on storytelling, the optimization of model training through advanced technologies like TPUs, and the integration of AI into various applications illustrate a vision of a future where technology not only improves efficiency but also fosters deeper understanding and connection among people. Additionally, the commitment to continuous investment in AI innovation and infrastructure underscores the belief that these advancements will significantly impact various industries and everyday life."
How does the speaker describe the combination of tools presented?,"I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. I got the news, dude. I got the news, dude. I got therada, I got theirdoooo. I got the news, dude. I got the news, dude. I got the news, dudes. I got the news, dude. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Me Organize, Since We Launched. And now, we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini-for-Works-based to the organization, and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5-Private Pro unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now, we've been trying to find a contractor to fix our roofs, and with Work Travel, I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read, and luckily for me, I can simply tap the summarized option up top and skip reading this long back and forth. Now, Gemini pulls up this helpful mobile card as an overlay, and this is where I can read a nice summary of all the salient information that I need to know. So, I see here that we have a quote from Jeff at Green Roofing, and he's ready to start. Now, I know we had other bids, and I don't remember the details. Previously, I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now, I can simply type out my question right here in the mobile card and say something like, compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox, for example, when am I shoes arriving, or what time the doors open for the next game, without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway, back to my roof. It looks like Gemini has found details that I got from two other contractors in completely different email threads, and I have this really nicely organized summary, and I can do a quick comparison. So, it seems like Jeff's quote was right in the middle, and he can start immediately, so Green Roofing it is. I'll open that last email from Jeff and confirm the project, and look at that. I see some suggested replies from Gemini. Now, what is really, really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread, and that Jeff was ready to start, so offers me a few customized options based on that context. So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example.
There's a parents group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multimodal model, we can do so much more. To show you an early demo of an audio output in notebook LM, here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son. And I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, he really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that, of course, means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes. The foundation for understanding how objects move and interact. Ah, yes. This is where multimodal really shines. Now, it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah. Can you give my son Jimmy a basketball example? Hey, Jimmy. That's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay. So first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now, what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity, or Isaac Newton, but nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format, and it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Jor. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning, and memory are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision. We are still in the early days, and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes, and a lot less fun to return them when they don't fit.","The speaker describes the combination of tools presented as a seamless and integrated experience that enhances productivity and interaction. They highlight how tools like Gmail, Drive, Docs, and Calendar work together effectively, allowing for smooth transitions of information between applications. The speaker emphasizes the automation capabilities of Gemini, which can streamline tasks such as summarizing emails, comparing bids, and generating contextual replies. This integration is portrayed as a significant improvement in user experience, making it easier to manage information and complete tasks efficiently. The overall message conveys excitement about the potential of these tools to transform workflows and enhance user engagement."
What is the biggest challenge faced by teachers today according to the excerpt?,"What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.
And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube. In fact, you've already seen LearnLam on stage today, when it helps to meer with his son's homework on Android. Now, let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems, which will be available in the Gemini app and web experience, including one called Learning Coach. With Learning Coach, you can get step-by-step study guidance, along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features that Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy, to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use Generative AI. We're also working directly with educators to build more helpful Generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning, and enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real, how much it is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors, and how much more is still to come. This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar.","According to the excerpt, the biggest challenge faced by teachers today is finding a way to fit everything that's required of educators within the limited timeframe they feel they are given. This includes managing and meeting the needs and expectations of students, supervisors, the school, and the community."
How is generative AI being applied in education according to the document?,"What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.
And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube. In fact, you've already seen LearnLam on stage today, when it helps to meer with his son's homework on Android. Now, let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems, which will be available in the Gemini app and web experience, including one called Learning Coach. With Learning Coach, you can get step-by-step study guidance, along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features that Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy, to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use Generative AI. We're also working directly with educators to build more helpful Generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning, and enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real, how much it is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors, and how much more is still to come. This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar.","According to the document, generative AI is being applied in education through the development of tools that enhance the learning experience for both teachers and students. Specifically, it mentions the following applications:

1. **Personalized Learning**: Generative AI is used to create personalized learning experiences, allowing teachers to differentiate instruction better and tailor resources to meet the individual needs of students.

2. **Lesson Planning**: A pilot program in Google Classroom is being implemented to simplify the lesson planning process for educators. This program helps teachers discover new ideas, find engaging materials, and adjust their lessons to better suit their students.

3. **Interactive Learning Tools**: The introduction of LearnLM, a family of models based on Gemini, aims to make learning experiences more personal and engaging. This includes features like the ""Learning Coach,"" which provides step-by-step study guidance and memory techniques.

4. **Enhanced Educational Videos**: Generative AI is being integrated into platforms like YouTube to make educational videos more interactive, allowing students to ask questions, receive explanations, and take quizzes.

5. **Collaboration with Educational Institutions**: Google is partnering with experts and institutions to test and improve generative AI capabilities, ensuring that the tools developed are effective and beneficial for educators.

Overall, the document emphasizes that generative AI is transforming education by making learning more active, tailored, and efficient, while also preserving the essential human connection between teachers and students."
What pilot program is mentioned in the excerpt and what is its purpose?,"There's a parents group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multimodal model, we can do so much more. To show you an early demo of an audio output in notebook LM, here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son. And I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, he really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that, of course, means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes. The foundation for understanding how objects move and interact. Ah, yes. This is where multimodal really shines. Now, it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah. Can you give my son Jimmy a basketball example? Hey, Jimmy. That's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay. So first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now, what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity, or Isaac Newton, but nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format, and it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Jor. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning, and memory are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision. We are still in the early days, and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes, and a lot less fun to return them when they don't fit.
Let's click on the first one It's got a PDF that's an attachment from a hotel as a receipt and I see a suggestion in the side panel Help me organize and track my receipts. Let's click on this prompt The side panel now show me more detailed about what that really means and as you can see There's two steps here step one Create a drive folder and put this receipt and 37 others. It's found into that folder. Make sense Step two extract the relevant information from those receipts in that folder into a new spreadsheet now This sounds useful why not I also have the option to edit these actions or just hit OK So let's hit OK Gemini will now complete the two steps described above and this is where it gets even better Gemini offers you the option to automate this so that this particular workflow is run on all future emails Keeping your drive folder an expense sheet up to date with no effort from you Now Now we know that creating a complex spreadsheet like this can be daunting for most people But with this automation Gemini does the hard work of extracting all the right information from all the files and in that folder and Generates this sheet for you. So let's take a look. Okay. It's super well organized and it even has a category for expense type now We have the sheet things can get even more fun We can ask Gemini questions questions like show me where the money spent Gemini not only analyzes the data from the sheet But also creates a nice visual to help me see the complete breakdown by category And you can imagine how this extends to all sorts of use cases in your inbox like travel expenses Shopping remodeling projects you name it all of that information and Gmail can be put to good use and help you Work plan and play better now this particular This particular ability to organize your attachments and drive and generate a sheet and do data analysis via Q&A We'll be rolling out to labs users this September and it's just one of the many automations that we're working on in workspace Work space in the Gemini era will continue to unlock new ways of getting things done We're building advanced agent of experiences including customizing how you use Gemini Now as we look to 2025 and beyond we're exploring entirely new ways of working with AI Now with Gemini you have an AI power assistant always at your side But what if you could expand how you interact with AI For example when we work with other people we mention them in comments and docs or we send them emails We have group chats with them etc And it's not just how we collaborate with each other But we each have a specific role to play in the team and as the team works together We build a set of collective experiences and contacts to learn from each other We have the combined set of skills to draw from when we need help So how could we introduce AI into this mix and build on this shared expertise Well, here's one way We're prototyping a virtual Gemini powered teammate This teammate has an identity a workspace account along with a specific role and objectives Let me bring Tony up to show you what I mean Hey Tony Hey everyone okay So let me start by showing you how we set up this virtual teammate As you can see the teammate has this very own account and we can go ahead and give it a name. We'll do something fun like Chip Chips been given a specific job role at the set of descriptions on how to be helpful for the team You see that here and some of the jobs are to monitor and track projects We've listed a few out to organize information and provide context and a few more things Now that we've configured our virtual teammate Let's go ahead and see chip in action to do that. I'll switch us over here to Google chat First when planning for an event like IO we have a ton of chat rooms for various purposes Luckily for me chip is in all of them To quickly catch up I might ask a question like Anyone know if our IO Storyboards are approved Because we've instructed chip to track this project chip searches across all the conversations and knows respond with an answer There it is simple but very helpful Now as the team adds chip to more group chats more files more email threads Chip builds a collective memory of our work together Let's look at an example to show you all switch over to a different room. How about project sapphire over here And here we are discussing a product release coming up and as usual many pieces are still in flight so I can go ahead and ask Are we on track?","The pilot program mentioned in the excerpt is ""Gemini 1.5 Pro."" Its purpose is to enhance tools like ""notebook LM,"" allowing users to interact with educational materials in a more personalized and engaging way. Specifically, it can generate audio overviews and discussions based on the input materials, making learning more interactive and tailored to individual needs, such as providing age-appropriate examples and facilitating conversations around the content."
What is described as the 'magic' that only a teacher can provide?,"What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.
And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube. In fact, you've already seen LearnLam on stage today, when it helps to meer with his son's homework on Android. Now, let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems, which will be available in the Gemini app and web experience, including one called Learning Coach. With Learning Coach, you can get step-by-step study guidance, along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features that Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy, to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use Generative AI. We're also working directly with educators to build more helpful Generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning, and enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real, how much it is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors, and how much more is still to come. This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar.","The 'magic' that only a teacher can provide is described as the fundamental connection, the human connection between a teacher and a student, which is essential to learning."
What is the teacher's favorite part of their job as mentioned in the excerpt?,"What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.
And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube. In fact, you've already seen LearnLam on stage today, when it helps to meer with his son's homework on Android. Now, let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems, which will be available in the Gemini app and web experience, including one called Learning Coach. With Learning Coach, you can get step-by-step study guidance, along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features that Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy, to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use Generative AI. We're also working directly with educators to build more helpful Generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning, and enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real, how much it is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors, and how much more is still to come. This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar.","The teacher's favorite part of their job, as mentioned in the excerpt, is the feeling of having made something better in their classroom."
What is the significance of Gemma's tokenizer in relation to language diversity in India?,"The language is a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navrasa. Navrasa is a model that's trained for Indic languages. It's a fine tune model based on Google's Gemma. We built Navrasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. So we should use AI to make sure that no one is following and no one is using it. Today, the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of Generative AI that we want to bring to every corner of India and the entire world.
Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now since it launched less than three months ago it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, Polly Gemma, our first vision language open model and it's available right now. It's optimized for a range of image captioning visual Q&A and other image labeling tasks so go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model but it's still got a fitness size that's easy for all of you to use. So in a few weeks we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on next-gen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. To wrap up I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. Languages are a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navarasa. Navarasa is a model that's trained for Indic languages. It's a fine tuned model based on Google's Gemma. We built Navarasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. Today the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of generative AI that we want to bring to every corner of India and the entire world.","Gemma's tokenizer is significant in relation to language diversity in India because it enables the model to handle a vast array of words, symbols, and characters across numerous alphabets and language systems. Given India's linguistic diversity, where languages can change every few kilometers, a powerful tokenizer is essential for effectively capturing the nuances of different languages and dialects. This capability allows Gemma to adapt to various cultural contexts and support projects like Navrasa, which is specifically designed for Indic languages. By facilitating communication in native languages, Gemma's tokenizer plays a crucial role in making large language models culturally rooted and accessible, ultimately empowering individuals to solve real-world problems using technology in their preferred language."
What is the purpose of the Navrasa model?,"The language is a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navrasa. Navrasa is a model that's trained for Indic languages. It's a fine tune model based on Google's Gemma. We built Navrasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. So we should use AI to make sure that no one is following and no one is using it. Today, the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of Generative AI that we want to bring to every corner of India and the entire world.
So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on NextGen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. APPLAUSE So wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. The language is a very trusting problem to solve, actually. And given India has a huge variety of languages and it changes every five kilometers. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navarasa. Navarasa is a model that's trained for Indic languages. It's a fine tuned model based on Google's Gemma. We built Navarasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. So we need AI to use this technology that we don't want to ask for any of these. Today, the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of generative AI that we want to bring to every corner of India and the entire world. Lifting to everything that's been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context, and agents are the cutting edge of what this technology can do. Take it to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles, and we're learning from our users, partners, and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines, and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine three NVO create more realistic imagery and videos. We must also consider how they might be misused to spread misinformation. To help last year, we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today, we're expanding Synth ID to two new modalities, text, and video.","The purpose of the Navrasa model is to create instruction-tuned models that expand access to 15 Indic languages, making large language models culturally rooted. It allows people to communicate in their native languages and receive responses in those same languages. The goal is to ensure inclusivity for everyone from all corners of India, leveraging AI to address real-world problems through language technology."
What is the ultimate goal mentioned for the language model being developed?,"This size is optimized by Nvidia to run on next gen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. To wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. Languages are a very trusting problem to solve actually. And given India has a huge variety of languages and it changes every 5 kilometres. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navarasa. Navarasa is a model that's trained for Indic languages. It's a fine tune model based on Google's Gemma. We built Navarasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. We should use AI to make it a technology that no one is following and no one is using it. Today, the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of Generative AI that we want to bring to every corner of India and the entire world. Listening to everything that's been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context and agents are the cutting edge of what this technology can do. It's a technology that's taken to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here, I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like CAM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation, make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine 3NVO create more realistic imagery and videos. We must also consider how there might be misuse to spread misinformation. To help last year we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Jemeni, Imagine, Luria, and VR models.
The language is a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navrasa. Navrasa is a model that's trained for Indic languages. It's a fine tune model based on Google's Gemma. We built Navrasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. So we should use AI to make sure that no one is following and no one is using it. Today, the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of Generative AI that we want to bring to every corner of India and the entire world.","The ultimate goal mentioned for the language model being developed, specifically Navarasa, is to create a model that is culturally rooted, allowing people to communicate in their native languages and receive responses in their native languages. The aspiration is to include everyone from all corners of India and to use AI to empower individuals to solve real-world problems using the language they speak."
How does the speaker envision the role of AI in solving real-world problems?,"This is the end of the video. Thank you. Listening to everything that has been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context, and agents are the cutting edge of what this technology can do, take it to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners, and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting-edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines, and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine three NVO create more realistic imagery and videos. We must also consider how they might be misused to spread misinformation. To help last year, we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today, we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Gemini, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making. And in the coming months, we'll be open sourcing Synth ID text watermarking. This will be available in our updated responsible Gerritive AI Toolkit, which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supposed to appear collaborating with Adobe, Microsoft, Startups, and many others to build that implement standards that improve the transparency of digital media. Now, let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today, our AI advances are helping to solve real-world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases, helping predict floods in more than 80 countries, and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube.
This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning, and memory are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision. We are still in the early days, and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes, and a lot less fun to return them when they don't fit. Imagine if Gemini could do all the steps for you, searching your inbox for the receipt, locating the order number from your email, filling out a return form, and even scheduling a pickup. That's much easier, right? Applause Let's take another example that's a bit more complex. Say you just moved to Chicago. You can imagine Gemini and Chrome working together to help you do a number of things to get ready, organizing, reasoning, synthesizing on your behalf. For example, you will want to explore the city and find services nearby, from dry cleaners to dog walkers. You'll have to update your new address across dozens of websites. Gemini can work across these tasks and will prompt you for more information when needed, so you're always in control. That part is really important as we prototype these experiences. We are thinking hard about how to do it in a way that's private, secure, and works for everyone. These are simple use cases, but they give you a good sense of the types of problems you want to solve. By building intelligence systems, the think ahead, reason, and plan all on your behalf. The power of Gemini with multi-modality, long context, and agents, brings us closer to our ultimate goal, making AI helpful for everyone. We see this as how we will make the most progress against our mission, organizing the world's information across every input, making it accessible via any output, and combining the world's information with the information in your world in a way that's truly useful for you. To fully realize the benefits of AI will continue to break new ground. Google DeepMind is hard at work. To share more, please welcome for the first time on the IOTage, Sir Demis.","The speaker envisions AI as a powerful tool that can help solve real-world problems by enhancing productivity and addressing complex challenges. They highlight several key areas where AI is already making a significant impact:

1. **Scientific Research**: AI is assisting 1.8 million scientists across 190 countries in tackling issues like neglected diseases through tools like AlphaFold.

2. **Disaster Prediction**: AI is being used to predict floods in over 80 countries, showcasing its potential in disaster management and response.

3. **Global Goals Tracking**: Organizations like the United Nations are utilizing AI to track progress on the world's 17 sustainable development goals, demonstrating its role in promoting social and environmental objectives.

4. **Personalized Learning**: The speaker introduces ""LearnLam,"" a new family of AI models designed to enhance educational experiences by making them more personal and engaging. This aims to provide every learner with access to tailored educational support.

5. **Accessibility of Information**: AI is envisioned to make the world's information universally accessible, allowing individuals to have their own personal AI tutors and assistants, thereby transforming learning and education.

Overall, the speaker emphasizes that AI's advancements are not only about technological innovation but also about maximizing benefits for society while addressing potential risks and challenges."
What is the broader impact of Generative AI as discussed in the excerpt?,"This is the end of the video. Thank you. Listening to everything that has been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context, and agents are the cutting edge of what this technology can do, take it to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners, and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting-edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines, and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine three NVO create more realistic imagery and videos. We must also consider how they might be misused to spread misinformation. To help last year, we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today, we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Gemini, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making. And in the coming months, we'll be open sourcing Synth ID text watermarking. This will be available in our updated responsible Gerritive AI Toolkit, which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supposed to appear collaborating with Adobe, Microsoft, Startups, and many others to build that implement standards that improve the transparency of digital media. Now, let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today, our AI advances are helping to solve real-world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases, helping predict floods in more than 80 countries, and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube.
What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.","The broader impact of Generative AI, as discussed in the excerpt, includes several key aspects:

1. **Enhancing Everyday Tasks**: Generative AI is already assisting people in their daily activities, making tasks more efficient and productive.

2. **Addressing Risks and Misuse**: The technology comes with risks, and there is a focus on improving AI models to protect against misuse. This includes practices like red teaming to identify weaknesses in models and developing techniques such as AI-assisted red teaming.

3. **Collaboration and Standards**: There is a commitment to working with various stakeholders, including independent experts and organizations, to establish standards that improve the transparency of digital media and ensure responsible AI development.

4. **Real-World Problem Solving**: Generative AI is being applied to solve significant global challenges, such as aiding scientists in research, predicting natural disasters, and tracking progress on sustainable development goals.

5. **Personalized Learning**: The introduction of tools like LearnLM aims to make learning experiences more personalized and engaging, providing educators with resources to better meet the diverse needs of their students.

6. **Future Possibilities**: Generative AI opens up new possibilities for education, such as the concept of personal AI tutors, which could revolutionize how knowledge is accessed and learned.

Overall, the excerpt emphasizes that while Generative AI has the potential to greatly benefit society, it is crucial to navigate its complexities responsibly to maximize its advantages while mitigating risks."
What are the key highlights of the event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
And this is not just the model completing the code. It's the model coming up with an entire plan and running it. And not only do I get an answer to my initial question about the data set, but the AI agent also produces a fully functional collab notebook based on your instructions. And you can share it with others, allowing for real time collaboration on code, new visualizations, and all of the results. It's available today at labs.google slash code and let us know what you think about it. Now you see why I'm so excited today. You see, we're committed to empowering you to build with genitive AI boldly and responsibly from Gemini power assistance in your favorite IDEs to easy access for our most powerful Gemini model. And it comes with flexibility to use Gemma open models. You have an opportunity to reach anyone anywhere through Google's ecosystem of devices, app experiences, and platform. We continued investing resources to give your projects a boost. Google accelerators and the Google developer portfolio is just a couple of great examples. Our accelerators have guided over 1,300 startups in 84 countries with countless success stories. And that includes 20 startups that have grown to become unicorns. You Genie AI founded by Dr. Sudeth Boyd, it's a recent graduate of our climate change accelerator. You Genie was born out of Dr. Childry's desire to use AI for good. His team helps manufacturing companies to dramatically reduce their carbon emissions. And you Genie's algorithms apply Google DeepMind research which has helped to optimize energy use in Google's own data centers. You Genie was able to streamline their model development and their analytics with accurate emissions data through Google Earth Engine. We love working alongside startups to tackle big problems and invite all of you to apply to an upcoming accelerator soon. If you have one of the 15 million Google developer profiles, you enjoy invitations for early access programs, a chance to participate in research and opportunities to attend special events. Beginning today, your profile is evolving into something even better. Introducing the Google Developer Program. Members of the program can now access new benefits at no cost and I'm going to take you through them. First, you can access Gemini for learning and searching and chatting with documentation. Second, for IDX users, we're now enabling you to go from two work stations up to five. Third, if you're in the Google Cloud Innovators community, you get credits for interactive labs on Google Cloud Skills Boost. Sign up for the Google Developer Program today or if you have an existing profile, you're all set. More benefits are coming soon so be on the lookout. That brings us to the close of today's developer keynote but we're not quite done. There's a lot more to Google I.O than just the keynotes. If you're with us in person, attend the sessions, the workshops, and demos both today and tomorrow. If you're online, you'll find all the sessions available on IO.Google starting this Thursday morning, Mountain View Time. Last year, we kicked off the series of IO connect events to bring the in-person experience from Shoreline to more places that are closer to you. Well, we heard just how much you loved them so we've got three of them lined up this year. Starting in Berlin on June 27th and then we're in Bangalore in July and Beijing in August. And I'll mention, I'll be at all of them so I hope to see you there too. If you can join us at IO connect, come to one of the many community-led IO extended event happening in your local area. Now, I like to share something really cool with you. We had Project Astra watch the keynote this morning. Yes. Let's see what it says. Hey, my name is Michael Chang and we're watching the Google IO keynote while it's happening with Project Astra. Let's take a look. Hey, Gemini, currently watching Google IO keynote right now. You want to watch it with me? Sure, I'd love to watch it with you. What's happening in the keynote? What does it look like? What does it look like they're announcing right now? It looks like they're announcing Gemini 1.5 Flash. The speaker is talking about its speed and efficiency in comparison to the pro model. What are the other features? It looks like Gemini 1.5 Flash is optimized for tasks where low latency and high efficiency matter most. Cool. The screen also shows. Oh, yeah, sorry, I interrupted you. Keep going.","The key highlights of the Google I/O 2024 event include:

1. **Developer Conference**: Google I/O is described as the biggest developer conference, featuring around 4,000 developers in attendance and a two-day program filled with sandboxes, demos, and hands-on workshops.

2. **Keynote Address**: Sundar Pichai kicked off the keynote by discussing Google's progress over the past year and the advancements in Gemini models, which are enhancing various products and features across Google services.

3. **New Announcements**: Several new products and features were announced, including:
   - **Gemini 1.5 Pro** and **Gemini 1.5 Flash**: These models are noted for their performance improvements.
   - **LearnAlem** and **Imagine Three**: Highlighted as high-quality image generation models.
   - **Fio**, **VideoFX**, **Gemini Live**, **Asphoto**, and **Gemini Nano**: Additional features and tools aimed at enhancing user experience.

4. **Accessibility Features**: The event showcased how Gemini's multi-modal capabilities can improve accessibility, particularly in describing images for users with visual impairments.

5. **Developer Tools and Programs**: Updates to Android Studio were unveiled, along with new tools to assist developers in building applications. The Google Developer Program was introduced, offering new benefits and resources for developers.

6. **Community Engagement**: The event emphasized community involvement with plans for IO connect events in Berlin, Bangalore, and Beijing, as well as local community-led IO extended events.

7. **Live Demonstrations**: There were live demos, including a demonstration of the AI agent producing a functional collab notebook based on user instructions.

These highlights reflect a strong focus on innovation, developer support, and community engagement at the event."
Who were the presenters at the event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The presenters at the event included:

1. **Mark Rubier** - He hosted the I-O Pre-Show and used the new DJ mode in the music FX tool.
2. **Sundar Pichai** - He kicked off the main Google keynote, sharing progress and technical advances.
3. **Janine Banks** - She led the Developer X and Core team and announced updates and new tools for developers.
4. **Seneca Meeks** - A software engineer on the Quantum AI team, who demonstrated a quantum computing problem.
5. **Josh Woodward** - VP of Google Labs, who discussed the idea behind the Googlers-only demo slam.

Additionally, there were American Sign Language interpreters present for the keynote."
What new features were introduced for developers at Google I/O 2024?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","At Google I/O 2024, several new features and updates were introduced for developers, including:

1. **Updates to Android Studio**: Enhancements were made to help developers leverage Gemini 1.5 Pro to improve and accelerate their development processes.

2. **Real-time Code Generation**: Developers were able to take a mock-up of a screen and generate the code for it in Android Studio, even creating an interactive app in real-time.

3. **AI and ML Technologies**: New easy-to-use AI and machine learning technologies were showcased, including Multimodal Search.

4. **Developer Tools**: Tools like Compose were highlighted for building across various form factors.

5. **Cloud Solutions**: New summarization tools for workspace and reliable solutions on cloud were introduced.

6. **Experiential Demos**: Six demos in the AI sandbox showcased projects built on the latest AI models, including Gemini's image recognition and storytelling capabilities.

7. **Quantum Computing Demonstration**: A demonstration of a quantum computer's capabilities, showcasing a problem that would take a classical computer a billion years to compute, solved in minutes.

These features and updates aimed to enhance the development experience and empower developers with new tools and technologies."
How many startups have been supported by Google accelerators globally?,"I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!
We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision, we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course, search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities are high bar for information quality, our decades of experience connecting you to the richness of the web, the result is a product that does the work for you. Google search is a generative AI at the scale of human curiosity and it's our most exciting chapter of search yet to tell you more. Here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. Now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind and whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. To make this possible, we're introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates studios in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning.","The provided context does not contain information regarding the number of startups supported by Google accelerators globally. Therefore, I cannot provide an answer to that question based on the given text."
What was the atmosphere of the event described as?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The atmosphere of the event was described as buzzing with excitement as attendees made their way in. It was characterized as a unique opportunity for the company to come together, with a vibe reminiscent of a developer festival, highlighting the presence of about 4,000 developers in the audience and the overall excitement surrounding the innovations being shared."
What core technology is being utilized for the short film being created at Google Farms?,"Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it, right? As much as you wish. And so we've been hearing that feedback is that it allows for more... ...multinality, more iteration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want if you enter the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. Those are really enabling us to be more creative and to share that creativity with each other.
We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.","The core technology being utilized for the short film being created at Google Farms is Google DeepMind's generative video model, which has been trained to convert input text into output video."
How does the generative video model enhance the filmmaking process according to the excerpt?,"Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it, right? As much as you wish. And so we've been hearing that feedback is that it allows for more... ...multinality, more iteration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want if you enter the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. Those are really enabling us to be more creative and to share that creativity with each other.
We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.","The generative video model enhances the filmmaking process by allowing filmmakers to bring ideas to life that were previously not possible, enabling visualization on a time scale that is 10 or 100 times faster than before. It facilitates more functionality, inspiration, and improvisation, allowing creators to make mistakes faster, which is crucial in the artistic process. The model's multi-modal capabilities help capture nuances from prompts, incorporating cinematic techniques and visual effects, thereby providing total creative control. Ultimately, it democratizes the filmmaking process, suggesting that everyone can become a director and share their stories, fostering greater creativity and understanding among people."
What benefits does Gemini's multi-modal capabilities provide in the context of model training?,"There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son and I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, she really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that of course means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes, the foundation for understanding how objects move and interact. Ah, yes, this is where multimodal really shines. Now it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah, can you give my son Jimmy a basketball example? Hey, Jimmy, that's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay, so first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say, the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity or Isaac Newton. But nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format. And it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Shar. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead. Work across software and systems. All to get something done on your behalf. And most importantly, under your supervision. We are still in the early days and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes. And a lot less fun to return them when they don't fit. Imagine if Gemini could do all the steps for you.
There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son and I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, she really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that of course means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes, the foundation for understanding how objects move and interact. Ah, yes, this is where multimodal really shines. Now it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah, can you give my son Jimmy a basketball example? Hey, Jimmy, that's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay, so first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say, the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity or Isaac Newton. But nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format. And it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Shar. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead. Work across software and systems. All to get something done on your behalf. And most importantly, under your supervision. We are still in the early days and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes. And a lot less fun to return them when they don't fit.","Gemini's multi-modal capabilities offer several benefits in the context of model training:

1. **Personalization**: The ability to process and generate content in various formats (text, audio, etc.) allows for tailored educational experiences. For example, it can create personalized audio discussions based on text materials, making learning more engaging and effective for individual users.

2. **Interactivity**: Users can interact with the model in real-time, steering conversations and asking for specific examples or clarifications. This dynamic interaction enhances the learning process and allows for immediate feedback.

3. **Integration of Diverse Inputs**: Gemini can take multiple types of input (like worksheets, slide decks, and textbooks) and synthesize them into coherent outputs, such as summaries or study guides. This capability helps in training models to understand and integrate information from various sources.

4. **Enhanced Understanding**: By generating age-appropriate examples and explanations (like using basketball to explain physics concepts), Gemini helps users grasp complex ideas more easily, which can improve the model's ability to relate concepts across different domains.

5. **Efficiency in Learning**: The model can automate the creation of study aids, quizzes, and FAQs, saving time for both educators and students. This efficiency can lead to more effective training sessions and better retention of information.

6. **Exploration of New Use Cases**: The multi-modal approach opens up new possibilities for applications, such as AI agents that can reason, plan, and execute tasks across different systems, which can be beneficial for training models in real-world scenarios.

Overall, Gemini's multi-modal capabilities enhance the training process by making it more interactive, personalized, and efficient, ultimately leading to better learning outcomes."
What is the significance of storytelling as mentioned in the excerpt?,"What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.
That's just the beginning. Okay. Now you will choose the prompts and we'll leave it up to chance. As I said, everything here unscripted unplanned. I didn't know this was going to happen. What's this? I didn't even know you guys were going to be here. What are you guys doing here? Six props, bagpipes, Persian tar, jembe, dulcimer, flamenco guitar, Detroit techno. You know what those things are? You know what this is? You better. Shout them out. What do you want to hear? Flamenco guitar. Persian took bagpipes. No, no bagpipes. They said, no, get the bagpipes out of here. Okay. Persian tar, jembe, flamenco guitar. Jembe. What I'm going to do is pop these in here and then I'm going to use them as samples and I'm going to make something with this. Ideally, I want it to be simple so that I can use it as an element in like a fuller composition. Do you understand what I'm saying? It's okay if you don't. I don't. Jembe. Flamenco guitar. Go ahead, baby. Give it a little more. Persian tar. Hey. I've now muted. reacts, deltarict. Monoyo. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. Shaburam. Shaburam. Shaburam. Go go go go go go go go go. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam.","The excerpt emphasizes the importance of the human connection between a teacher and a student, which is essential to learning. Storytelling is significant as it allows teachers to create engaging and relatable experiences for their students, helping to make learning more active and tailored to individual needs. By utilizing generative AI tools like LearnLM, teachers can enhance their storytelling abilities, differentiate instruction, and provide resources that resonate with each student's unique learning style. This approach not only enriches the educational experience but also fosters a deeper understanding and connection, which is described as the ""magic"" that only a teacher can provide."
What does the speaker imply about the future of directing with the advancements in technology?,"We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.
Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it, right? As much as you wish. And so we've been hearing that feedback is that it allows for more... ...multinality, more iteration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want if you enter the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. Those are really enabling us to be more creative and to share that creativity with each other.","The speaker implies that advancements in technology, particularly through tools like Google DeepMind's generative video model, will democratize the art of directing. They suggest that everyone will have the opportunity to become a director, as these technologies enable more creative control and facilitate storytelling. The ability to visualize ideas quickly and make mistakes faster will empower more individuals to express their creativity and share their stories, ultimately enhancing understanding among people. The speaker envisions a future where storytelling becomes more accessible to everyone, not just traditional filmmakers."
What are the key highlights of the Google I/O 2024 event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
And this is not just the model completing the code. It's the model coming up with an entire plan and running it. And not only do I get an answer to my initial question about the data set, but the AI agent also produces a fully functional collab notebook based on your instructions. And you can share it with others, allowing for real time collaboration on code, new visualizations, and all of the results. It's available today at labs.google slash code and let us know what you think about it. Now you see why I'm so excited today. You see, we're committed to empowering you to build with genitive AI boldly and responsibly from Gemini power assistance in your favorite IDEs to easy access for our most powerful Gemini model. And it comes with flexibility to use Gemma open models. You have an opportunity to reach anyone anywhere through Google's ecosystem of devices, app experiences, and platform. We continued investing resources to give your projects a boost. Google accelerators and the Google developer portfolio is just a couple of great examples. Our accelerators have guided over 1,300 startups in 84 countries with countless success stories. And that includes 20 startups that have grown to become unicorns. You Genie AI founded by Dr. Sudeth Boyd, it's a recent graduate of our climate change accelerator. You Genie was born out of Dr. Childry's desire to use AI for good. His team helps manufacturing companies to dramatically reduce their carbon emissions. And you Genie's algorithms apply Google DeepMind research which has helped to optimize energy use in Google's own data centers. You Genie was able to streamline their model development and their analytics with accurate emissions data through Google Earth Engine. We love working alongside startups to tackle big problems and invite all of you to apply to an upcoming accelerator soon. If you have one of the 15 million Google developer profiles, you enjoy invitations for early access programs, a chance to participate in research and opportunities to attend special events. Beginning today, your profile is evolving into something even better. Introducing the Google Developer Program. Members of the program can now access new benefits at no cost and I'm going to take you through them. First, you can access Gemini for learning and searching and chatting with documentation. Second, for IDX users, we're now enabling you to go from two work stations up to five. Third, if you're in the Google Cloud Innovators community, you get credits for interactive labs on Google Cloud Skills Boost. Sign up for the Google Developer Program today or if you have an existing profile, you're all set. More benefits are coming soon so be on the lookout. That brings us to the close of today's developer keynote but we're not quite done. There's a lot more to Google I.O than just the keynotes. If you're with us in person, attend the sessions, the workshops, and demos both today and tomorrow. If you're online, you'll find all the sessions available on IO.Google starting this Thursday morning, Mountain View Time. Last year, we kicked off the series of IO connect events to bring the in-person experience from Shoreline to more places that are closer to you. Well, we heard just how much you loved them so we've got three of them lined up this year. Starting in Berlin on June 27th and then we're in Bangalore in July and Beijing in August. And I'll mention, I'll be at all of them so I hope to see you there too. If you can join us at IO connect, come to one of the many community-led IO extended event happening in your local area. Now, I like to share something really cool with you. We had Project Astra watch the keynote this morning. Yes. Let's see what it says. Hey, my name is Michael Chang and we're watching the Google IO keynote while it's happening with Project Astra. Let's take a look. Hey, Gemini, currently watching Google IO keynote right now. You want to watch it with me? Sure, I'd love to watch it with you. What's happening in the keynote? What does it look like? What does it look like they're announcing right now? It looks like they're announcing Gemini 1.5 Flash. The speaker is talking about its speed and efficiency in comparison to the pro model. What are the other features? It looks like Gemini 1.5 Flash is optimized for tasks where low latency and high efficiency matter most. Cool. The screen also shows. Oh, yeah, sorry, I interrupted you. Keep going.","The key highlights of the Google I/O 2024 event include:

1. **Developer Conference**: Google I/O is positioned as the biggest developer conference, featuring around 4,000 developers in attendance and a variety of programming over two days.

2. **Innovations and Announcements**: The keynote featured significant announcements, including:
   - **Gemini 1.5 Pro** and **Gemini 1.5 Flash**, showcasing advancements in AI capabilities.
   - New features across various Google products, including search, workspace, photos, and Android.

3. **Live Streaming**: The keynote was streamed live, attracting over a million viewers from more than 186 countries.

4. **Music FX DJ Technology**: Mark Rubier hosted the I-O Pre-Show using the new DJ mode in the music FX tool, demonstrating its capabilities in a live setting.

5. **Accessibility Features**: The event highlighted accessibility improvements, particularly how Gemini's multi-modal capabilities can enhance features like image description for visually impaired users.

6. **Developer Tools and Resources**: Announcements included updates to Android Studio and the introduction of the Google Developer Program, which offers new benefits and access to resources for developers.

7. **Community Engagement**: The event emphasized community involvement with sessions, workshops, and demos available both in-person and online, along with upcoming IO connect events in various global locations.

8. **Project Astra**: A demonstration of Project Astra, which interacted with the keynote in real-time, showcasing the integration of AI in understanding and summarizing live events.

Overall, the event focused on showcasing Google's commitment to innovation in AI, developer support, and community engagement."
Who were the presenters at the Google I/O 2024 event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The presenters at the Google I/O 2024 event included:

1. **Sundar Pichai** - He kicked off the keynote and shared progress over the past year, discussing technical advances in Gemini models.
2. **Mark Rubier** - He hosted the I-O Pre-Show and used the new DJ mode in the music FX tool.
3. **Janine Banks** - She led the Developer X and Core team and announced updates and new tools for developers.
4. **Seneca Meeks** - A software engineer on the Quantum AI team, who demonstrated a problem solved by a quantum computer.
5. **Josh Woodward** - VP of Google Labs, who discussed the idea behind the Googlers-only demo slam.

Additionally, there were various presenters involved in the demos and breakout sessions throughout the event."
What new tools were announced for developers during the event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","During the event, several new tools were announced for developers, including:

1. **Updates to Android Studio**: These updates help developers leverage Gemini 1.5 Pro to improve and accelerate their development processes.
2. **Code Generation Feature**: Developers were able to take a mock-up of a screen and generate the code for it in Android Studio, even creating an interactive app in real-time.
3. **AI ML Technologies**: New easy-to-use AI machine learning technologies were introduced, such as Multimodal Search.
4. **Developer Tools for Android**: Tools like Compose were highlighted for building across all form factors.
5. **Cloud Solutions**: New summarization tools for workspace and reliable solutions on the cloud were also announced.

Additionally, there were various demos showcasing products and features related to web, mobile, cloud, and AI technologies."
How many startups have been supported by Google accelerators?,"I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!
We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision, we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course, search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities are high bar for information quality, our decades of experience connecting you to the richness of the web, the result is a product that does the work for you. Google search is a generative AI at the scale of human curiosity and it's our most exciting chapter of search yet to tell you more. Here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. Now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind and whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. To make this possible, we're introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates studios in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning.","The provided context does not contain information about the number of startups supported by Google accelerators. Therefore, I cannot answer that question based on the given text."
What was the atmosphere of the Google I/O 2024 event described as?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The atmosphere of the Google I/O 2024 event was described as buzzing with excitement, with attendees making their way into the Shoreline Amphitheater. It was characterized as a unique opportunity for the company to come together and celebrate innovations, with a vibe reminiscent of a developer festival. The event featured a lively pre-show hosted by musician Mark Rubier and included a range of programming for developers, hands-on workshops, and engaging demos, contributing to an energetic and enthusiastic environment."
What was the initial reaction of the speaker to the announcement of the 1 million token context window?,"I remember at the announcement the 1 million token context window and my first reaction was there's no way they were able to achieve this. I wanted to test this technical skills so I uploaded a line chart. It was temperatures between Tokyo and Berlin and how they were across the 12 months of year. So I got in there and I threw in the Python library that I was really struggling with. And I just asked it a simple question. And it nailed it. It could find specific references, the comments in the code and specific requests that people had made and other issues that people had had but then suggest a fix for it that related to what I was working on. I immediately tried to kind of crash it so I took you know four or five research papers out on my desktop and it's in my mulling experience when you add so much text and then you see the kind of amount of tokens your ad is not even at half the capacity. It felt a little bit like Christmas because you saw things kind of peppered up to the top of your feet about like oh wow I built this thing or oh it's doing this and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf and I shot a video just panning my camera along the bookshelf and I fed the video into the model. It gave me the titles and authors of the books even though the authors weren't visible on those book spines and on the bookshelf there was a squirrel nutcracker sat in front of the book truncating the title you could just see the word site C and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click like this is it. I thought like I had like a super part in my hands. It was poetry it was beautiful I was so happy it just this this is going to be amazing this is this is going to help people. This is kind of where the future of language models are going. Personalize to you not because you trained it to be personal to you but personal to you because you can give it such a fast understanding of who you are.
I immediately tried to kind of crash it. So I took, you know, four or five research papers out of the desktop, and it's a mind-blowing experience when you add so much text. And then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas because you saw things kind of peppered up to the top of your feed about like, oh, wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those book spines, and on the bookshelf there was a squirrel nut cracker sat in front of the book, truncating the title. You could just see the word site C, and it still gets the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry. It was beautiful. I was so happy. It just, this is going to be amazing. This is going to help people. This is kind of where the future of language models are going. Personalize to you not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been rolling out the bookshelf. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we have talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parents group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multimodal model, we can do so much more. To show you an early demo of an audio output in notebook LM, here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.","The initial reaction of the speaker to the announcement of the 1 million token context window was skepticism. They expressed disbelief by saying, ""there's no way they were able to achieve this."""
What experiment did the speaker conduct to test the capabilities of the model?,"Thanks, and it's so great to be here. Ever since I was a kid playing chess for the England Junior Team, I'd been thinking about the nature of intelligence. I was captivated by the idea of a computer that could think like a person. It's ultimately why it became a programmer and studied neuroscience. I co-founded DeepMind in 2010 with the goal of one day building AGI, Artificial General Intelligence, a system that has human level cognitive capabilities. I've always believed that if we could build this technology responsibly, its impact would be truly profound and it could benefit humanity in incredible ways. Last year, we reached a milestone on that path when we formed Google DeepMind, combining AI talent from across the company into one super unit. Since then, we built AI systems that can do an amazing range of things, from turning language and vision into action for robots, navigating complex virtual-threading environments, solving a limpiav level math problems, and even discovering thousands of new materials. Just last week, we announced our next-generation alpha-fold model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. All of this was made possible with the best infrastructure for the AI era, including our highly optimized tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multi-modal because that's how we interact with and understand the world around us. We've built a variety of models for different use cases, you've seen how powerful Gemini 1.5 Pro is, but we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale while still featuring multi-modal reasoning capabilities and breakthrough-long context. Flash is optimized for tasks where low latency and efficiency matter most. Starting today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI, and developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. You'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models, but we're always striving to push the state of the art even further. At any one time, we have many different models in training, and we use our very large and powerful ones to help teach and train our production-ready models. Together with user feedback, this cutting-edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multi-modal AI. And this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multi-modal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action, and it would have to be proactive, teachable and personal, so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multi-modal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called?
There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son and I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, she really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that of course means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes, the foundation for understanding how objects move and interact. Ah, yes, this is where multimodal really shines. Now it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah, can you give my son Jimmy a basketball example? Hey, Jimmy, that's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay, so first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say, the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity or Isaac Newton. But nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format. And it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Shar. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead. Work across software and systems. All to get something done on your behalf. And most importantly, under your supervision. We are still in the early days and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes. And a lot less fun to return them when they don't fit.","The speaker conducted an experiment to test the capabilities of the model by demonstrating a prototype that involved two parts captured in real time. During the test, the speaker asked the model to identify when it saw something that makes sound, to which the model responded by identifying a speaker. This interaction showcased the model's ability to process information quickly and understand context in a conversational manner."
What was the outcome when the speaker tested the model with multiple research papers?,"I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.
I immediately tried to kind of crash it. So I took, you know, four or five research papers out of the desktop, and it's a mind-blowing experience when you add so much text. And then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas because you saw things kind of peppered up to the top of your feed about like, oh, wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those book spines, and on the bookshelf there was a squirrel nut cracker sat in front of the book, truncating the title. You could just see the word site C, and it still gets the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry. It was beautiful. I was so happy. It just, this is going to be amazing. This is going to help people. This is kind of where the future of language models are going. Personalize to you not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been rolling out the bookshelf. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we have talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parents group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multimodal model, we can do so much more. To show you an early demo of an audio output in notebook LM, here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.","The speaker described the experience as ""mind-blowing"" when they tested the model with multiple research papers. They noted that even after adding a significant amount of text, the model was not even at half its capacity. This led to a feeling of excitement, akin to ""Christmas,"" as they observed unexpected results and capabilities from the model. The speaker felt they had a powerful tool in their hands, expressing happiness and optimism about the potential of the model to help people and its future in language processing."
How did the model perform when the speaker fed it a video of their bookshelf?,"I immediately tried to kind of crash it. So I took, you know, four or five research papers out of the desktop, and it's a mind-blowing experience when you add so much text. And then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas because you saw things kind of peppered up to the top of your feed about like, oh, wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those book spines, and on the bookshelf there was a squirrel nut cracker sat in front of the book, truncating the title. You could just see the word site C, and it still gets the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry. It was beautiful. I was so happy. It just, this is going to be amazing. This is going to help people. This is kind of where the future of language models are going. Personalize to you not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been rolling out the bookshelf. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we have talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parents group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multimodal model, we can do so much more. To show you an early demo of an audio output in notebook LM, here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.
I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.","The model performed impressively when the speaker fed it a video of their bookshelf. It was able to identify the titles and authors of the books, even though the authors' names were not visible on the spines of the books. Additionally, it correctly identified a book despite part of the title being obscured by a squirrel nutcracker placed in front of it. This demonstrated the model's capability to understand and process visual information effectively, showcasing the range of possibilities it offers. The speaker expressed a sense of amazement and happiness at the model's performance, indicating that they felt they had a powerful tool in their hands that could significantly help people."
What did the speaker mean by saying the model felt like a 'super part' in their hands?,"I immediately tried to kind of crash it. So I took, you know, four or five research papers out of the desktop, and it's a mind-blowing experience when you add so much text. And then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas because you saw things kind of peppered up to the top of your feed about like, oh, wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those book spines, and on the bookshelf there was a squirrel nut cracker sat in front of the book, truncating the title. You could just see the word site C, and it still gets the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry. It was beautiful. I was so happy. It just, this is going to be amazing. This is going to help people. This is kind of where the future of language models are going. Personalize to you not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been rolling out the bookshelf. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we have talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parents group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multimodal model, we can do so much more. To show you an early demo of an audio output in notebook LM, here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.
I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.","When the speaker referred to the model feeling like a ""super part"" in their hands, they were expressing a sense of empowerment and excitement about the capabilities of the model. This phrase suggests that the speaker felt they had access to a powerful tool or technology that could significantly enhance their ability to accomplish tasks, similar to how a superhero might possess extraordinary abilities. The speaker was likely highlighting the model's advanced features, such as its ability to understand and process information quickly and effectively, which made them feel optimistic about its potential to help people and transform their experiences with technology."
What significant changes have occurred in the past year according to the document?,"Google's ambitions and artificial intelligence. Who wants a Gemini? AI is rolling out to work so it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas. And new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never to old bird-tree house. We trained for things. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started.
I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. I got the news, dude. I got the news, dude. I got therada, I got theirdoooo. I got the news, dude. I got the news, dude. I got the news, dudes. I got the news, dude. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Me Organize, Since We Launched. And now, we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini-for-Works-based to the organization, and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5-Private Pro unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now, we've been trying to find a contractor to fix our roofs, and with Work Travel, I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read, and luckily for me, I can simply tap the summarized option up top and skip reading this long back and forth. Now, Gemini pulls up this helpful mobile card as an overlay, and this is where I can read a nice summary of all the salient information that I need to know. So, I see here that we have a quote from Jeff at Green Roofing, and he's ready to start. Now, I know we had other bids, and I don't remember the details. Previously, I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now, I can simply type out my question right here in the mobile card and say something like, compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox, for example, when am I shoes arriving, or what time the doors open for the next game, without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway, back to my roof. It looks like Gemini has found details that I got from two other contractors in completely different email threads, and I have this really nicely organized summary, and I can do a quick comparison. So, it seems like Jeff's quote was right in the middle, and he can start immediately, so Green Roofing it is. I'll open that last email from Jeff and confirm the project, and look at that. I see some suggested replies from Gemini. Now, what is really, really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread, and that Jeff was ready to start, so offers me a few customized options based on that context. So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example.","According to the document, significant changes in the past year include:

1. The rollout of artificial intelligence (AI) technologies, specifically mentioning the introduction of Gemini, which is transforming the way people work.
2. New beginnings and the discovery of new ideas and solutions to longstanding problems.
3. An increase in productivity, as exemplified by a customer who improved their customer support team's productivity by over 30% using Gemini.
4. The expansion of features such as automatic language detection and real-time captions in meetings, now available in 68 languages.
5. The introduction of new capabilities in Gmail Mobile that enhance user experience, such as summarizing email threads and providing quick answers to questions without extensive searching.

Overall, the document emphasizes a year of rapid advancements and improvements in technology and productivity tools."
What metaphor is used to describe the pace of change over the past months?,"Google's ambitions and artificial intelligence. Who wants a Gemini? AI is rolling out to work so it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas. And new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never to old bird-tree house. We trained for things. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started.
Our model even uses contextual factors like the time of the year. So since it's warm in Dallas, you can get rooftop patios as an idea. And it pulls everything together into a dynamic whole page experience. You'll start to see this new AI organized search results page when you look for inspiration, starting with dining and recipes, and coming to movies, music, books, hotels, shopping, and more. Today you've seen how you can bring any question to search and Google takes the work out of searching. But your questions aren't limited to words in a text box and sometimes even that picture can't tell the whole story. Earlier, Demis showed you our latest advancements in video understanding. And I'm really excited to share that soon you'll be able to ask questions with video right in Google search. Let me introduce Rose to show you this in a live demo. Thank you, Liz. I have always known a record player. And I got this one and some finals that you're still recently. But when I go to play it, this thing keeps sliding off. I have no idea how to fix it. Well, where to even start? Before, I would have pieced together a bunch of searches to try to figure this out. Like, what make is this record player? What's the model? And what does this thing actually call? But now, I can just ask of a video. So let's try it. Let's do a live demo. I'm going to take a video and ask Google why will this not stay in place? In the near instant, Google gives me an AI overview. I get some reasons this might be happening and steps I can take to troubleshoot. So it's like, first, this is called a toner. Very helpful. And it looks like it may be unbalanced and there's some really helpful steps here. And I love that because I'm new to all this. I can check out this helpful link from AudioTechnica to learn even more. So that was pretty quick. There. Let me walk you through what just happened. Thanks to your combination of our steady art speech models, our deep visual understanding, and our custom Gemini model, and our search was able to understand the question I asked out loud, break down the video frame by frame. Each frame was fed into Gemini's long context window you heard about earlier today. So this search could then pinpoint the exact make a model of my record player and make sense of the motion across frames, so identify that the toner was drifting. Search fan down and comb the web to find relevant insights from articles, forums, videos, and more. And it stitched all of this together into my AI overview. The result was music to my ears. Back to your lives. Applause Everything you saw today is just a glimpse of how we're reimagining Google Search in the Gemini era. We're taking the very best of what makes Google Google. All the reasons why billions of people turn to Google Search and have relied on us for decades. And we're bringing in the power of Gemini's agent of capabilities. So Google will do the searching, the researching, the planning, the brainstorming, and so much more. All you need to do is just ask. You'll start to see these features rolling out in search in the coming weeks. Opt into search labs to be among the first to try them out. Now let's take a look at how this all comes together in Google Search this year. I got that hush, that straight up swing. I got that, no, no, no, no, no, no, no, damn. Do my thing, do my thing. I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. You know, you know, you know, you know, you know, you know. I got the news, dude. I got the new game. Yeah, you know? Yeah, you know? I got the news, dude. I got the news, no. No, no, no, no. No, no, no, no, no. I got the news, dude. I got the news, dude.","The metaphor used to describe the pace of change over the past months is ""We filled days like they were weeks"" and ""more happened in months than it's happened in years."" This suggests that the changes and developments have been so rapid and significant that time feels compressed, with a lot of progress occurring in a short span."
What does the phrase 'Cannonball!' signify in the context of the document?,"Google's ambitions and artificial intelligence. Who wants a Gemini? AI is rolling out to work so it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas. And new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never to old bird-tree house. We trained for things. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started.
I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. I got the news, dude. I got the news, dude. I got therada, I got theirdoooo. I got the news, dude. I got the news, dude. I got the news, dudes. I got the news, dude. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Me Organize, Since We Launched. And now, we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini-for-Works-based to the organization, and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5-Private Pro unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now, we've been trying to find a contractor to fix our roofs, and with Work Travel, I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read, and luckily for me, I can simply tap the summarized option up top and skip reading this long back and forth. Now, Gemini pulls up this helpful mobile card as an overlay, and this is where I can read a nice summary of all the salient information that I need to know. So, I see here that we have a quote from Jeff at Green Roofing, and he's ready to start. Now, I know we had other bids, and I don't remember the details. Previously, I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now, I can simply type out my question right here in the mobile card and say something like, compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox, for example, when am I shoes arriving, or what time the doors open for the next game, without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway, back to my roof. It looks like Gemini has found details that I got from two other contractors in completely different email threads, and I have this really nicely organized summary, and I can do a quick comparison. So, it seems like Jeff's quote was right in the middle, and he can start immediately, so Green Roofing it is. I'll open that last email from Jeff and confirm the project, and look at that. I see some suggested replies from Gemini. Now, what is really, really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread, and that Jeff was ready to start, so offers me a few customized options based on that context. So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example.","In the context of the document, the phrase 'Cannonball!' signifies a moment of excitement and boldness, likely representing a leap into new experiences or initiatives. It suggests a sense of enthusiasm and a willingness to take risks, reflecting the overall theme of embracing change and innovation, particularly in relation to advancements in artificial intelligence and the transformative impact it has on work and creativity. This phrase encapsulates the spirit of making significant strides and taking decisive actions, akin to jumping into a pool with a cannonball splash, which aligns with the document's message about new beginnings and the rapid pace of progress."
How does the document suggest the changes have impacted different groups of people?,"I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. I got the news, dude. I got the news, dude. I got therada, I got theirdoooo. I got the news, dude. I got the news, dude. I got the news, dudes. I got the news, dude. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Me Organize, Since We Launched. And now, we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini-for-Works-based to the organization, and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5-Private Pro unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now, we've been trying to find a contractor to fix our roofs, and with Work Travel, I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read, and luckily for me, I can simply tap the summarized option up top and skip reading this long back and forth. Now, Gemini pulls up this helpful mobile card as an overlay, and this is where I can read a nice summary of all the salient information that I need to know. So, I see here that we have a quote from Jeff at Green Roofing, and he's ready to start. Now, I know we had other bids, and I don't remember the details. Previously, I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now, I can simply type out my question right here in the mobile card and say something like, compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox, for example, when am I shoes arriving, or what time the doors open for the next game, without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway, back to my roof. It looks like Gemini has found details that I got from two other contractors in completely different email threads, and I have this really nicely organized summary, and I can do a quick comparison. So, it seems like Jeff's quote was right in the middle, and he can start immediately, so Green Roofing it is. I'll open that last email from Jeff and confirm the project, and look at that. I see some suggested replies from Gemini. Now, what is really, really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread, and that Jeff was ready to start, so offers me a few customized options based on that context. So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example.
Google's ambitions and artificial intelligence. Who wants a Gemini? AI is rolling out to work so it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas. And new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never to old bird-tree house. We trained for things. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started.","The document suggests that the changes brought about by the introduction of Gemini and its capabilities have positively impacted various groups of people, particularly businesses and consumers. It highlights that tens of thousands of customers have been using features like Help Me Right, Help Me Visualize, and Help Me Organize, which have improved productivity significantly—specifically mentioning a 30% improvement in the productivity of a customer support team at a local business in California.

Additionally, the document emphasizes that the new features, such as automatic language detection and real-time captions, have expanded to 68 languages, indicating an inclusive approach that benefits a diverse range of users. The improvements in Gmail, such as the ability to summarize long email threads and provide contextual smart replies, suggest that these changes make communication and information management easier for individuals, enhancing their overall experience and efficiency in both personal and professional contexts. Overall, the document conveys a sense of excitement about the potential of these advancements to benefit everyone involved."
What is implied by the statement 'we're just getting started'?,"Google's ambitions and artificial intelligence. Who wants a Gemini? AI is rolling out to work so it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas. And new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never to old bird-tree house. We trained for things. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started.
That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways. And thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AGI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases tenfold. Google was built for this. For 25 years, we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensor processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Amtropic have trained their models on TPUs as well. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs, we are proud to offer CPUs and GPUs to support any workload that includes the new Axion processes we announced last month our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge Blackwell GPUs available in early 2025. We are fortunate to have a long-standing partnership with NVIDIA excited to bring Blackwell's capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying the raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago we created search to help people make sense of the waves of information moving online. With each platform shift we have delivered breakthroughs to help answer your questions better. On mobile we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk.","The statement ""we're just getting started"" implies that there is much more to come in terms of progress, innovation, and development. It suggests that the current achievements or advancements are only the beginning of a larger journey, indicating optimism and the potential for future growth and breakthroughs, particularly in the context of artificial intelligence and technology. This phrase often conveys a sense of excitement and anticipation for what lies ahead, reinforcing the idea that significant changes and improvements are expected to continue unfolding."
What is the significance of the Gemini 1.5 Pro model introduced at Google I/O 2024?,"Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
A Google we are fully in our Gemini era. Today all of our two billion user products use Gemini. Gemini 1.5 Pro is available today in Workspace Labs. Let's see how this comes to life with Google Workspace. People are always searching their emails and Gmail. We are working to make it much more powerful with Gemini. Now we can ask Gemini to summarize all recent emails from the school. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. People love using photos to search across their life. With Gemini, you're making that a whole lot easier. And ask photos can also help you search your memories in a deeper way. For example, you might be reminiscing about your daughter Lucius, early milestones. You can ask photos, show me how Lucius swimming has progressed. Here Gemini goes beyond a simple search, recognizing different contexts and photos packages it up all together in a summary. Unlocking knowledge across formats is why we build Gemini to be multimodal from the ground up. It's one model with all the modalities built in. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. So today, we are expanding the context window to two million tokens. So far, we've talked about two technical advances, multimodality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. But what if we could go even further? That's one of the opportunities we see with AI agents. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Here's a video of our prototype, which we'll see has two parts. Each part was captured in a single take in real time. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector, IV. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. Give me a banner name for this duo. Golden Stripes. Nice. Thanks, Gemini. Today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context. There's one more area I'm really excited to share with you. Our teams have made some incredible progress in genitive video. Today, I'm excited to announce our newest, most capable genitive video model, called VO. VO creates high quality 1080p videos from text, image and video prompts. It can capture the details of your instructions in different visual and cinematic styles. For 25 years, we have invested in world-class technical infrastructure. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. Google Search is a generative AI at the scale of human curiosity, and it's our most exciting chapter of Search yet. All the advancements you'll see today are made possible by a new Gemini model, customized for Google Search. What really sets this apart is our three unique strengths. This is Search in the Gemini era. By the end of the year, AI overviews will come to over a billion people. We're making AI overviews even more helpful for your most complex questions. The types that are really more like 10 questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. I'm really excited to share that soon you'll be able to ask questions with video. Why will this not stay in place? In the near-instead, Google gives me an AI overview. I guess some reasons this might be happening and steps I can take to troubleshoot.","The Gemini 1.5 Pro model introduced at Google I/O 2024 is significant for several reasons:

1. **Multimodal Capabilities**: Gemini 1.5 Pro is designed to handle multiple types of inputs simultaneously, including text, images, audio, and video. This allows developers to create applications that can interleave these modalities, enhancing the user experience and enabling more complex interactions.

2. **Large Context Window**: The model supports a massive context window of up to 2 million tokens, which is particularly beneficial for applications requiring extensive context, such as summarizing long documents or processing large datasets.

3. **Quality Improvements**: The 1.5 Pro version includes a series of quality enhancements aimed at delivering higher quality responses, making it suitable for complex tasks where accuracy and detail are paramount.

4. **Cost Efficiency**: The pricing structure is designed to be competitive, with costs set at $7 per million tokens, and promotional pricing for prompts up to 128K tokens at $3.50. This makes it accessible for developers looking to integrate advanced AI capabilities into their applications.

5. **Developer Features**: New features such as video frame extraction, parallel function calling, and context caching are introduced, which streamline the development process and improve the efficiency of using the model.

6. **Integration with Google Cloud**: The model is integrated with Google’s infrastructure, allowing developers to scale their applications easily and access enterprise-grade features as their needs grow.

Overall, the Gemini 1.5 Pro model represents a significant advancement in AI capabilities, providing developers with powerful tools to create innovative applications that leverage multimodal AI and extensive context processing."
How many developers are currently using Gemini models across Google's tools?,"Welcome to the 16th Google I.O. I want to thank our fans, have developed our community all millions of you around the world for choosing to build with Google. Literally, whatever you can imagine, you can bring it to life because only Google's ecosystem gives you the potential to reach people on 3 billion Android devices and 2 billion Chrome and Chromium-based browsers. You've created millions of helpful apps with Firebase, Google Cloud, and generative AI models like Gemini and Gemma. At Google, we're on a mission to make generative AI accessible to every developer on the planet. It's transforming the very fundamentals of software development, including new techniques for how you build and new inspiration for what you build. AI helps you to be more productive by assisting you with many different development tasks, such as writing, debugging, and testing your code, or generating documentation, and understanding your code base. That's why Gemini is available to you, all developers to use in Android Studio, Chrome Dev Tools, Project IDX, CoLab, Beath Code, IntelliJ, and Firebase. CHEERING AND APPLAUSE Gemini is even more helpful with context such as app settings, performance data, your logs, and source code. This is just a glimpse of AI changing how we built. Now, let's go to what you built. Today, people just assumed that things will work great. Of course, any platform, any device that they use, literally anywhere they are, whether on the go, at home, or at work, in their local town, or on the other side of the world. At the same time, the tools that you use need to make that possible and they're constantly evolving. It's great to see the latest technologies come out, but that means more complexity, especially with so many different AI concepts to learn. We want to make your lives simpler. As code becomes content and coders become creators, whether you're building an AI-powered mobile app, a web app, or an interactive experience that works across all platforms, we're providing you with powerful, more integrated development tools, both from Google and the OpenEka system, and it comes with secure, easy access to Google's most capable AI models and global cloud infrastructure. Ultimately, it's about helping you to stay focused on what you want to build, amazing products and services, and not the plumbing, which takes me to our agenda for today. Up next, you'll hear more in how you can use the Gemini API in Google AI Studio to develop engaging and multimodal apps. Then we will take you through how Gemini helps you to create a whole new category of AI-powered experiences on Android and Web, with exceptional levels of productivity. We've also been hard at work creating the very best experienced, full-stack, multi-platform development. Our team will share what's now possible with Project IDX, Flutter, and Firebase. And we think that will dramatically expand what you can build. Then we will talk about how you can build and customize your own AI models with Gemma. You'll see how CoLab, Carrison Jacks, make it really easy. Finally, I'll be back up here to share one more thing before we wrap it up. Are you ready to jump in? OK. Awesome. Jacqueline, come join us. Our investment in our AI research and infrastructure allows us to put the power of AI directly into your hands. We make it easy to start building an AI app with a simple API and a tool that can be used to create a new product. It can be used to create an AI app with a simple API integration. So you can focus on building the best possible products for your users. Now, meeting many of you at Hackathons and Meetups, I am blown away by how you're pushing the boundaries with our new AI models. From new ideas using audio and video inputs, the creative use of longer context windows. It's really inspiring to see. And like you heard about earlier today, balance across quality, cost, and speed. That's why I'm thrilled that Gemini 1.5 Flash is officially open to all developers to start building with today. You can get started using the Gemini API in Google AI Studio, which is now available in more than 200 countries and territories. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. But what I'm seeing more and more these days is that it isn't just about selecting the right model, but also the right techniques to bring your ideas to life.
Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in monks than is happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. So, we are in the very early days of the AI platform shift. We see so much opportunity ahead for creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too, including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries, as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. Applause There's so much innovation happening in search. Thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.",More than 1.5 million developers are currently using Gemini models across Google's tools.
What are the new features introduced in Google Photos with the integration of Gemini?,"One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. The so much innovation happening in search, thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago. Since then, people have used it to organize their most important memories. Today, that amounts to more than 6 billion photos and videos uploaded every single day. And people are using photos to search across their life. With Gemini, you're making that a whole lot easier. Say you're at a parking station ready to pay, but you can't recall your license plate number. Before you could search photos for keywords, then scroll through years worth of photos looking for the right one. Now, you can simply ask photos. It knows the cards that appear often, it triangulates which one is yours, and just tells you the license plate number. And ask photos can also help you search your memories in a deeper way. For example, you might be reminiscing about your daughter Lucius early milestones. You can ask photos when did Lucius learn to swim. You can even fall up with something more complex. Show me how Lucius swimming has progressed. Here, Gemini goes beyond a simple search, recognizing different contexts from doing laps in the pool, to snorkeling in the ocean, to the texture and dates on her swimming certificates, and photos packages it up all together in a summary. You can really take it all in and relive amazing memories all over again. We are rolling out asked photos this summer with more capabilities to come. APPLAUSE Unlocking knowledge across formats is why we build Gemini to be multi-model from the ground up. It's one model with all the modalities built in. So not only does it understand each type of input, it finds connections between them. Multi-modality radically expands the questions we can ask, and the answers we will get back. Long context takes this a step further, enabling us to bring in even more information, hundreds of pages of text, hars of audio, a full hour of video, or entire code repos. Or if you want, roughly 96 cheesecake factory menus. For that many menus, you need a one-million token context window, now possible with Gemini 1.5 Pro. Developers have been using it in super interesting ways. Let's take a look. I remember the announcement, the one-million token context window, and my first reaction was, there's no way they were able to achieve this. I wanted to test this technical skills. So I uploaded a line chart. It was temperatures between Tokyo and Berlin, and how they were across the 12 months of the year. So I got in there, and I threw in the Python library that I was really struggling with. And I just asked it a simple question. And it nailed it. It could find specific references, the comments in the code, and specific requests that people had made, and other issues that people had had, but then suggest a fix for it, that related to what I was working on. I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model.
Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Oops! Reacts. Things got bigger. Like way bigger. And it wasn't all just for him, or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. Still, we are in the very early days of the AI platform shift. We see so much opportunity ahead. For creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too. Including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. The so much innovation happening in search, thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.","The new features introduced in Google Photos with the integration of Gemini include:

1. **Enhanced Search Capabilities**: Users can now search for specific items in their photos by simply asking the app. For example, if a user cannot recall their license plate number, they can ask Google Photos, which will recognize frequently appearing cars and provide the correct license plate number.

2. **Contextual Memory Search**: Users can ask more complex questions about their memories, such as ""When did Lucius learn to swim?"" or ""Show me how Lucius swimming has progressed."" Gemini can recognize different contexts and compile relevant photos and information, summarizing them for the user.

3. **Multi-Modal Understanding**: Gemini's multi-modal capabilities allow it to understand various types of input (text, images, etc.) and find connections between them, enabling richer and more powerful search experiences.

These features aim to make it easier for users to interact with their photo collections and retrieve memories in a more intuitive and efficient manner."
What is the purpose of the Gemini Advanced subscription service announced at the event?,"Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. Over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now Gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now based on my flight information, Gemini knows that I needed two and a half day, itinerary. You can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens.
Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens. That is the longest context window of any chatbot in the world. It unlocks incredible new potential in AI, so you can tackle complex problems that were previously unimaginable. You can upload a PDF up to 1,500 pages long or multiple files to get insights across a project. And soon you can upload as much as 30,000 lines of code or even an hour long video. Gemini Advanced is the only chatbot that lets you process this amount of information. Now just imagine how useful this will be for students. Let's say you've spent months on your thesis and you could really use a fresh perspective. You can upload your entire thesis, your sources, your notes, your research, and soon interview audio recordings and videos too. So Gemini has all of this context to give you actionable advice. It can dissect your main points, identify improvements, and even roleplay as your professor. So you can feel confident in your work. And check out what Gemini Advanced can do with your spreadsheets, with the new data analysis feature launching in the coming weeks. Maybe you have a side hustle selling handcrafted products. But you're a better artist than accountant, and it's really hard to understand which products are worth your time. Simply upload all of your spreadsheets and ask Gemini to visualize your earnings and help you understand your profit. Gemini goes to work, calculating your returns, and pulling its analysis together into a single chart so you can easily understand which products are really paying off. Now behind the scenes, Gemini writes custom Python code to crunch these numbers. And of course your files are not used to train our models. Oh, and just one more thing. Later this year, we'll be doubling the long context window to two million tokens. We absolutely can't wait for you to try all of this for yourself. Gemini is continuing to evolve and improve at a breakthrough pace. We're making Gemini more multimodal, more agentive, and more intelligent, with the capacity to process the most information of any chatbot in the world. And as you heard earlier, we're also expanding Gemini Advanced to over 35 supported languages available today. But of course, what makes Gemini so compelling is how easy it is to do just about anything you want with a simple prompt. Let's take a look. Enter a prompt here. Okay, it can be that hard. How about generate an image of a cat playing guitar? Is that how it works? Am I doing AI? Yeah, just does whatever you type. What a last minute gift ideas you can make with arts and crafts. Plan a workout routine to get bigger calves. Help me think of titles my tell all men more. What's something smart I can say about Renoir? Generate another image of a cat playing guitar. If a girl calls me a snack, how do I reply? Yeah, that's how it works. You're doing AI. Make this email somewhere professional before it hits. What's a good excuse to cancel dinner with my friends? They're literally sitting right here. You know I can do other stuff, right?","The purpose of the Gemini Advanced subscription service announced at the event is to provide users with access to the latest AI innovations from Google, including enhanced capabilities for personal AI assistance. Gemini Advanced allows subscribers to utilize advanced features such as a longer context window for processing information, the ability to upload large documents (up to 1,500 pages or 30,000 lines of code), and tools for data analysis. This service aims to help users tackle complex problems, gain insights across projects, and receive actionable advice on various tasks, making it particularly useful for students and professionals alike. Additionally, it expands the functionality of the Gemini app by enabling more multimodal interactions and supporting over 35 languages."
What advancements in AI technology were highlighted in relation to Google Search during the event?,"Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago we created search to help people make sense of the waves of information moving online. With each platform shift we have delivered breakthroughs to help answer your questions better. On mobile we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities are high bar for information quality, our decades of experience connecting you to the richness of the web. The result is a product that does the work for you. Google search is generative AI at the scale of human curiosity. And it's our most exciting chapter of search yet. To tell you more here's Liz. Thanks Sundar. With each of these platform shifts we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI search will do more for you than you ever imagined. So whatever is on your mind and whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real time information with over a trillion facts about people, places and things. Second, our in parallel ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly. Complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub questions and get an AI overview in seconds. To make this possible, we're introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates studios in Boston and show you details on their inter offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there.
To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly, complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today, with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. You make this possible for introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates to use in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you've got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there. So since you're asking about places, it taps into Google's index of information about the real world with over 250 million places and updated in real time, including their ratings, reviews, business hours, and more. Research that might have taken you minutes or even hours Google can now do on your behalf in just seconds. Next, let me show you another way multi-step reasoning in Google search can make your life that much easier. Take planning, for example. Planning of trips and meal plans can be fun. But doing the work of actually figuring it all out, no thank you. With Gemini in search, Google does the planning with you. Planning is really hard for AI to get right. It's the type of problem that takes advance reasoning and logic. After all, if you're meal planning, you probably don't want mac and cheese for breakfast, lunch, and dinner. Okay, my tics might, but say you're looking for a bit more variety. Now, you can ask search to create a three-day meal plan for a group that's easy to prepare. And here you get a plan with a wide range of recipes from across the web. This one for overnight oats looks particularly interesting. And you can easily head over to the website to learn how to prepare them. If you want to get more veggies in, you can simply ask search to swap in a vegetarian dish. And just like that, search customizes your meal plan.","The advancements in AI technology highlighted in relation to Google Search during the event include:

1. **Generative AI Search**: Google Search is now enhanced with generative AI capabilities, allowing it to do more for users than ever before. This includes providing instant answers to queries with a range of perspectives and links for deeper exploration.

2. **AI Overviews**: A new feature called AI overviews was introduced, which simplifies the process of gathering information by providing comprehensive answers to complex questions. This feature will roll out to users in the US and expand globally, aiming to reach over a billion people by the end of the year.

3. **Multi-Step Reasoning**: Google Search will utilize multi-step reasoning to break down complex questions into manageable parts, enabling it to conduct research on behalf of the user. This allows users to ask detailed questions and receive organized, relevant information quickly.

4. **Real-Time Information Access**: The integration of real-time information with over a trillion facts about people, places, and things enhances the search experience, ensuring users receive the most accurate and up-to-date information.

5. **Custom Gemini Model**: The new Gemini model acts as an AI agent, leveraging advanced reasoning and logic to provide tailored responses, whether for finding local services or planning meals.

These advancements collectively aim to make Google Search more intuitive and efficient, significantly improving the user experience."
What was the total number of developers in attendance at Google I/O 2024?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The total number of developers in attendance at Google I/O 2024 was about 4,000."
What are some of the key features introduced in the Gemini models during the keynote?,"And this is not just the model completing the code. It's the model coming up with an entire plan and running it. And not only do I get an answer to my initial question about the data set, but the AI agent also produces a fully functional collab notebook based on your instructions. And you can share it with others, allowing for real time collaboration on code, new visualizations, and all of the results. It's available today at labs.google slash code and let us know what you think about it. Now you see why I'm so excited today. You see, we're committed to empowering you to build with genitive AI boldly and responsibly from Gemini power assistance in your favorite IDEs to easy access for our most powerful Gemini model. And it comes with flexibility to use Gemma open models. You have an opportunity to reach anyone anywhere through Google's ecosystem of devices, app experiences, and platform. We continued investing resources to give your projects a boost. Google accelerators and the Google developer portfolio is just a couple of great examples. Our accelerators have guided over 1,300 startups in 84 countries with countless success stories. And that includes 20 startups that have grown to become unicorns. You Genie AI founded by Dr. Sudeth Boyd, it's a recent graduate of our climate change accelerator. You Genie was born out of Dr. Childry's desire to use AI for good. His team helps manufacturing companies to dramatically reduce their carbon emissions. And you Genie's algorithms apply Google DeepMind research which has helped to optimize energy use in Google's own data centers. You Genie was able to streamline their model development and their analytics with accurate emissions data through Google Earth Engine. We love working alongside startups to tackle big problems and invite all of you to apply to an upcoming accelerator soon. If you have one of the 15 million Google developer profiles, you enjoy invitations for early access programs, a chance to participate in research and opportunities to attend special events. Beginning today, your profile is evolving into something even better. Introducing the Google Developer Program. Members of the program can now access new benefits at no cost and I'm going to take you through them. First, you can access Gemini for learning and searching and chatting with documentation. Second, for IDX users, we're now enabling you to go from two work stations up to five. Third, if you're in the Google Cloud Innovators community, you get credits for interactive labs on Google Cloud Skills Boost. Sign up for the Google Developer Program today or if you have an existing profile, you're all set. More benefits are coming soon so be on the lookout. That brings us to the close of today's developer keynote but we're not quite done. There's a lot more to Google I.O than just the keynotes. If you're with us in person, attend the sessions, the workshops, and demos both today and tomorrow. If you're online, you'll find all the sessions available on IO.Google starting this Thursday morning, Mountain View Time. Last year, we kicked off the series of IO connect events to bring the in-person experience from Shoreline to more places that are closer to you. Well, we heard just how much you loved them so we've got three of them lined up this year. Starting in Berlin on June 27th and then we're in Bangalore in July and Beijing in August. And I'll mention, I'll be at all of them so I hope to see you there too. If you can join us at IO connect, come to one of the many community-led IO extended event happening in your local area. Now, I like to share something really cool with you. We had Project Astra watch the keynote this morning. Yes. Let's see what it says. Hey, my name is Michael Chang and we're watching the Google IO keynote while it's happening with Project Astra. Let's take a look. Hey, Gemini, currently watching Google IO keynote right now. You want to watch it with me? Sure, I'd love to watch it with you. What's happening in the keynote? What does it look like? What does it look like they're announcing right now? It looks like they're announcing Gemini 1.5 Flash. The speaker is talking about its speed and efficiency in comparison to the pro model. What are the other features? It looks like Gemini 1.5 Flash is optimized for tasks where low latency and high efficiency matter most. Cool. The screen also shows. Oh, yeah, sorry, I interrupted you. Keep going.
We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.","During the keynote, several key features of the Gemini models were introduced:

1. **Enhanced AI Assistance**: Gemini is designed to empower users to build with generative AI, providing assistance in various Integrated Development Environments (IDEs) and easy access to powerful AI models.

2. **Collaboration Tools**: The AI agent can produce fully functional collaborative notebooks based on user instructions, facilitating real-time collaboration on code and visualizations.

3. **Gemini App**: The app is described as a personal AI assistant that allows users to interact using text, voice, or camera, making it natively multimodal.

4. **Live Interaction**: A new feature called ""live"" allows users to have in-depth conversations with Gemini using their voice, with the ability to interrupt and adapt to speech patterns.

5. **Customization with Gems**: Users can create personalized ""gems"" that act as experts on specific topics, allowing for tailored interactions with the AI.

6. **Trip Planning Experience**: Gemini can assist in planning trips by gathering information from various sources (like Gmail and maps) to create personalized vacation plans, considering user preferences and constraints.

7. **Dynamic User Interface**: The trip planning feature includes a dynamic UI that adjusts itineraries based on user input, such as changing start times for activities.

These features highlight Gemini's focus on enhancing productivity, creativity, and personalized user experiences."
What unique feature does the music FX DJ tool provide according to Mark Rubier?,"Sabak Give me the hell out of here. Get me the hell out of this cup. Get me out. Yeah! Google! Google! Google! Google! Google! Ha ha ha! We all ready to do a little googling. Everyone get out your computers. It's time to have ourselves a nice Google. Siri Navigate to google.com. Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Are you awake? Have you had your coffee? Are you awake? Don't worry baby. Google's gonna wake you right up. Don't worry baby. Google! Google! Google! Google's gonna wake you right up. Bleep bloop, bloop, bloop, rappy bloop, bloop, bloop. Up time to get up. You silly little nerd's wake up. okay Google.com. What is it? What does it mean? Why are we here? No one knows. And you're not, you're not going to find out. Not today. Those answers? Not part of the presentation. Buckle up. I'm Mark Rabier. It's nice to meet you, nice to see you, nice to be here. In beautiful Honolulu Hawaii. Here for Google I.O. If you have no idea who I am, that's perfectly I would expect that. Not many people do, but I'm an improvisational musician. I make stuff up from scratch. And that's exactly what we're going to be doing this entire show. We have nothing planned. That includes the rest of the conference. None of this has been rehearsed. Sundar's going to come out, say some stuff. No, we don't know what's going to happen here today. They're making up these visuals right now. Someone's back there. So what I'm here to talk about is music. I'm here to talk to you about music. And in particular, a very exciting new technology that we've been developing here at Google.com. Music FX. DJ. That's right. No more do you need to haul around the case of records. No, the computer will take care of it. Music FX DJ is a generative artificial intelligence tool that will from scratch generate from nothing. A series of sounds as it interprets your prompts. I'll demonstrate that to you now. Utilizing technology at my fingertips. And utilizing the colossal power of Google's machine. Connecting to Google's machine now. Music. We'll start with a viola. How about that? Everyone loves a viola. And if you don't, I don't care. I don't care. You do now. Viola. No one wrote this. Google wrote this. Let's throw in something weird. Hip hop beat from the 808 drum machine. It's pretty tough actually. So we have the sort of melodic element of the viola still in there. We can pull that out. Chiptones. I think that's chiptones. Let's do it out. It's pretty tough. Pulling that out. The machine is in control now. And that's what it can do. And let me tell you something. Let me tell you something right now. That is just the beginning. You're going to be hearing that a lot at this keynote. That's just the beginning. Okay. Now you will choose the prompts and we'll leave it up to chance. As I said, everything here unscripted unplanned. I didn't know this was going to happen. What's this? I didn't even know you guys were going to be here. What are you guys doing here? Six props, bagpipes, Persian tar, jembe, dulcimer, flamenco guitar, Detroit techno. You know what those things are? You know what this is? You better. Shout them out. What do you want to hear? Flamenco guitar. Persian took bagpipes. No, no bagpipes. They said, no, get the bagpipes out of here. Okay. Persian tar, jembe, flamenco guitar. Jembe.
While we've made some great strides in developing AI systems that can understand multi-modal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called? That is the tweeter. It produces high frequency sounds. Give me a creative alliteration about these. Creative crayons color cheerfully. They certainly craft colorful creations. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector IV. That's right. What neighborhood do you think I'm in? This appears to be the King's Cross area of London. It is known for its railway station and transportation connections. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk and you're a red apple. What can I add here to make this system faster? Add in a cache between the server and database could improve speed. What does this remind you of? Trudginger's cat. All right, give me a band name for this duo. Golden Stripes. Nice. Thanks, Dominic. I think you'll agree it's amazing to see how far AI has come, especially when it comes to spatial understanding, video processing and memory. It's easy to envisage a future where you can have an expert assistant by your side through your phone or new exciting form factors like glasses. Some of these agent capabilities will come to Google products like Gemini app later this year. For those of you on site today, you can try out a live demo version of this experience in the AI sandbox area. Next, let's take a look at how our innovations are helping people bring new creative ideas to life. Today, we're introducing a series of updates across our genitive media tools with new models covering image, music and video. Over the past year, we've been enhancing quality, improving safety and increasing access to help tell this story. Here's Doug. Thanks, Demis. Over the past few months, we've been working hard to build a new image generation model from the ground up with stronger evaluations, extensive red teaming and state-of-the-art watermarking with synth ID. Today, I'm so excited to introduce Imagine 3. It's our most capable image generation model yet. Imagine 3 is more photorealistic. You can literally count the whiskers on its snout with richer details like this incredible sunlight in the shot and fewer visual artifacts or distorted images. It understands prompts written the way people write, the more creative and detailed you are the better and Imagine 3 remembers to incorporate small details like the wild flowers or small blue bird in this longer prompt. Plus, this is our best model yet for rendering text, which has been a challenge for image generation models. Inside-by-side comparisons, independent evaluators preferred Imagine 3 over other popular image generation models. In some, Imagine 3 is our highest quality image generation model so far. You can sign up today to try Imagine 3 in image FX, part of our suite of AI tools at labs.google, and it'll be coming soon to developers and enterprise customers in Vertex AI. Another area full of creative possibility is generative music. I've been working in this space for over 20 years, and this is by far the most exciting year of my career. We're exploring ways of working with artists to expand their creativity with AI. Together with YouTube, we've been building music AI sandbox, a suite of professional music AI tools that can create new instrumental sections from scratch, transfer styles between tracks, and more. To help us design and test them, we've been working closely with incredible musicians, songwriters, and producers. Some of them made even entirely new songs in ways that would have not been possible without these tools.","The Music FX DJ tool provides a unique feature of generative artificial intelligence that can create a series of sounds from scratch based on user prompts. This means that users no longer need to carry physical records, as the computer will generate the music dynamically in response to the prompts given."
What significant updates were announced for Android Studio during the developer keynote?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","During the developer keynote, significant updates were announced for Android Studio, which included enhancements that allow developers to leverage Gemini 1.5 Pro to improve and accelerate their development. One particularly exciting feature was the ability for developers to take a mock-up of a screen and generate the code for it in Android Studio, as well as generate an interactive app in real-time. This capability aimed to energize the audience and enhance the development experience."
What was the purpose of the Googlers-only demo slam at Google I/O 2024?,"I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!
Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.","The purpose of the Googlers-only demo slam at Google I/O 2024 was to allow Googlers to present live demos across various labs, including Gemini, Search, Android, and more. Each demo had a time limit of four minutes before a humorous ""dinosaur"" would signal them to leave the stage. This event aimed to showcase the innovations and projects developed within Google, providing an engaging platform for Googlers to demonstrate their work to the audience."
What are the key AI innovations mentioned in the presentation?,"This is the end of the video. Thank you. Listening to everything that has been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context, and agents are the cutting edge of what this technology can do, take it to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners, and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting-edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines, and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine three NVO create more realistic imagery and videos. We must also consider how they might be misused to spread misinformation. To help last year, we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today, we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Gemini, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making. And in the coming months, we'll be open sourcing Synth ID text watermarking. This will be available in our updated responsible Gerritive AI Toolkit, which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supposed to appear collaborating with Adobe, Microsoft, Startups, and many others to build that implement standards that improve the transparency of digital media. Now, let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today, our AI advances are helping to solve real-world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases, helping predict floods in more than 80 countries, and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube.
We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.","The key AI innovations mentioned in the presentation include:

1. **Multiple-Dality**: This refers to the ability of AI to process and understand information from multiple modalities (e.g., text, images, video) simultaneously.

2. **Long-Context**: This innovation allows AI models to maintain and utilize longer contextual information, enhancing their understanding and response capabilities.

3. **AI-Assisted Red Teaming**: A technique that involves training AI agents to compete against each other to identify weaknesses in AI models, improving their robustness and safety.

4. **Synth ID**: A tool that adds imperceptible watermarks to AI-generated images and audio, which is being expanded to text and video to help identify and prevent misuse.

5. **LearnLam**: A new family of models based on Gemini, fine-tuned for educational purposes, aimed at making learning experiences more personal and engaging.

6. **Generative Video Model**: A model developed by Google DeepMind that converts input text into output video, enhancing creative control and storytelling capabilities.

These innovations are part of Google's commitment to building AI responsibly while maximizing its benefits for people and society."
What is the purpose of the AI-assisted red teaming technique?,"This is the end of the video. Thank you. Listening to everything that has been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context, and agents are the cutting edge of what this technology can do, take it to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners, and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting-edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines, and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine three NVO create more realistic imagery and videos. We must also consider how they might be misused to spread misinformation. To help last year, we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today, we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Gemini, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making. And in the coming months, we'll be open sourcing Synth ID text watermarking. This will be available in our updated responsible Gerritive AI Toolkit, which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supposed to appear collaborating with Adobe, Microsoft, Startups, and many others to build that implement standards that improve the transparency of digital media. Now, let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today, our AI advances are helping to solve real-world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases, helping predict floods in more than 80 countries, and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube.
But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and contacts to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives. Let me bring Tony up to show you what I mean. Hey, Tony. Hey, partner. Hey, everyone. Okay. So let me start by showing you how we set up this virtual teammate. As you can see, the teammate has this very own account. And we can go ahead and give it a name. We'll do something fun like Chip. Chip's been given a specific job role at the set of descriptions on how to be helpful for the team. You can see that here. And some of the jobs are to monitor and track projects. We've listed a few out to organize information and provide context and a few more things. Now that we've configured our virtual teammate, let's go ahead and see Chip in action. To do that, I'll switch this over here to Google Chat. First, when planning for an event like I.O., we have a ton of chat rooms for various purposes. Luckily for me, Chip is in all of them. To quickly catch up, I might ask a question like, anyone know if our I.O. storyboards are approved? Because we've instructed Chip to track this project, Chip searches across all the conversations and knows respond with an answer. There it is. Simple but very helpful. Now, as the team adds Chip to more group chats, more files, more email threads, Chip builds a collective memory of our work together. Let's look at an example to show you all switch over to a different room. How about a project sapphire over here? And here we are discussing a product release coming up. And as usual, many pieces are still in flight. So I can go ahead and ask, are we on track for launch? Chip gets to work not only searching through everything it has access to, but also synthesizing what's found and coming back with an up-to-date response. There it is. A clear timeline, a nice summary, and notice even in this first message here, Chip flags a potential issue the team should be aware of. Because we're in a group space, everyone can follow along, anyone can jump in at any time, as you see someone just did, asking Chip to help create a doc to help address the issue. A task like this could take me hours, dozens of hours, Chip can get all done in just a few minutes, sending the doc over right when it's ready. So much of this practical helpfulness comes from how we've customized Chip to our team's needs, and how seamlessly this AI is integrated directly into where we're already working back to you, Aparna. Thank you, Tony. Now, I can imagine a number of virtual types of, number of different types of virtual teammates configured by businesses to help them do what they need. Now we have a lot of work to do to figure out how to bring these agent of experiences like virtual teammates into workspace, including enabling third parties to make their very own versions of Chip. We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience.","The purpose of the AI-assisted red teaming technique is to improve AI models by testing them to identify weaknesses and vulnerabilities. This technique involves training AI agents to compete against each other, which enhances the scope of red teaming capabilities. It aims to address adversarial prompting and limit problematic outputs, thereby making the models more accurate, reliable, and safer. This approach combines human insight with safety testing methods to better navigate the risks associated with AI technologies."
"How many scientists are using AlphaFold, and in how many countries?","Just last week, we announced our next generation AlphaFull model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. And all of this was made possible with the best infrastructure for the AI era, including our highly optimized, tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multimodal because that's how we interact with and understand the world around us. We built a variety of models for different use cases. You've seen how powerful Gemini 1.5 Pro is. But we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash. Flash is a lightweight model compared to Pro. It's designed to be fast and cost efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context. Flash is optimized for tasks where low latency and efficiency matter most. Starting today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI. And developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. And you'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models, but we're always striving to push the state of the art even further. At any one time, we have many different models in training, and we use our very large and powerful ones to help teach and train our production ready models. Together with user feedback, this cutting edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multimodal AI. And this would how this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multimodal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action. And it would have to be proactive, teachable, and personal, so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called? That is the tweeter. It produces high frequency sounds. Give me a creative alliteration about these. Creative crayons color cheerfully. They certainly craft colorful creations. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector, IV. That's right. What neighborhood do you think I'm in? This appears to be the King's Cross area of London. It is known for its railway station and transportation connections. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. What can I add here to make this system faster? Adding a cache between the server and database could improve speed. What does this remind you of? I think you'll agree. It's amazing to see how far AI has come.
Just last week, we announced our next generation AlphaFull model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. And all of this was made possible with the best infrastructure for the AI era, including our highly optimized, tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multimodal because that's how we interact with and understand the world around us. We built a variety of models for different use cases. You've seen how powerful Gemini 1.5 Pro is. But we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash. Flash is a lightweight model compared to Pro. It's designed to be fast and cost efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context. Flash is optimized for tasks where low latency and efficiency matter most. Starting today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI. And developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. And you'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models, but we're always striving to push the state of the art even further. At any one time, we have many different models in training, and we use our very large and powerful ones to help teach and train our production ready models. Together with user feedback, this cutting edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multimodal AI. And this would how this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multimodal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action. And it would have to be proactive, teachable, and personal, so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called? That is the tweeter. It produces high frequency sounds. Give me a creative alliteration about these. Creative crayons color cheerfully. They certainly craft colorful creations. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector, IV. That's right. What neighborhood do you think I'm in? This appears to be the King's Cross area of London. It is known for its railway station and transportation connections. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. What can I add here to make this system faster? Adding a cache between the server and database could improve speed. What does this remind you of? I think you'll agree. It's amazing to see how far AI has come.","The provided context does not contain specific information about the number of scientists using AlphaFold or the number of countries involved. Therefore, I cannot answer that question based on the given text."
"What is LearnLam, and how is it intended to be used?","This is particularly important as technical advances like better intonation, make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine 3NVO create more realistic imagery and videos. We must also consider how there might be misuse to spread misinformation. To help last year we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Jemeni, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making, and in the coming months we'll be open sourcing Synth ID text-watermarking. This will be available in our updated responsible, generative AI toolkit which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supporting C2PA collaborating with Adobe, Microsoft, startups, and many others to build that implement standards that improve the transparency of digital media. Now let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today our AI advances are helping to solve real-world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases. Helping predict floods in more than 80 countries and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now generative AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day and generative AI is opening up new possibilities allowing us to ask questions like, what if everyone everywhere could have their own personal AI tutor on any topic or what if every educator could have the role assistant in the classroom. Today marks a new chapter for learning and education at Google. I'm excited to introduce LearnLam. Our new family of models based on Gemini and fine tuned for learning. LearnLam is grounded in educational research making learning experiences more personal and engaging. And it's coming to the products you use every day like search, Android, Gemini and YouTube. In fact, you've already seen LearnLam on stage today when it helps to me with his son's homework on Android. Now let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems which will be available in the Gemini app and web experience including one called Learning Coach. With Learning Coach you can get step-by-step study guidance along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features like Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering with experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use generative AI. We're also working directly with educators to build more helpful generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning. And enable teachers to tailor lessons and content to meet the individual needs of their students.
And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube. In fact, you've already seen LearnLam on stage today, when it helps to meer with his son's homework on Android. Now, let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems, which will be available in the Gemini app and web experience, including one called Learning Coach. With Learning Coach, you can get step-by-step study guidance, along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features that Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy, to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use Generative AI. We're also working directly with educators to build more helpful Generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning, and enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real, how much it is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors, and how much more is still to come. This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar.","LearnLam is a new family of models introduced by Google, based on their Gemini technology and fine-tuned specifically for educational purposes. It is designed to make learning experiences more personal and engaging, integrating educational research into its functionality. LearnLam will be incorporated into various Google products, such as search, Android, Gemini, and YouTube.

The intended use of LearnLam includes:

1. **Personal AI Tutoring**: It aims to provide users with their own personal AI tutor on any topic, enhancing the learning experience.
2. **Learning Coach**: One of the features, called Learning Coach, offers step-by-step study guidance and helpful memory techniques to aid understanding rather than just providing answers.
3. **Interactive Educational Videos**: In YouTube, LearnLam will make educational videos more interactive, allowing users to ask questions, receive explanations, and take quizzes.
4. **Support for Educators**: LearnLam is being developed in collaboration with educational institutions to create tools that assist teachers in lesson planning and tailoring content to meet individual student needs.

Overall, LearnLam is part of Google's effort to leverage generative AI to improve accessibility and usefulness of information for learning, aiming to transform education and learning experiences."
What collaborations are mentioned in relation to the development of Generative AI tools for education?,"What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.
This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering with experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use generative AI. We're also working directly with educators to build more helpful generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning. And enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real. How much is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors. And how much more is still to come? This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar. Thanks, James. All of this shows the important progress we've made as we take a bold and responsible approach to making AI helpful for everyone. Before we wrap, I have a feeling that someone out there might be counting how many times we have mentioned AI today. And since a big team today has been letting Google do the work for you, we went ahead and counted so that you don't have to. That might be a record in how many times someone has said AI. I'm tempted to say it a few more times, but I won't. Anyhow, this tally is more than just a punchline. It reflects something much deeper. We've been AI first in our approach for a long time. Our decades of research leadership have pioneered many of the modern breakthroughs that power AI progress for us and for the industry. On top of that, we have world-leading infrastructure built for the AI era. Cutting edge innovation in search now powered by Gemini, products that help with extraordinary scale, including 15 products with over half a billion users, and platforms that enable everyone, partners, customers, creators, and all of you to invent the future. This progress is only possible because of our incredible developer community. You're making it real through the experiences you build every day. So to everyone here in Showline and the millions more watching around the world, here's to the possibilities ahead and creating them together. Thank you. Hello, me to re-educe myself. My name is... What does this remind you of? Soap in your head. Wow! When all of these tools come together, it's a powerful combination. It's amazing. It's amazing. It's an entire suite of different kinds of possibilities. Hi, I'm Gemini. What neighborhood do you think I'm in? This appears to be the campus cross-area of London. Together we're creating a new era. Ah...","The collaborations mentioned in relation to the development of Generative AI tools for education include partnerships with experts and institutions such as Columbia Teachers College, Arizona State University, and Khan Academy. Additionally, there is a collaboration with MIT Rays to develop an online course aimed at helping educators better understand and use generative AI."
"What are the two models introduced in the Gemini 1.5 series, and what are their primary use cases?","Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response. And it's OK if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to aisstudio.google.com, log in with your Google account, and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit run. Now, in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, it starts streaming it back. Now, this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from Notebook LM, like we showed earlier. Now, what's great about this is that you can take something like this in AI Studio, prototype tier in 10 seconds. And with one click in the upper left, get an API key, or over here in the upper right, just tap get code. And you've got all of the model configurations, the safety settings, ready to go, straight into your IDE. Now, over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way, you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. OK. OK. APPLAUSE Now, let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now, since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. APPLAUSE It's optimized for a range of image captioning, visual Q&A, and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model, but it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on NextGen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. APPLAUSE So wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look.","The two models introduced in the Gemini 1.5 series are **1.5 Pro** and **1.5 Flash**. 

- **1.5 Pro** is used for complex tasks where the highest quality response is desired, even if it takes a bit longer to receive the output.
- **1.5 Flash** is designed for quick tasks where speed is the most important factor. 

Developers can try both models to determine which one works best for their specific use cases."
What is the pricing structure for the Gemini 1.5 Pro and Flash models based on token usage?,"Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response. And it's OK if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to aisstudio.google.com, log in with your Google account, and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit run. Now, in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, it starts streaming it back. Now, this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from Notebook LM, like we showed earlier. Now, what's great about this is that you can take something like this in AI Studio, prototype tier in 10 seconds. And with one click in the upper left, get an API key, or over here in the upper right, just tap get code. And you've got all of the model configurations, the safety settings, ready to go, straight into your IDE. Now, over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way, you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. OK. OK. APPLAUSE Now, let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now, since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. APPLAUSE It's optimized for a range of image captioning, visual Q&A, and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model, but it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on NextGen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. APPLAUSE So wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look.","The pricing structure for the Gemini 1.5 Pro and Flash models based on token usage is as follows:

- **Gemini 1.5 Pro**: 
  - $7 per 1 million tokens.
  - For prompts up to 128K tokens, the price is reduced to $3.50 (50% less).

- **Gemini 1.5 Flash**: 
  - Starts at 35 cents per 1 million tokens."
"What new developer features are being added to the Gemini API, and when will they be available?","We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. Over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now Gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now based on my flight information, Gemini knows that I needed two and a half day, itinerary. You can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens.","The new developer features being added to the Gemini API include the ability to create ""gems,"" which are personal experts that users can customize for their specific needs. These gems can be set up easily by writing instructions once and can assist with various tasks, such as acting as a personal writing coach or a tutor. Additionally, the Gemini app will feature enhanced capabilities for trip planning, allowing it to gather information and create personalized vacation plans based on user preferences.

These features are expected to roll out in the coming months, with the trip planning experience specifically mentioned to be available in Gemini Advanced this summer, just in time for Labor Day weekend."
"What is the significance of the Gemma model family, and what are its available sizes?","Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now since it launched less than three months ago it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, Polly Gemma, our first vision language open model and it's available right now. It's optimized for a range of image captioning visual Q&A and other image labeling tasks so go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model but it's still got a fitness size that's easy for all of you to use. So in a few weeks we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on next-gen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. To wrap up I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. Languages are a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navarasa. Navarasa is a model that's trained for Indic languages. It's a fine tuned model based on Google's Gemma. We built Navarasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. Today the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of generative AI that we want to bring to every corner of India and the entire world.
Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response. And it's OK if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to aisstudio.google.com, log in with your Google account, and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit run. Now, in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, it starts streaming it back. Now, this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from Notebook LM, like we showed earlier. Now, what's great about this is that you can take something like this in AI Studio, prototype tier in 10 seconds. And with one click in the upper left, get an API key, or over here in the upper right, just tap get code. And you've got all of the model configurations, the safety settings, ready to go, straight into your IDE. Now, over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way, you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. OK. OK. APPLAUSE Now, let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now, since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. APPLAUSE It's optimized for a range of image captioning, visual Q&A, and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model, but it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on NextGen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. APPLAUSE So wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look.","The Gemma model family is significant for driving AI innovation and responsibility. It is built from the same research and technology as the Gemini models and offers top performance. The Gemma models come in lightweight sizes of 7 billion (7B) and 2 billion (2B) parameters. These models have been widely adopted since their launch, with millions of downloads across major model hubs, allowing developers and researchers to customize the base Gemma model and utilize pre-trained variants. The introduction of Gemma models aims to enhance accessibility and performance in various AI applications."
"What is the purpose of the Navarasa project, and how does it relate to the Gemma model?","The language is a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navrasa. Navrasa is a model that's trained for Indic languages. It's a fine tune model based on Google's Gemma. We built Navrasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. So we should use AI to make sure that no one is following and no one is using it. Today, the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of Generative AI that we want to bring to every corner of India and the entire world.
Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now since it launched less than three months ago it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, Polly Gemma, our first vision language open model and it's available right now. It's optimized for a range of image captioning visual Q&A and other image labeling tasks so go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model but it's still got a fitness size that's easy for all of you to use. So in a few weeks we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on next-gen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. To wrap up I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. Languages are a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navarasa. Navarasa is a model that's trained for Indic languages. It's a fine tuned model based on Google's Gemma. We built Navarasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. Today the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of generative AI that we want to bring to every corner of India and the entire world.","The purpose of the Navarasa project is to create a set of instruction-tuned models that expand access to 15 Indic languages. It aims to make large language models culturally rooted, allowing people to communicate in their native languages and receive responses in the same language. 

Navarasa is a fine-tuned model based on Google's Gemma model. Gemma features a powerful tokenizer that enables it to handle a vast vocabulary, including hundreds of thousands of words, symbols, and characters across various alphabets and language systems. This capability is critical for adapting Gemma to support projects like Navarasa, ensuring that the technology can understand and address the linguistic nuances of diverse cultures in India. The overarching goal is to leverage generative AI to solve real-world problems in native languages, making technology more accessible to everyone in India and beyond."
What are the three breakthroughs in Android that were announced at Google I/O 2024?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
You'll see these and work coming to hundreds of billions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, on-device foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel, later this year, we'll be expanding what's possible with our latest model Gemini Nano with multi-modality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed Talkback an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses Talkback will typically come across 90 unlabeled images per day. Thankfully, Talkback makes them accessible. And now we're taking that to the next level with the multi-modal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the styling cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to Talkback are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scandal continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello. I am calling from the Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh yeah, what kind of charges? I can't give you specifics over the phone, but to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. And look at this. My phone gives me a warning that this call might be a stab. Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone, thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools like Gemini Nano and Gemini in Android Studio. Also, stay tuned tomorrow for our upcoming Android 15 updates, which we can't wait to share with you. As we said at the outset, we're reimagining Android with Gemini at the core from your favorite apps to the OS itself. We're bringing the power of AI to every aspect of the smartphone experience. And with that, let me head over to Josh to share more on our use for developers. Thank you.","The three breakthroughs in Android announced at Google I/O 2024 include:

1. **On-Device AI with Gemini Nano**: Android is the first mobile operating system to integrate a built-in, on-device foundation model, allowing for faster experiences while protecting user privacy. This enables the phone to understand the world through various inputs, including text, sounds, and spoken language.

2. **Enhanced Accessibility Features**: The improvements to the Talkback feature, which assists users with blindness or low vision, will now provide richer and clearer descriptions of images and other content, enhancing accessibility for users.

3. **Fraud Protection Features**: Android will introduce capabilities to help protect users from fraud by detecting suspicious activity in real-time, such as warning users about potentially fraudulent calls, all while keeping the audio processing private and on the device. 

These advancements aim to elevate the overall smartphone experience by leveraging AI technology."
"How many devices currently support the Circle to Search feature, and what is the goal for the end of the year?","Is that how it works? Am I doing AI? Yeah, it just does whatever you type. What a last minute gift ideas you can make with arts and crafts. Plan a workout routine to get bigger calves. Help me think of titles my tell all men war. What's something smart I can say about Renoir? Generate another image of a cat playing guitar. If a girl calls me a snack, how do I reply? Yeah, that's how it works. You do AI. Make this email sound more professional before it hits. What's a good excuse to cancel dinner with my friends? We're literally sitting right here. No wrong way to prompt. Hey, if you begin your own way to prompt, just every time you're prompting a prompt bar. Or just generate an image of a cat playing guitar. You know, it can do other stuff, right? Hi, everyone. It's great to be back at Google I.O. Today, you've seen how AI is transforming our products across Gemini, search, workspace, and more. We're bringing all these innovations right onto your Android phone. And we're going even further to make Android the best place to experience Google AI. This new era of AI is a profound opportunity to make smart phones truly smart. Our phones have come a long way in a short time. But if you think about it, it's been years since the user experience has fundamentally transformed. This is a once in a generation moment to reinvent what phones can do. So we've embarked on a multi-year journey to reimagine Android with AI at the core. And it starts with three breakthroughs you'll see this year. First, we're putting AI-powered search right at your fingertips, creating entirely new ways to get the answers you need. Second, Gemini is becoming your new AI assistant on Android. There to help you anytime. And third, we're harnessing on device AI to unlock new experiences that work as fast as you do while keeping your sensitive data private. Let's start with AI-powered search. Earlier this year, we took an important first step at Samsung unpacked by introducing Circle to Search. It brings the best of search directly into the user experience. So you can go deeper on anything you see on your phone without switching apps. Fashionistas are finding the perfect shoes. Home chefs are discovering new ingredients. And with our latest update, it's never been easier to translate whatever's on your screen, like a social post in another language. And there are even more ways Circle to Search can help. One thing we've heard from students is that they're doing more of their school work directly on their phones and tablets. So we thought, could Circle to Search be your perfect study buddy? Let's say my son needs help with a tricky physics word problem, like this one. My first thought is, oh boy, it's been a while since I've thought about kinematics. If he stumped on this question, instead of putting me on the spot, he can circle the exact part he's stuck on and get step by step instructions. Right where he's already doing to work. Ah, of course, final velocity equals initial velocity plus acceleration times elapsed time. Right. I was just about to say that. Seriously though, I love how it shows how to solve the problem, not just the answer. This new capability is available today. And later this year, Circle to Search will be able to tackle more complex problems involving symbolic formulas, diagrams, graphs, and more. Circle to Search is only on Android. It's available on more than 100 million devices today, and we're on track to double that by the end of the year. You've already heard from Sissy about the incredible updates coming to the Gemini app. On Android, Gemini is so much more. It's becoming a foundational part of the Android experience. Here's Dave to share more. Hey everyone. A couple of months ago, we launched Gemini on Android. And like Circle to Search, Gemini works at the system level. So instead of going to a separate app, I can bring Gemini right to what I'm doing. Now, we're making Gemini context aware. So it can then anticipate what you're trying to do and provide more helpful suggestions in the moment. In other words, to be a more helpful assistant. So let me show you how this works. And I have my shiny new Pixel 8A here to help me.
Is that how it works? Am I doing AI? Yeah, it just does whatever you type. What a last minute gift ideas you can make with arts and crafts. Plan a workout routine to get bigger calves. Help me think of titles my tell all men war. What's something smart I can say about Renoir? Generate another image of a cat playing guitar. If a girl calls me a snack, how do I reply? Yeah, that's how it works. You're doing AI. Make this email sound more professional before it hits. What's a good excuse to cancel dinner with my friends? We're literally sitting right here. No wrong way to prompt. Hey, if you begin, no wrong way to prompt. Just every time you're prompting a prompt bar. We're just generating an image of a cat playing guitar. You know I can do other stuff, right? Hi, everyone. It's great to be back at Google I.O. Today, you've seen how AI is transforming our products across Gemini, Search, Workspace, and more. We're bringing all these innovations right onto your Android phone. And we're going even further to make Android the best place to experience Google AI. This new era of AI is a profound opportunity to make smart phones truly smart. Our phones have come a long way in a short time. But if you think about it, it's been years since the user experience has fundamentally transformed. This is a once in a generation moment to reinvent what phones can do. So we've embarked on a multi-year journey to reimagine Android with AI at the core. And it starts with three breakthroughs you'll see this year. First, we're putting AI-powered search right at your fingertips, creating entirely new ways to get the answers you need. Second, Gemini is becoming your new AI assistant on Android. There to help you anytime. And third, we're harnessing on device AI to unlock new experiences that work as fast as you do while keeping your sensitive data private. Let's start with AI-powered search. Earlier this year, we took an important first step at Samsung Unpacked by introducing Circle to Search. It brings the best of search directly into the user experience. So you can go deeper on anything you see on your phone without switching apps. Fashionistas are finding the perfect shoes. Home chefs are discovering new ingredients and with our latest update, it's never been easier to translate whatever's on your screen, like a social post in another language. And there are even more ways Circle to Search can help. One thing we've heard from students is that they're doing more of their schoolwork directly on their phones and tablets. So we thought, could Circle to Search be your perfect study buddy? Let's say my son needs help with a tricky physics word problem, like this one. My first thought is, oh boy, it's been a while since I've thought about kinematics. If he stumped on this question, instead of putting me on the spot, he can circle the exact part he's stuck on and get step by step instructions, right, where he's already doing to work. Of course, final velocity equals initial velocity, plus acceleration times elapsed time. Right. I was just about to say that. Seriously though, I love how it shows how to solve the problem, not just the answer. This new capability is available today. And later this year, Circle to Search will be able to tackle more complex problems involving symbolic formulas, diagrams, graphs, and more. Circle to Search is only on Android. It's available on more than 100 million devices today, and we're on track to double that by the end of the year. You've already heard from Sissy about the incredible updates coming to the Gemini app. On Android, Gemini is so much more. It's becoming a foundational part of the Android experience. Here's Dave to share more. Hi, everyone. A couple of months ago, we launched Gemini on Android. And like Circle to Search, Gemini works at the system level. So instead of going to a separate app, I can bring Gemini right to what I'm doing. Now, we're making Gemini context aware. So it can then dissipate what you're trying to do and provide more helpful suggestions in the moment. In other words, to be a more helpful assistant. So let me show you how this works. And I have my shiny new Pixel 8A here to help me.","Currently, the Circle to Search feature is available on more than 100 million devices. The goal is to double that number by the end of the year."
What is the significance of Gemini Nano in the context of Android's AI capabilities?,"You'll see these and work coming to hundreds of billions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, on-device foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel, later this year, we'll be expanding what's possible with our latest model Gemini Nano with multi-modality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed Talkback an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses Talkback will typically come across 90 unlabeled images per day. Thankfully, Talkback makes them accessible. And now we're taking that to the next level with the multi-modal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the styling cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to Talkback are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scandal continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello. I am calling from the Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh yeah, what kind of charges? I can't give you specifics over the phone, but to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. And look at this. My phone gives me a warning that this call might be a stab. Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone, thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools like Gemini Nano and Gemini in Android Studio. Also, stay tuned tomorrow for our upcoming Android 15 updates, which we can't wait to share with you. As we said at the outset, we're reimagining Android with Gemini at the core from your favorite apps to the OS itself. We're bringing the power of AI to every aspect of the smartphone experience. And with that, let me head over to Josh to share more on our use for developers. Thank you.
And let's hit that. Because I've heard that rule may be changing. Now, because I'm a Gemini advanced user, this works on any PDF and takes full advantage of the long context window. And there's just lots of times when that's useful. For example, let's say you're looking for a quick answer in an appliance user manual. And there you have it. It turns out, no, spin serves are not allowed. So Gemini not only gives me a clear answer to my question, it also shows me exactly where on the PDF to learn more. Awesome. OK. So that's a few of the ways that we're enhancing Gemini to do more context aware and helpful in the moment. And what you've seen here are the first really many new ways that Gemini will unlock new experiences at the system level. And they're only available on Android. You'll see these and work coming to hundreds of millions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, undivided foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel later this year, we'll be expanding what's possible with our latest model, Gemini Nano with multimodality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed TalkBack, an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses TalkBack, will typically come across 90 unlabeled images per day. Thankfully, TalkBack makes them accessible. And now we're taking that to the next level, with the multimodal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the style and cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to TalkBack are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scan continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello? Hi, I'm calling from Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh, yeah. What kind of charges? I can't give you specifics over the phone. But to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. LAUGHTER And look at this. My phone gives me a warning that this call might be a scam. APPLAUSE Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools, like Gemini Nano and Gemini in Android Studio.","Gemini Nano is significant in the context of Android's AI capabilities as it represents a major advancement in integrating artificial intelligence directly into the operating system. This built-in, on-device foundation model enhances the smartphone experience by enabling faster and more private interactions. Key aspects of Gemini Nano include:

1. **Multi-modality**: It allows devices to understand inputs not just through text, but also through sights, sounds, and spoken language, making interactions more intuitive and accessible.

2. **Accessibility Enhancements**: For users with disabilities, such as blindness or low vision, Gemini Nano improves features like Talkback, providing richer descriptions of images and enhancing navigation through touch and spoken feedback.

3. **Fraud Protection**: The AI can detect suspicious activities in real-time, alerting users to potential scams directly on their devices, thus enhancing security and privacy.

4. **Context Awareness**: Gemini Nano can understand the content displayed on the screen and provide relevant suggestions, improving user experience across various applications.

5. **On-device Processing**: By processing data locally, Gemini Nano minimizes latency and ensures that sensitive information remains private, as it does not require a network connection for many of its functions.

Overall, Gemini Nano is positioned as a transformative technology that reimagines how users interact with their smartphones, making AI an integral part of the Android ecosystem."
What improvements to the Talkback feature are expected with the introduction of Gemini Nano?,"You'll see these and work coming to hundreds of billions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, on-device foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel, later this year, we'll be expanding what's possible with our latest model Gemini Nano with multi-modality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed Talkback an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses Talkback will typically come across 90 unlabeled images per day. Thankfully, Talkback makes them accessible. And now we're taking that to the next level with the multi-modal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the styling cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to Talkback are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scandal continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello. I am calling from the Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh yeah, what kind of charges? I can't give you specifics over the phone, but to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. And look at this. My phone gives me a warning that this call might be a stab. Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone, thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools like Gemini Nano and Gemini in Android Studio. Also, stay tuned tomorrow for our upcoming Android 15 updates, which we can't wait to share with you. As we said at the outset, we're reimagining Android with Gemini at the core from your favorite apps to the OS itself. We're bringing the power of AI to every aspect of the smartphone experience. And with that, let me head over to Josh to share more on our use for developers. Thank you.
And let's hit that. Because I've heard that rule may be changing. Now, because I'm a Gemini advanced user, this works on any PDF and takes full advantage of the long context window. And there's just lots of times when that's useful. For example, let's say you're looking for a quick answer in an appliance user manual. And there you have it. It turns out, no, spin serves are not allowed. So Gemini not only gives me a clear answer to my question, it also shows me exactly where on the PDF to learn more. Awesome. OK. So that's a few of the ways that we're enhancing Gemini to do more context aware and helpful in the moment. And what you've seen here are the first really many new ways that Gemini will unlock new experiences at the system level. And they're only available on Android. You'll see these and work coming to hundreds of millions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, undivided foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel later this year, we'll be expanding what's possible with our latest model, Gemini Nano with multimodality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed TalkBack, an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses TalkBack, will typically come across 90 unlabeled images per day. Thankfully, TalkBack makes them accessible. And now we're taking that to the next level, with the multimodal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the style and cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to TalkBack are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scan continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello? Hi, I'm calling from Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh, yeah. What kind of charges? I can't give you specifics over the phone. But to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. LAUGHTER And look at this. My phone gives me a warning that this call might be a scam. APPLAUSE Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools, like Gemini Nano and Gemini in Android Studio.","The improvements to the Talkback feature with the introduction of Gemini Nano include enhanced multimodal capabilities that will provide richer and clearer descriptions of images sent to users. For instance, when a user who relies on Talkback receives a photo, they will get a more detailed description of what is happening in the image. Additionally, when shopping online, users will receive clear descriptions of the style and cut of clothing items, helping them find the perfect look. These enhancements aim to make images more accessible and improve the overall user experience for individuals with blindness or low vision. The improvements are expected to minimize latency as the processing will occur on-device, and the model will continue to function even without a network connection."
What proactive feature does Gemini Nano provide to protect users from potential fraud during phone calls?,"And that means I can ask very esoteric questions like, for example, our spin serves allowed. And let's hit that because I've heard that rule maybe changing. Now, because I'm a Gemini advanced user, this works on any PDF and takes full advantage of the long context window. And there's just lots of times when that's useful. For example, let's say you're looking for a quick answer in an appliance user manual. And there you have it. It turns out, no, spin serves are not allowed. So Gemini not only gives me a clear answer to my question, it also shows me exactly when we're on the PDF to learn more. Awesome. Okay. So that's a few of the ways that we're enhancing Gemini to do more context aware and helpful in the moment. And what you've seen here are the first really many new ways that Gemini will unlock new experiences at the system level. And they're only available on Android. You'll see these and work coming to hundreds of billions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, on-device foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel later this year, we'll be expanding what's possible with our latest model, Gemini Nano with multi-modality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed TalkBack, an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Carol, who uses TalkBack, will typically come across 90 unlabeled images per day. Thankfully, TalkBack makes them accessible. And now we're taking that to the next level with the multi-modal capabilities of Gemini Nano. So when someone sends Carol a photo, she'll get a richer and clearer description of what's happening. Or let's say Carol is shopping online for an outfit. Now she can get a crystal clear description of the styling cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to TalkBack are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scandal continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello. Hi, I'm Pauline from Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh yeah, what kind of charges? I can't give you specifics over the phone. But to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. And look at this. My phone gives me a warning that this call might be a scam. I'm going to help you. Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone, thanks to the on device model. So remember that pickleball example earlier? Gemini and Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone.
And let's hit that. Because I've heard that rule may be changing. Now, because I'm a Gemini advanced user, this works on any PDF and takes full advantage of the long context window. And there's just lots of times when that's useful. For example, let's say you're looking for a quick answer in an appliance user manual. And there you have it. It turns out, no, spin serves are not allowed. So Gemini not only gives me a clear answer to my question, it also shows me exactly where on the PDF to learn more. Awesome. OK. So that's a few of the ways that we're enhancing Gemini to do more context aware and helpful in the moment. And what you've seen here are the first really many new ways that Gemini will unlock new experiences at the system level. And they're only available on Android. You'll see these and work coming to hundreds of millions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, undivided foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel later this year, we'll be expanding what's possible with our latest model, Gemini Nano with multimodality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed TalkBack, an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses TalkBack, will typically come across 90 unlabeled images per day. Thankfully, TalkBack makes them accessible. And now we're taking that to the next level, with the multimodal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the style and cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to TalkBack are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scan continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello? Hi, I'm calling from Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh, yeah. What kind of charges? I can't give you specifics over the phone. But to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. LAUGHTER And look at this. My phone gives me a warning that this call might be a scam. APPLAUSE Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools, like Gemini Nano and Gemini in Android Studio.","Gemini Nano provides a proactive feature that alerts users when it detects suspicious activity during phone calls. For example, if a caller claims to be from a bank and asks the user to transfer money to a secure account due to unauthorized charges, Gemini Nano will warn the user that the call might be a scam. This alert helps protect users from potential fraud by keeping the audio processing private and on the device."
What is the primary vision for the Gemini app as described in the event?,"We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. Over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now Gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now based on my flight information, Gemini knows that I needed two and a half day, itinerary. You can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens.","The primary vision for the Gemini app, as described in the event, is to be the most helpful personal AI assistant by providing users with direct access to Google's latest AI models. The app aims to assist users in various tasks such as learning, creating, coding, and more, while offering a natively multimodal experience that allows interaction through text, voice, or camera. Additionally, the app is designed to adapt to individual user needs and preferences, enabling customization through features like ""gems,"" which are personal experts on specific topics."
"What new feature allows users to customize their experience with Gemini, and what are they called?","Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. Over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now Gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now based on my flight information, Gemini knows that I needed two and a half day, itinerary. You can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens.
We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. You're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a pure reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new Dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunery for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules. And Gemini did this in a fraction of the time.","The new feature that allows users to customize their experience with Gemini is called ""Gems."" Users can create personal experts on any topic they want by setting up these Gems, which are simple to create and can be tailored to specific needs."
"What significant capability does Gemini Advanced offer regarding context windows, and how does it compare to other chatbots?","Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens. That is the longest context window of any chatbot in the world. It unlocks incredible new potential in AI, so you can tackle complex problems that were previously unimaginable. You can upload a PDF up to 1,500 pages long or multiple files to get insights across a project. And soon you can upload as much as 30,000 lines of code or even an hour long video. Gemini Advanced is the only chatbot that lets you process this amount of information. Now just imagine how useful this will be for students. Let's say you've spent months on your thesis and you could really use a fresh perspective. You can upload your entire thesis, your sources, your notes, your research, and soon interview audio recordings and videos too. So Gemini has all of this context to give you actionable advice. It can dissect your main points, identify improvements, and even roleplay as your professor. So you can feel confident in your work. And check out what Gemini Advanced can do with your spreadsheets, with the new data analysis feature launching in the coming weeks. Maybe you have a side hustle selling handcrafted products. But you're a better artist than accountant, and it's really hard to understand which products are worth your time. Simply upload all of your spreadsheets and ask Gemini to visualize your earnings and help you understand your profit. Gemini goes to work, calculating your returns, and pulling its analysis together into a single chart so you can easily understand which products are really paying off. Now behind the scenes, Gemini writes custom Python code to crunch these numbers. And of course your files are not used to train our models. Oh, and just one more thing. Later this year, we'll be doubling the long context window to two million tokens. We absolutely can't wait for you to try all of this for yourself. Gemini is continuing to evolve and improve at a breakthrough pace. We're making Gemini more multimodal, more agentive, and more intelligent, with the capacity to process the most information of any chatbot in the world. And as you heard earlier, we're also expanding Gemini Advanced to over 35 supported languages available today. But of course, what makes Gemini so compelling is how easy it is to do just about anything you want with a simple prompt. Let's take a look. Enter a prompt here. Okay, it can be that hard. How about generate an image of a cat playing guitar? Is that how it works? Am I doing AI? Yeah, just does whatever you type. What a last minute gift ideas you can make with arts and crafts. Plan a workout routine to get bigger calves. Help me think of titles my tell all men more. What's something smart I can say about Renoir? Generate another image of a cat playing guitar. If a girl calls me a snack, how do I reply? Yeah, that's how it works. You're doing AI. Make this email somewhere professional before it hits. What's a good excuse to cancel dinner with my friends? They're literally sitting right here. You know I can do other stuff, right?
As context windows grow, we're discovering new ways to interact with models to solve tasks. Huge context windows unlock a new set of AI applications, but they're also more computationally expensive, which is why I'm excited to announce the new context-caching feature coming next month. So how does it work? Well, let's say a large part of your prompt doesn't change. You can cache that once and easily call it unsubssequent turns for a fraction of the price. So imagine a major analyst trying to figure out what to write about next. To help me brainstorm ideas, I might want to gather all the content I've already created. Previous articles, some video clips, maybe some voice memos. Using context-caching, I'll send all this content at once, which let's say it will cost me about 20 cents for that first turn, along with the regular input price for the rest of my prompts. Since I cache that content that I'm referencing, each following turn will only cost me 10 cents, plus the cost of any new prompts. Of course, this concept is extensible to so many different applications that you might want to build. These could involve things like legal documents, research papers, school assignments, or employee training materials. The sky is the limit. Gemini models are empowering developers worldwide to transform workflows, enhancing efficiency, and also adding a touch of user delight. Take, for example, front-end development. Locify.ai takes a Figma design and uses their own AI models to generate code. But what's really unique is how they're pairing with Gemini's 1 million token context window. You see, oftentimes, front-end designs contain hundreds of layers with nondescript names such as rectangle one or group 76. Locify sends this entire design to 1.5 pro to generate meaningful names in the front-end code. Because Gemini understands the purpose behind rectangle one, it's actually able to rename it in code as something more useful like header background. Another one that I love is how AI models are unlocking new abilities. Invision is powering people with low vision to better understand their immediate environment through an app or smart glasses and ask specific questions. Because of the speed of 1.5 flash, users will be able to get quick responses. Another developer, Zapier, is enabling anybody to seamlessly connect different apps and automate repetitive tasks like podcast editing. They built a workflow to automatically remove speaker artifacts like OZ and OMS from audio tracks that's triggered as soon as a user uploads a file to Google Drive. As developers, the ideas you have and the code you write will help transform AI into practical applications for the world. We're announcing our Gemini API developer competition for a chance to win the grand prize, a custom electric de lorean. Woo! Woo! Woo! Woo! Woo! Woo! For those of you in the audience, be sure to check it out at the AI Pavilion, maybe grab a picture, and visit our website for more details. And now I'm going to leave you with my favorite developer to tell you a little bit more about it. Hey! You may recognize me for my acting work, but my real passion is programming. When I'm in the zone, I'm unstoppable. I don't know if you know this about me, but I'm really excited about the future and Google's Gemini models are tech that can get it there. And you know I'm going to be a great developer that you know I'm submitting to Google's new Gemini developer contest, which is so cool, it'll blow your mind back to beta. A chance to win for the most creative, useful, and remarkable apps. If you're a dev or a dev team that wants to submit, great! But you'll have some catching up to do. I just drank three cups of hydro cold brew and I'm ready to crank out some serious times of the world poll. Now we'll look at examples of how AI unlocks excellent experiences and how we can get the best results. Jacqueline talked about how Gemini models benefit all developers. Now we'll look at examples of how AI unlocks excellent experiences on Android, the world's most popular operating system. We're reimagining Android with AI at the core, enabling you to build a whole new class of apps on mobile. This unlocks experiences that were not even possible just a few years ago. So let's dive into the ways that Gemini can power building the next generation of apps on Android.","Gemini Advanced offers a significant capability with the longest context window of any chatbot in the world, allowing for up to 1 million tokens. This extensive context window enables users to tackle complex problems that were previously unimaginable, such as uploading large documents (up to 1,500 pages) or multiple files for insights across a project. In comparison to other chatbots, Gemini Advanced stands out by allowing users to process a greater amount of information, making it particularly useful for tasks like thesis reviews, data analysis, and more, which enhances its utility for students and professionals alike. Additionally, there are plans to double the context window to two million tokens later in the year, further expanding its capabilities."
"How does Gemini assist users in trip planning, and what factors does it consider?","We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules. And Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens. That is the longest context window of any chatbot in the world. It unlocks incredible new potential in AI so you can tackle complex problems that were previously unimaginable. You can upload a PDF up to 1500 pages long or multiple files to get insights across a project. And soon you can upload as much as 30,000 lines of code or even an hour long video. Gemini Advanced is the only chatbot that lets you process this amount of information. Now just imagine how useful this will be for students. Let's say you spent months on your thesis and you could really use a fresh perspective. You can upload your entire thesis, your sources, your notes, your research, and soon interview audio recordings and videos too. So Gemini has all of this context to give you actionable advice. It can dissect your main points, identify improvements, and even role play as your professor. So you can feel confident in your work. And check out what Gemini Advanced can do with your spreadsheets, with the new data analysis feature launching in the coming weeks. Maybe you have a side hustle selling handcrafted products, but you're a better artist than accountant. And it's really hard to understand which products are worth your time. Simply upload all of your spreadsheets and ask Gemini to visualize your earnings and help you understand your profit. Gemini goes to work, calculating your returns, and pulling its analysis together into a single chart so you can easily understand which products are really paying off. Now behind the scenes, Gemini writes custom Python code to crunch these numbers. And of course your files are not used to train our models. Oh, and just one more thing. Later this year, we'll be doubling the long context window to two million tokens. We absolutely can't wait for you to try all of this for yourself. Gemini is continuing to evolve and improve at a breakthrough pace. We're making Gemini more multimodal, more agentive, and more intelligent, with the capacity to process the most information of any chatbot in the world. And as you heard earlier, we're also expanding Gemini Advanced to over 35 supported languages available today. But of course, what makes Gemini so compelling is how easy it is to do just about anything you want with a simple prompt. Let's take a look. Enter a prompt here. Okay, it can be that hard. How about generate an image of a cat playing guitar? Is that how it works? Am I doing AI? Yeah, it just does whatever you type. What a last minute gift ideas you can make with arts and crafts. Plan a workout routine to get bigger calves. Help me think of titles my tell all men war. What's something smart I can say about Renoir? Generate another image of a cat playing guitar. If a girl calls me a snack, how do I reply? Yeah, that's how it works. You do AI. Make this email sound more professional before it hits. What's a good excuse to cancel dinner with my friends? We're literally sitting right here. No wrong way to prompt. Hey, if you begin your own way to prompt, just every time you're prompting a prompt bar. Or just generate an image of a cat playing guitar. You know, it can do other stuff, right?","Gemini assists users in trip planning by gathering information from various sources, including search results and helpful extensions like maps and Gmail. It creates a dynamic graph of possible travel options based on the user's input, which includes personal preferences and constraints. 

The factors that Gemini considers in trip planning include:

1. **User Preferences**: It takes into account the interests of different family members, such as art and seafood, as mentioned in the example.
2. **Logistics**: Gemini considers flight and hotel details, which it retrieves from the user's Gmail inbox.
3. **Spatial Data**: It uses spatial information to make decisions about activities and dining options based on proximity to the hotel and the timing of the flight.
4. **Dynamic Adjustments**: Users can modify their itinerary, and Gemini will adjust the rest of the trip accordingly, ensuring that the plan remains cohesive and tailored to the user's needs.

Overall, Gemini aims to streamline the trip planning process, making it more efficient and personalized for the user."
"What upcoming feature related to data analysis will Gemini Advanced provide, and how can it benefit users?","Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. Over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now Gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now based on my flight information, Gemini knows that I needed two and a half day, itinerary. You can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens.
Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens. That is the longest context window of any chatbot in the world. It unlocks incredible new potential in AI, so you can tackle complex problems that were previously unimaginable. You can upload a PDF up to 1,500 pages long or multiple files to get insights across a project. And soon you can upload as much as 30,000 lines of code or even an hour long video. Gemini Advanced is the only chatbot that lets you process this amount of information. Now just imagine how useful this will be for students. Let's say you've spent months on your thesis and you could really use a fresh perspective. You can upload your entire thesis, your sources, your notes, your research, and soon interview audio recordings and videos too. So Gemini has all of this context to give you actionable advice. It can dissect your main points, identify improvements, and even roleplay as your professor. So you can feel confident in your work. And check out what Gemini Advanced can do with your spreadsheets, with the new data analysis feature launching in the coming weeks. Maybe you have a side hustle selling handcrafted products. But you're a better artist than accountant, and it's really hard to understand which products are worth your time. Simply upload all of your spreadsheets and ask Gemini to visualize your earnings and help you understand your profit. Gemini goes to work, calculating your returns, and pulling its analysis together into a single chart so you can easily understand which products are really paying off. Now behind the scenes, Gemini writes custom Python code to crunch these numbers. And of course your files are not used to train our models. Oh, and just one more thing. Later this year, we'll be doubling the long context window to two million tokens. We absolutely can't wait for you to try all of this for yourself. Gemini is continuing to evolve and improve at a breakthrough pace. We're making Gemini more multimodal, more agentive, and more intelligent, with the capacity to process the most information of any chatbot in the world. And as you heard earlier, we're also expanding Gemini Advanced to over 35 supported languages available today. But of course, what makes Gemini so compelling is how easy it is to do just about anything you want with a simple prompt. Let's take a look. Enter a prompt here. Okay, it can be that hard. How about generate an image of a cat playing guitar? Is that how it works? Am I doing AI? Yeah, just does whatever you type. What a last minute gift ideas you can make with arts and crafts. Plan a workout routine to get bigger calves. Help me think of titles my tell all men more. What's something smart I can say about Renoir? Generate another image of a cat playing guitar. If a girl calls me a snack, how do I reply? Yeah, that's how it works. You're doing AI. Make this email somewhere professional before it hits. What's a good excuse to cancel dinner with my friends? They're literally sitting right here. You know I can do other stuff, right?","Gemini Advanced will provide a new data analysis feature that allows users to upload their spreadsheets. This feature can help users, such as those running a side hustle, to visualize their earnings and understand their profits. By analyzing the uploaded data, Gemini can calculate returns and present the analysis in a single chart, making it easier for users to identify which products are worth their time. Additionally, Gemini writes custom Python code behind the scenes to perform these calculations, ensuring that users receive actionable insights based on their data."
What percentage improvement in productivity did Sports Basement experience after rolling out Gemini for workspace?,"The Sons last May we've been hard at work making German IFA workspace even more helpful for businesses and consumers across the world Tens of thousands of customers have been using help me write help me visualize help me organize since we launched and Now we're really excited that the new Gemini powered side panel will be generally available next month One of our customers is a local favorite right here in California sports basement They rolled out Gemini for workspace to the organization and this has helped improve the productivity of their customer support team by more than 30 percent Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages We are really excited about what Gemini 1.5 Pro So price pro unlocks for workspace and AI premiums customers Let me start by showing you three new capabilities coming to Gmail mobile This is my Gmail account Okay, there's an email up top from my husband help me sort out the roof repair thing please now We've been trying to find a contractor to fix our roofs and with work travel. I have clearly dropped the ball It looks like there's an email thread on this with lots of emails that I haven't read and luckily for me I can simply tap the summarize option up top and skip reading this long back and forth Now Gemini pulls up this helpful mobile card as an overlay and this is where I can read a nice summary of all the salient information that I need to know So I see here that we have a quote from Jeff at Green Roofing and he's ready to start Now I know we had other bids and I don't remember the details Previously I would have had to do a number of searches in Gmail and then remember and compare information across different emails Now I can simply type out my question right here in the mobile card and say something like Compare my roof repair bids by price and availability This new Q&A feature makes it so easy to get quick answers on anything in my inbox For example when am I shoes arriving or what time the doors open for the next game without having to first search Gmail Then open the email and then look for the specific information and attachments and so on Anyway back to my roofs It looks like Gemini has found details that I got from two other contractors in completely different email threads And I have this really nicely organized summary and I can do a quick comparison So it seems like Jeff's quote was right in the middle. You can start immediately. So Green Roofing it is I'll open that last email from Jeff and confirm the project and look at that I Seasons suggested replies from Gemini now what is really really neat about this evolution of smart reply is that it's Contextual Gemini understood the back and forth and that thread and that Jeff was ready to start so offers me a few customized options based on that context So you know here. I see I have declined the service suggest a new time. I'll choose proceed and confirm time I can even see a preview of the full reply simply by long pressing This looks reasonable so I'll hit send These new capabilities and Gemini and Gmail will start rolling out this month to labs users Okay, so one of the really neat things about works space apps like Gmail drive docs calendar is how well they work together And in our daily lives we often have information that flows from one app to another like say adding a calendar entry from Gmail or creating reminders from a spreadsheet tracker But what if Gemini can make these journeys totally seamless perhaps even automate them for you entirely So let me show you what I mean with a real-life example My sister is a self-employed photographer and her inbox is full of appointment bookings receipts client feedback on photos and so much more Now if you're a freelancer or a small business you really want to focus on your craft and not on bookkeeping and logistics So let's go to her inbox and take a look Lots of unread emails. Let's click on the first one It's got a PDF that's an attachment from a hotel as a receipt and I see a suggestion in the side panel Help me organize and track my receipts. Let's click on this prompt The side panel now show me more detailed about what that really means and as you can see There's two steps here step one Create a drive folder and put this receipt and 37 others. It's found into that folder.
Do my thing, do my thing. I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. You know, you know, you know, you know, you know, you know. I got the news, dude. I got the new game. Yeah, you know? Yeah, you know? I got the news, dude. I got the news, no. No, no, no, no. No, no, no, no, no. I got the news, dude. I got the news, dude. I got the news, duh. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Help Me Organize since we launched. And now we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini for workspace to the organization and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5 Pro Unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now we've been trying to find a contractor to fix our roofs and with work travel I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read. And luckily for me I can simply tap the summarize option up top and skip reading this long back and forth. Now Gemini pulls up this helpful mobile card as an overlay and this is where I can read a nice summary of all the salient information that I need to know. So I see here that we have a quote from Jeff at Green Roofing and he's ready to start. Now I know we had other bids and I don't remember the details. Previously I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now I can simply type out my question right here in the mobile card and say something like compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox. For example when am I shoes arriving or what time to doors open for the next game without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway back to my roofs. It looks like Gemini has found details that I got from two other contractors in completely different email threads and I have this really nicely organized summary and I can do a quick comparison. So it seems like Jeff's quote was right in the middle and you can start immediately so Green Roofing it is. I'll open that last email from Jeff and confirm the project and look at that. I see some suggested replies from Gemini. Now what is really really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread and that Jeff was ready to start so offers me a few customized options based on that context. So here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay so one of the really neat things about Workspace Apps like Gmail, Drive, Docs, Calendar is how well they work together. In our daily lives we often have information that flows from one app to another like say adding a calendar entry from Gmail or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely. So let me show you what I mean with a real life example.",Sports Basement experienced a productivity improvement of more than 30 percent after rolling out Gemini for workspace.
How many languages does the automatic language detection feature of Gemini support?,"And it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to AIStudio.Google.com, log in with your Google account. And you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit Run. Now in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, it starts streaming it back. Now this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from notebook LM, like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype here in 10 seconds. And with one click in the upper left, get an API key. Or over here in the upper right, just tap Get Code. And you've got all of the model configurations, the safety settings, ready to go, straight into your IDE. Now over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and then using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. It's optimized for a range of image captioning visual Q&A and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model. But it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on next gen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. To wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. Languages are a very trusting problem to solve actually. And given India has a huge variety of languages and it changes every 5 kilometres. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India.
I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.",The automatic language detection feature of Gemini supports 35 languages.
What are the two main steps Gemini automates for organizing receipts in the example provided?,"Let's click on the first one It's got a PDF that's an attachment from a hotel as a receipt and I see a suggestion in the side panel Help me organize and track my receipts. Let's click on this prompt The side panel now show me more detailed about what that really means and as you can see There's two steps here step one Create a drive folder and put this receipt and 37 others. It's found into that folder. Make sense Step two extract the relevant information from those receipts in that folder into a new spreadsheet now This sounds useful why not I also have the option to edit these actions or just hit OK So let's hit OK Gemini will now complete the two steps described above and this is where it gets even better Gemini offers you the option to automate this so that this particular workflow is run on all future emails Keeping your drive folder an expense sheet up to date with no effort from you Now Now we know that creating a complex spreadsheet like this can be daunting for most people But with this automation Gemini does the hard work of extracting all the right information from all the files and in that folder and Generates this sheet for you. So let's take a look. Okay. It's super well organized and it even has a category for expense type now We have the sheet things can get even more fun We can ask Gemini questions questions like show me where the money spent Gemini not only analyzes the data from the sheet But also creates a nice visual to help me see the complete breakdown by category And you can imagine how this extends to all sorts of use cases in your inbox like travel expenses Shopping remodeling projects you name it all of that information and Gmail can be put to good use and help you Work plan and play better now this particular This particular ability to organize your attachments and drive and generate a sheet and do data analysis via Q&A We'll be rolling out to labs users this September and it's just one of the many automations that we're working on in workspace Work space in the Gemini era will continue to unlock new ways of getting things done We're building advanced agent of experiences including customizing how you use Gemini Now as we look to 2025 and beyond we're exploring entirely new ways of working with AI Now with Gemini you have an AI power assistant always at your side But what if you could expand how you interact with AI For example when we work with other people we mention them in comments and docs or we send them emails We have group chats with them etc And it's not just how we collaborate with each other But we each have a specific role to play in the team and as the team works together We build a set of collective experiences and contacts to learn from each other We have the combined set of skills to draw from when we need help So how could we introduce AI into this mix and build on this shared expertise Well, here's one way We're prototyping a virtual Gemini powered teammate This teammate has an identity a workspace account along with a specific role and objectives Let me bring Tony up to show you what I mean Hey Tony Hey everyone okay So let me start by showing you how we set up this virtual teammate As you can see the teammate has this very own account and we can go ahead and give it a name. We'll do something fun like Chip Chips been given a specific job role at the set of descriptions on how to be helpful for the team You see that here and some of the jobs are to monitor and track projects We've listed a few out to organize information and provide context and a few more things Now that we've configured our virtual teammate Let's go ahead and see chip in action to do that. I'll switch us over here to Google chat First when planning for an event like IO we have a ton of chat rooms for various purposes Luckily for me chip is in all of them To quickly catch up I might ask a question like Anyone know if our IO Storyboards are approved Because we've instructed chip to track this project chip searches across all the conversations and knows respond with an answer There it is simple but very helpful Now as the team adds chip to more group chats more files more email threads Chip builds a collective memory of our work together Let's look at an example to show you all switch over to a different room. How about project sapphire over here And here we are discussing a product release coming up and as usual many pieces are still in flight so I can go ahead and ask Are we on track?
So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example. My sister is a self-employed photographer, and her inbox is full of appointment bookings, receipts, client feedback on photos, and so much more. Now, if you're a freelancer or a small business, you really want to focus on your craft and not on bookkeeping and logistics. So, let's go to her inbox and take a look. Lots of unread emails. Let's click on the first one. It's got a PDF. That's an attachment from a hotel as a receipt, and I see a suggestion in the side panel. Help me organize and track my receipts. Let's click on this prompt. The side panel now will show me more details about what that really means. And as you can see, there's two steps here. Step one. Create a drive folder and put this receipt and 37 others it's found into that folder. Make sense. Step two. Extract the relevant information from those receipts in that folder into a new spreadsheet. Now, this sounds useful. Why not? I also have the option to edit these actions, or just hit OK. So, let's hit OK. Gemini will now complete the two steps described above, and this is where it gets even better. Gemini offers you the option to automate this so that this particular workflow is run on all future emails, keeping your drive folder and expense sheet up to date with no effort from you. Now, we know that creating a complex spreadsheet like this can be daunting for most people. But with this automation, Gemini does the hard work of extracting all the right information from all the files and in that folder and generates this sheet for you. So, let's take a look. OK, it's super well organized and it even has a category for expense type. Now, we have the sheet. Things can get even more fun. We can ask Gemini questions, questions like show me where the money spent. Gemini not only analyzes the data from the sheet, but also creates a nice visual to help me see the complete breakdown by category. You can imagine how this extends to all sorts of use cases in your inbox like travel expenses, shopping, remodeling projects, you name it. All of that information in Gmail can be put to good use and help you work, plan, and play better. Now, this particular... I know. This particular ability to organize your attachments and drive and generate a sheet and do data analysis via Q&A will be rolling out to labs users this September. And it's just one of the many automations that we're working on in workspace. Workspace in the Gemini era will continue to unlock new ways of getting things done. We're building advanced agent of experiences, including customizing how you use Gemini. Now, as we look to 2025 and beyond, we're exploring entirely new ways of working with AI. Now, with Gemini, you have an AI-powered assistant always at your side. But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs, or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and context to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives.","The two main steps that Gemini automates for organizing receipts in the example provided are:

1. **Create a drive folder** and put the receipt along with 37 others that it finds into that folder.
2. **Extract the relevant information** from those receipts in that folder into a new spreadsheet."
What is the name of the virtual Gemini powered teammate prototype mentioned in the presentation?,"Now, with Gemini, you have an AI-powered assistant always at your side. But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs, or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and context to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives. Let me bring Tony up to show you what I mean. Hey, Tony. Hey, partner. Hey, everyone. Okay. So let me start by showing you how we set up this virtual teammate. As you can see, the teammate has this very own account, and we can go ahead and give it a name. We'll do something fun like Chip. Chip's been given a specific job role at the set of descriptions on how to be helpful for the team. You can see that here. And some of the jobs are to monitor and track projects. We've listed a few out to organize information and provide context and a few more things. Now that we've configured our virtual teammate, let's go ahead and see Chip in action to do that. I'll switch us over here to Google Chat. First, when planning for an event like IO, we have a ton of chat rooms for various purposes. Luckily for me, Chip is in all of them. To quickly catch up, I might ask a question like, anyone know if our IO storyboards are approved? Because we've instructed Chip to track this project, Chip searches across all the conversations and knows respond with an answer. There it is. Simple but very helpful. Now, as the team adds Chip to more group chats, more files, more email threads, Chip builds a collective memory of our work together. Let's look at an example to show you all switch over to a different room. How about a project sapphire over here? And here we are discussing a product release coming up. And as usual, many pieces are still in flight. So I can go ahead and ask, are we on track for launch? Chip gets to work not only searching through everything it has access to, but also synthesizing what's found and coming back with an up-to-date response. There it is. A clear timeline and a summary, and notice even in this first message here, Chip flags a potential issue the team should be aware of. Because we're in a group space, everyone can follow along, anyone can jump in at any time, as you see someone just did, asking Chip to help create a doc to help address the issue. A task like this could take me hours, dozens of hours, Chip can get all done in just a few minutes, sending the doc over right when it's ready. And so much of this practical helpfulness comes from how we've customized Chip to our team's needs, and how seamlessly this AI is integrated directly into where we're already working. Back to you, Aparna. Thank you, Tony. Now, I can imagine a number of virtual types of virtual teammates configured by businesses to help them do what they need. Now, we have a lot of work to do to figure out how to bring these agent of experiences like virtual teammates into workspace, including enabling third parties to make their very own versions of Chip. We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google.
But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and contacts to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives. Let me bring Tony up to show you what I mean. Hey, Tony. Hey, partner. Hey, everyone. Okay. So let me start by showing you how we set up this virtual teammate. As you can see, the teammate has this very own account. And we can go ahead and give it a name. We'll do something fun like Chip. Chip's been given a specific job role at the set of descriptions on how to be helpful for the team. You can see that here. And some of the jobs are to monitor and track projects. We've listed a few out to organize information and provide context and a few more things. Now that we've configured our virtual teammate, let's go ahead and see Chip in action. To do that, I'll switch this over here to Google Chat. First, when planning for an event like I.O., we have a ton of chat rooms for various purposes. Luckily for me, Chip is in all of them. To quickly catch up, I might ask a question like, anyone know if our I.O. storyboards are approved? Because we've instructed Chip to track this project, Chip searches across all the conversations and knows respond with an answer. There it is. Simple but very helpful. Now, as the team adds Chip to more group chats, more files, more email threads, Chip builds a collective memory of our work together. Let's look at an example to show you all switch over to a different room. How about a project sapphire over here? And here we are discussing a product release coming up. And as usual, many pieces are still in flight. So I can go ahead and ask, are we on track for launch? Chip gets to work not only searching through everything it has access to, but also synthesizing what's found and coming back with an up-to-date response. There it is. A clear timeline, a nice summary, and notice even in this first message here, Chip flags a potential issue the team should be aware of. Because we're in a group space, everyone can follow along, anyone can jump in at any time, as you see someone just did, asking Chip to help create a doc to help address the issue. A task like this could take me hours, dozens of hours, Chip can get all done in just a few minutes, sending the doc over right when it's ready. So much of this practical helpfulness comes from how we've customized Chip to our team's needs, and how seamlessly this AI is integrated directly into where we're already working back to you, Aparna. Thank you, Tony. Now, I can imagine a number of virtual types of, number of different types of virtual teammates configured by businesses to help them do what they need. Now we have a lot of work to do to figure out how to bring these agent of experiences like virtual teammates into workspace, including enabling third parties to make their very own versions of Chip. We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience.","The name of the virtual Gemini-powered teammate prototype mentioned in the presentation is ""Chip."""
What new capabilities are being introduced to Gmail mobile with Gemini 1.5 Pro?,"Do my thing, do my thing. I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. You know, you know, you know, you know, you know, you know. I got the news, dude. I got the new game. Yeah, you know? Yeah, you know? I got the news, dude. I got the news, no. No, no, no, no. No, no, no, no, no. I got the news, dude. I got the news, dude. I got the news, duh. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Help Me Organize since we launched. And now we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini for workspace to the organization and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5 Pro Unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now we've been trying to find a contractor to fix our roofs and with work travel I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read. And luckily for me I can simply tap the summarize option up top and skip reading this long back and forth. Now Gemini pulls up this helpful mobile card as an overlay and this is where I can read a nice summary of all the salient information that I need to know. So I see here that we have a quote from Jeff at Green Roofing and he's ready to start. Now I know we had other bids and I don't remember the details. Previously I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now I can simply type out my question right here in the mobile card and say something like compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox. For example when am I shoes arriving or what time to doors open for the next game without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway back to my roofs. It looks like Gemini has found details that I got from two other contractors in completely different email threads and I have this really nicely organized summary and I can do a quick comparison. So it seems like Jeff's quote was right in the middle and you can start immediately so Green Roofing it is. I'll open that last email from Jeff and confirm the project and look at that. I see some suggested replies from Gemini. Now what is really really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread and that Jeff was ready to start so offers me a few customized options based on that context. So here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay so one of the really neat things about Workspace Apps like Gmail, Drive, Docs, Calendar is how well they work together. In our daily lives we often have information that flows from one app to another like say adding a calendar entry from Gmail or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely. So let me show you what I mean with a real life example.
I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. I got the news, dude. I got the news, dude. I got therada, I got theirdoooo. I got the news, dude. I got the news, dude. I got the news, dudes. I got the news, dude. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Me Organize, Since We Launched. And now, we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini-for-Works-based to the organization, and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5-Private Pro unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now, we've been trying to find a contractor to fix our roofs, and with Work Travel, I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read, and luckily for me, I can simply tap the summarized option up top and skip reading this long back and forth. Now, Gemini pulls up this helpful mobile card as an overlay, and this is where I can read a nice summary of all the salient information that I need to know. So, I see here that we have a quote from Jeff at Green Roofing, and he's ready to start. Now, I know we had other bids, and I don't remember the details. Previously, I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now, I can simply type out my question right here in the mobile card and say something like, compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox, for example, when am I shoes arriving, or what time the doors open for the next game, without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway, back to my roof. It looks like Gemini has found details that I got from two other contractors in completely different email threads, and I have this really nicely organized summary, and I can do a quick comparison. So, it seems like Jeff's quote was right in the middle, and he can start immediately, so Green Roofing it is. I'll open that last email from Jeff and confirm the project, and look at that. I see some suggested replies from Gemini. Now, what is really, really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread, and that Jeff was ready to start, so offers me a few customized options based on that context. So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example.","The new capabilities being introduced to Gmail Mobile with Gemini 1.5 Pro include:

1. **Summarization Feature**: Users can tap a summarize option to get a concise summary of long email threads, allowing them to quickly grasp the salient information without reading through all the emails.

2. **Q&A Feature**: This feature enables users to type questions directly in the mobile card, such as comparing roof repair bids by price and availability, making it easier to get quick answers without searching through emails.

3. **Contextual Smart Replies**: Gemini offers suggested replies based on the context of the email thread, understanding the back-and-forth conversation and providing customized options for responses.

These capabilities aim to enhance productivity and streamline the email management experience for users."
What is the expected performance improvement of the sixth generation TPUs called Trillium compared to the previous generation?,"Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years, we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensile processing units that power our AI advances, Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs, we are proud to offer CPUs and GPUs to support any workload that includes the new Axion processes we announced last month, our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge black-well GPUs available in early 2025. We are fortunate to have a long-standing partnership with NVIDIA, excited to bring black-wells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI Hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying the raw hardware and chips. Our AI Hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We've been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly one gigawatt and growing. That's close to 70 times the capacity of any other fleet. Undroline this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers using better context, location awareness, and real time information. With advances in natural language understanding and computer vision, we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course, search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities, our high bar for information quality, our decades of experience connecting you to the richness of the web, the result is a product that does the work for you. Google search is a generative AI at the scale of human curiosity and it's our most exciting chapter of search yet. To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search.
At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features are really good. They're a great way to create a new world. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways, and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AGI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensed processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs we are proud to offer CPUs and GPUs to support any workload. That includes the new Axion processes we announced last month. Our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge blackwell GPUs available in the market. We are fortunate to have a long standing partnership with NVIDIA, excited to bring blackwells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better.",The expected performance improvement of the sixth generation TPUs called Trillium compared to the previous generation is 4.7 times (4.7x).
How much has the industry demand for ML compute grown in the last six years?,"Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years, we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensile processing units that power our AI advances, Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs, we are proud to offer CPUs and GPUs to support any workload that includes the new Axion processes we announced last month, our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge black-well GPUs available in early 2025. We are fortunate to have a long-standing partnership with NVIDIA, excited to bring black-wells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI Hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying the raw hardware and chips. Our AI Hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We've been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly one gigawatt and growing. That's close to 70 times the capacity of any other fleet. Undroline this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers using better context, location awareness, and real time information. With advances in natural language understanding and computer vision, we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course, search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities, our high bar for information quality, our decades of experience connecting you to the richness of the web, the result is a product that does the work for you. Google search is a generative AI at the scale of human curiosity and it's our most exciting chapter of search yet. To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search.
At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features are really good. They're a great way to create a new world. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways, and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AGI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensed processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs we are proud to offer CPUs and GPUs to support any workload. That includes the new Axion processes we announced last month. Our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge blackwell GPUs available in the market. We are fortunate to have a long standing partnership with NVIDIA, excited to bring blackwells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better.",The industry demand for ML compute has grown by a factor of 1 million in the last six years.
What are the three unique strengths of Google search in the Gemini era as mentioned in the presentation?,"A Google we are fully in our Gemini era. Today all of our two billion user products use Gemini. Gemini 1.5 Pro is available today in Workspace Labs. Let's see how this comes to life with Google Workspace. People are always searching their emails and Gmail. We are working to make it much more powerful with Gemini. Now we can ask Gemini to summarize all recent emails from the school. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. People love using photos to search across their life. With Gemini, you're making that a whole lot easier. And ask photos can also help you search your memories in a deeper way. For example, you might be reminiscing about your daughter Lucius, early milestones. You can ask photos, show me how Lucius swimming has progressed. Here Gemini goes beyond a simple search, recognizing different contexts and photos packages it up all together in a summary. Unlocking knowledge across formats is why we build Gemini to be multimodal from the ground up. It's one model with all the modalities built in. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. So today, we are expanding the context window to two million tokens. So far, we've talked about two technical advances, multimodality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. But what if we could go even further? That's one of the opportunities we see with AI agents. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Here's a video of our prototype, which we'll see has two parts. Each part was captured in a single take in real time. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector, IV. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. Give me a banner name for this duo. Golden Stripes. Nice. Thanks, Gemini. Today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context. There's one more area I'm really excited to share with you. Our teams have made some incredible progress in genitive video. Today, I'm excited to announce our newest, most capable genitive video model, called VO. VO creates high quality 1080p videos from text, image and video prompts. It can capture the details of your instructions in different visual and cinematic styles. For 25 years, we have invested in world-class technical infrastructure. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. Google Search is a generative AI at the scale of human curiosity, and it's our most exciting chapter of Search yet. All the advancements you'll see today are made possible by a new Gemini model, customized for Google Search. What really sets this apart is our three unique strengths. This is Search in the Gemini era. By the end of the year, AI overviews will come to over a billion people. We're making AI overviews even more helpful for your most complex questions. The types that are really more like 10 questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. I'm really excited to share that soon you'll be able to ask questions with video. Why will this not stay in place? In the near-instead, Google gives me an AI overview. I guess some reasons this might be happening and steps I can take to troubleshoot.
To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly, complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today, with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. You make this possible for introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates to use in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you've got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there. So since you're asking about places, it taps into Google's index of information about the real world with over 250 million places and updated in real time, including their ratings, reviews, business hours, and more. Research that might have taken you minutes or even hours Google can now do on your behalf in just seconds. Next, let me show you another way multi-step reasoning in Google search can make your life that much easier. Take planning, for example. Planning of trips and meal plans can be fun. But doing the work of actually figuring it all out, no thank you. With Gemini in search, Google does the planning with you. Planning is really hard for AI to get right. It's the type of problem that takes advance reasoning and logic. After all, if you're meal planning, you probably don't want mac and cheese for breakfast, lunch, and dinner. Okay, my tics might, but say you're looking for a bit more variety. Now, you can ask search to create a three-day meal plan for a group that's easy to prepare. And here you get a plan with a wide range of recipes from across the web. This one for overnight oats looks particularly interesting. And you can easily head over to the website to learn how to prepare them. If you want to get more veggies in, you can simply ask search to swap in a vegetarian dish. And just like that, search customizes your meal plan.","The three unique strengths of Google search in the Gemini era, as mentioned in the presentation, are:

1. **Real-time information**: Google has access to over a trillion facts about people, places, and things, allowing it to provide up-to-date information.

2. **Unparalleled ranking and quality systems**: Google has trusted ranking and quality systems that have been developed over decades to deliver the very best of the web to users.

3. **The power of Gemini**: The Gemini model unlocks new agent capabilities right in search, enhancing the overall search experience and enabling more advanced functionalities."
"What is the total deployed fleet capacity for liquid cooling systems at Google, and how does it compare to other fleets?","We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision, we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course, search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities are high bar for information quality, our decades of experience connecting you to the richness of the web, the result is a product that does the work for you. Google search is a generative AI at the scale of human curiosity and it's our most exciting chapter of search yet to tell you more. Here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. Now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind and whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. To make this possible, we're introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates studios in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning.
At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features are really good. They're a great way to create a new world. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways, and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AGI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensed processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs we are proud to offer CPUs and GPUs to support any workload. That includes the new Axion processes we announced last month. Our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge blackwell GPUs available in the market. We are fortunate to have a long standing partnership with NVIDIA, excited to bring blackwells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better.",The total deployed fleet capacity for liquid cooling systems at Google is nearly 1 gigawatt. This capacity is close to 70 times the capacity of any other fleet in the industry.
What new capabilities will Google search offer in terms of planning and organizing information?,"To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly, complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today, with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. You make this possible for introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates to use in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you've got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there. So since you're asking about places, it taps into Google's index of information about the real world with over 250 million places and updated in real time, including their ratings, reviews, business hours, and more. Research that might have taken you minutes or even hours Google can now do on your behalf in just seconds. Next, let me show you another way multi-step reasoning in Google search can make your life that much easier. Take planning, for example. Planning of trips and meal plans can be fun. But doing the work of actually figuring it all out, no thank you. With Gemini in search, Google does the planning with you. Planning is really hard for AI to get right. It's the type of problem that takes advance reasoning and logic. After all, if you're meal planning, you probably don't want mac and cheese for breakfast, lunch, and dinner. Okay, my tics might, but say you're looking for a bit more variety. Now, you can ask search to create a three-day meal plan for a group that's easy to prepare. And here you get a plan with a wide range of recipes from across the web. This one for overnight oats looks particularly interesting. And you can easily head over to the website to learn how to prepare them. If you want to get more veggies in, you can simply ask search to swap in a vegetarian dish. And just like that, search customizes your meal plan.
Soon you'll be able to ask search to find the best yoga Pilates studios in Boston and show you details on their inter offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there. So since you're asking about places, it taps into Google's index of information about the real world with over 250 million places and updated in real time, including their ratings, reviews, business hours, and more. Research that might have taken you minutes or even hours Google can now do on your behalf in just seconds. Next, let me show you another way multi-step reasoning in Google search can make your life that much easier. Take planning, for example. Dreaming of trips and meal plans can be fun. But doing the work of actually figuring it all out, no thank you. With Gemini in search, Google does the planning with you. Planning is really hard for AI to get right. It's the type of problem that takes advanced reasoning and logic. After all, if you're meal planning, you probably don't want mac and cheese for breakfast, lunch, and dinner. Okay, my tips might, but say you're looking for a bit more variety. Now you can ask search to create a three day meal plan for a group that's easy to prepare. And here you get a plan with a wide range of recipes from across the web. This one for overnight oats looks particularly interesting. And you can easily head over to the website to learn how to prepare them. If you want to get more veggies in, you can simply ask search to swap in a vegetarian dish. And just like that, search customizes your meal plan. And you can export your meal plan or get the ingredients as a list just by tapping here. Looking ahead, you can imagine asking Google to add everything to your preferred shopping cart. Then we're really cooking. These planning capabilities mean search will be able to help plan everything from meals and trips to parties, dates, work out routines, and more. So you can get all the fun of planning without any of the hassle. You've seen how Google search can help with increasingly complex questions and planning. But what about all those times when you don't know exactly what to ask? And you need some help brainstorming. When you come to search for ideas, you'll get more than an AI generate answer. You'll get an entire AI organized page, custom built for you, end your question. Say you're heading to Dallas to celebrate your anniversary and you're looking for the perfect restaurant. What you get here breaks AI out of the box and it brings it to the whole page. Our Gemini model, it covers the most interesting angles for you to explore and organizes these results into these helpful clusters. Like you might never have considered restaurants with live music or ones with historic charm. Our model even uses contextual factors like the time of the year. So since it's warm in Dallas, you can get rooftop patios as an idea. And it pulls everything together into a dynamic whole page experience. You'll start to see this new AI organized search results page when you look for inspiration, starting with dining and recipes, and coming to movies, music, books, hotels, shopping, and more. Today you've seen how you can bring any question to search and Google takes the work out of searching. But your questions aren't limited to words in a text box and sometimes even that picture can't tell the whole story. Earlier, Demis showed you our latest advancements in video understanding. And I'm really excited to share that soon you'll be able to ask questions with video right in Google search. Let me introduce Rose to show you this in a live demo. Thank you, Liz. I have always known a record player. And I got this one and some finals that you're still recently.","Google search will introduce several new capabilities that enhance planning and organizing information through its custom Gemini model. These capabilities include:

1. **Multi-Step Reasoning**: Google search will utilize multi-step reasoning to break down complex questions into manageable parts, allowing it to provide comprehensive answers. For example, users can ask for detailed information about yoga and Pilates studios in Boston, including ratings, introductory offers, and walking distances from specific locations.

2. **Meal Planning**: Users will be able to ask Google to create meal plans tailored to their needs. For instance, they can request a three-day meal plan that is easy to prepare, and Google will provide a variety of recipes. Users can also customize their meal plans by swapping in vegetarian dishes or exporting ingredient lists.

3. **Dynamic Search Results**: When users seek ideas or inspiration, Google will present an AI-organized page that clusters results based on various angles. This feature will help users explore options they might not have considered, such as restaurants with live music or rooftop patios, depending on contextual factors like the season.

4. **Comprehensive Planning Assistance**: Google search will assist with planning various activities, from meals and trips to parties and workouts, making the planning process more enjoyable and less burdensome.

5. **Video Interaction**: Users will soon be able to ask questions using video directly in Google search, enhancing the way information is organized and accessed.

Overall, these advancements aim to simplify the research and planning process, allowing users to get relevant information quickly and efficiently."
What is the primary goal of DeepMind as stated by its co-founder?,"Thanks, and it's so great to be here. Ever since I was a kid playing chess for the England Junior Team, I'd been thinking about the nature of intelligence. I was captivated by the idea of a computer that could think like a person. It's ultimately why it became a programmer and studied neuroscience. I co-founded DeepMind in 2010 with the goal of one day building AGI, Artificial General Intelligence, a system that has human level cognitive capabilities. I've always believed that if we could build this technology responsibly, its impact would be truly profound and it could benefit humanity in incredible ways. Last year, we reached a milestone on that path when we formed Google DeepMind, combining AI talent from across the company into one super unit. Since then, we built AI systems that can do an amazing range of things, from turning language and vision into action for robots, navigating complex virtual-threading environments, solving a limpiav level math problems, and even discovering thousands of new materials. Just last week, we announced our next-generation alpha-fold model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. All of this was made possible with the best infrastructure for the AI era, including our highly optimized tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multi-modal because that's how we interact with and understand the world around us. We've built a variety of models for different use cases, you've seen how powerful Gemini 1.5 Pro is, but we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale while still featuring multi-modal reasoning capabilities and breakthrough-long context. Flash is optimized for tasks where low latency and efficiency matter most. Starting today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI, and developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. You'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models, but we're always striving to push the state of the art even further. At any one time, we have many different models in training, and we use our very large and powerful ones to help teach and train our production-ready models. Together with user feedback, this cutting-edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multi-modal AI. And this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multi-modal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action, and it would have to be proactive, teachable and personal, so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multi-modal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called?
We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.","The primary goal of DeepMind, as stated by its co-founder, is to build Artificial General Intelligence (AGI), a system that has human-level cognitive capabilities. The co-founder believes that if this technology is built responsibly, its impact could be truly profound and benefit humanity in incredible ways."
What are the key features of the newly introduced Gemini 1.5 Flash model?,"Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response. And it's OK if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to aisstudio.google.com, log in with your Google account, and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit run. Now, in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, it starts streaming it back. Now, this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from Notebook LM, like we showed earlier. Now, what's great about this is that you can take something like this in AI Studio, prototype tier in 10 seconds. And with one click in the upper left, get an API key, or over here in the upper right, just tap get code. And you've got all of the model configurations, the safety settings, ready to go, straight into your IDE. Now, over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way, you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. OK. OK. APPLAUSE Now, let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now, since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. APPLAUSE It's optimized for a range of image captioning, visual Q&A, and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model, but it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on NextGen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. APPLAUSE So wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look.","The key features of the newly introduced Gemini 1.5 Flash model include:

1. **Speed**: Designed for quick tasks where the speed of the model is a priority, making it suitable for applications that require fast responses.

2. **Low Latency**: The model is optimized for low latency, allowing it to process information and return results rapidly.

3. **Multimodal Capabilities**: Like its counterpart, Gemini 1.5 Pro, it supports natively multimodal inputs, meaning it can handle text, images, audio, and video together.

4. **Large Context Window**: It can manage a massive context window of up to 1 million tokens, with an option for a 2 million token context window available for 1.5 Pro users.

5. **Developer Features**: It includes new developer features such as:
   - **Video Frame Extraction**: Available in the Gemini API.
   - **Parallel Function Calling**: Allows multiple function calls to be returned at once.
   - **Context Caching**: Enables users to send files to the model once without needing to resend them repeatedly.

6. **Cost-Effective Pricing**: The pricing model is competitive, starting at 35 cents per 1 million tokens, with promotional pricing for larger prompts.

7. **Integration with AI Studio**: Users can easily access and utilize the model through AI Studio, which is free to use and requires no configuration to get started.

These features make Gemini 1.5 Flash a versatile tool for developers looking for efficiency and speed in their AI applications."
What significant milestone was reached by Google DeepMind last year?,"Thanks, and it's so great to be here. Ever since I was a kid playing chess for the England Junior Team, I'd been thinking about the nature of intelligence. I was captivated by the idea of a computer that could think like a person. It's ultimately why it became a programmer and studied neuroscience. I co-founded DeepMind in 2010 with the goal of one day building AGI, Artificial General Intelligence, a system that has human level cognitive capabilities. I've always believed that if we could build this technology responsibly, its impact would be truly profound and it could benefit humanity in incredible ways. Last year, we reached a milestone on that path when we formed Google DeepMind, combining AI talent from across the company into one super unit. Since then, we built AI systems that can do an amazing range of things, from turning language and vision into action for robots, navigating complex virtual-threading environments, solving a limpiav level math problems, and even discovering thousands of new materials. Just last week, we announced our next-generation alpha-fold model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. All of this was made possible with the best infrastructure for the AI era, including our highly optimized tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multi-modal because that's how we interact with and understand the world around us. We've built a variety of models for different use cases, you've seen how powerful Gemini 1.5 Pro is, but we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale while still featuring multi-modal reasoning capabilities and breakthrough-long context. Flash is optimized for tasks where low latency and efficiency matter most. Starting today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI, and developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. You'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models, but we're always striving to push the state of the art even further. At any one time, we have many different models in training, and we use our very large and powerful ones to help teach and train our production-ready models. Together with user feedback, this cutting-edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multi-modal AI. And this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multi-modal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action, and it would have to be proactive, teachable and personal, so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multi-modal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called?
Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years, we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensile processing units that power our AI advances, Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs, we are proud to offer CPUs and GPUs to support any workload that includes the new Axion processes we announced last month, our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge black-well GPUs available in early 2025. We are fortunate to have a long-standing partnership with NVIDIA, excited to bring black-wells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI Hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying the raw hardware and chips. Our AI Hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We've been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly one gigawatt and growing. That's close to 70 times the capacity of any other fleet. Undroline this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers using better context, location awareness, and real time information. With advances in natural language understanding and computer vision, we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course, search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities, our high bar for information quality, our decades of experience connecting you to the richness of the web, the result is a product that does the work for you. Google search is a generative AI at the scale of human curiosity and it's our most exciting chapter of search yet. To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search.","Last year, Google DeepMind reached a significant milestone by forming Google DeepMind, which combined AI talent from across the company into one super unit."
"How does the new generative video model, VO, enhance video creation?","We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.
This is what excites me. As a hip-hop producer, we dug in the crates. We play these vinyls, and the part with it is no vocal, we pull it, we sample it, and we create an entire song around that. Right now we dig in an infinite crate. It's endless. Where I found the AI really useful for me, this way to fill in the sparser elements of my loops. Okay, let's try Bongo's. We're going to put Viola. We're going to put Rhythnic clapping. And we're going to see what happens there. And it makes it sound ironically at the end of the day a little more human. So then this is entirely Google's loops right here. These are glutes. It's like having like this weird friend that's just like, try this, try that. And then you're like, oh, okay, yeah, no, that's pretty dope. It's a group in, group in, group in, group in, up, up, up. The tools are capable, speeding up the process of what's in my head getting it out. You're able to move light speed with your creativity. This is amazing. Right there. I think this really shows what's possible when we work with the artist's community on the future of music. You can find some brand new songs from these acclaimed artists and songwriters on their YouTube channels now. There's one more area I'm really excited to share with you. Our teams have made some incredible progress in genitive video. Today, I'm excited to announce our newest, most capable genitive video model called VO. VO creates high quality 1080p videos from text, image and video prompts. It can capture the details of your instructions in different visual and cinematic styles. You can prompt for things like aerial shots of a landscape or time lapse and further edit your videos using additional prompts. You can use VO in our new experimental tool called Video FX. We're exploring features like storyboarding and generating longer scenes. VO gives you unprecedented creative control. Techniques for generating static images have come a long way, but generating video is a different challenge altogether. Not only is it important to understand where an object or subject should be in space, it needs to maintain this consistency over time, just like the car in this video. VO builds upon years of our pioneering genitive video model work, including GQN, FNACI, WALT, Video Poet, Lumiere and much more. We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into a video model. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it, right? As much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today.","The new generative video model, VO, enhances video creation in several significant ways:

1. **High-Quality Output**: VO is capable of creating high-quality 1080p videos from various prompts, including text, images, and videos.

2. **Creative Control**: It offers unprecedented creative control, allowing users to prompt for specific visual and cinematic styles, such as aerial shots or time-lapse sequences.

3. **Efficiency in Visualization**: VO enables users to visualize ideas on a time scale that is 10 to 100 times faster than traditional methods, facilitating quicker iterations and creative exploration.

4. **Improved Consistency**: The model builds upon years of research and combines various architectures to enhance consistency, quality, and output resolution in video creation.

5. **Multi-Modal Capabilities**: By utilizing Gemini's multi-modal capabilities, VO can better capture nuances from prompts, including cinematic techniques and visual effects.

6. **Facilitating Storytelling**: VO emphasizes storytelling, making it accessible for everyone to become a director and share their stories, thereby fostering creativity and understanding among users.

7. **Experimental Tools**: It is integrated into new experimental tools like Video FX, which explore features such as storyboarding and generating longer scenes, further expanding the creative possibilities.

Overall, VO represents a significant advancement in generative video technology, enabling users to create videos more efficiently and creatively than ever before."
What is Project Astra and what capabilities does it aim to develop?,"Yes. Let's see what it says. Hey, my name is Michael Chang and we're watching the Google IO keynote while it's happening with Project Astra. Let's take a look. Hey, Gemini, currently watching Google IO keynote right now. You want to watch it with me? Sure, I'd love to watch it with you. What's happening in the keynote? What does it look like? What does it look like they're announcing right now? It looks like they're announcing Gemini 1.5 Flash. The speaker is talking about its speed and efficiency in comparison to the pro model. What are the other features? It looks like Gemini 1.5 Flash is optimized for tasks where low latency and high efficiency matter most. Cool. The screen also shows. Oh, yeah, sorry, I interrupted you. Keep going. That's all right. What were we talking about? Well, we were talking about the capabilities of 1.5 Flash. Do you remember? Yes, we were discussing the new Gemini model and its efficiency and speed. The presentation seems to have moved on though. What would you like to talk about now? Well, I'm curious about what he's going to talk about next. He seems to be moving on to the topic of model training and feedback. Perhaps they will talk about how user input helps train the models. It looks like he just announced a new project. What is that project called? They are calling that project, Project Astra. It looks like they are calling that project Astra. It looks like they are calling that project Astra. It looks like they are calling that project Astra. It looks to focus on the future of AI assistance. What do you think a project Astra will be helpful for? It sounds like a very ambitious project. It will be interesting to see what kind of AI assistant they create and how it integrates with our daily lives. They seem to say it's a universal agent. What would you use a universal agent for? A universal agent that can help with everyday tasks would be helpful. It could potentially be useful for things like scheduling, finding information, or even controlling smart home devices. Oh, that would be super cool. I agree it sounds amazing. Having a seamless and intuitive AI agent could revolutionize how we interact with technology. Yeah, can you summarize what he showed so far? The speaker introduced Project Astra, an AI-powered universal agent for everyday tasks that is still under development. They discussed the challenges of building this technology and how their team is working toward making Astra helpful and intuitive for users. Wow, amazing. And what did he talk about before Project Astra? He was discussing the Gemini 1.5 model, particularly Gemini 1.5 Flash, which focuses on efficiency and speed. The presentation was focused on the future direction of AI models and user feedbacks roll in training those models. It looks like they're now doing a demo of Project Astra. What does it look like they're doing right now? In the video, someone is holding a phone and panning across the table. The phone is running Project Astra, which can identify a speaker and a cup of crayons. And what is it doing now? It appears to be scanning a computer screen displaying code. Astra then explains what the highlighted section of the code is doing. Wow, that's super cool. Thanks for explaining all of these things to me, Gemini. You're very welcome. Is there anything else I can explain about the presentation? Let's just keep on watching. What's currently happening? In the video, Astra is identifying the King's Cross Neighborhood of London using the phone's camera view. It's also providing some facts about the area. Wow. I can't believe this is...this technology exists right now. It does look quite impressive. Astra seems very advanced at interpreting what it sees and providing relevant information. That was a lot of fun to watch. I'm looking forward to Project Astra as I'm sure all of you are too. Thank you for being here. Thank you for tuning in. Let's go build something amazing!
And this is not just the model completing the code. It's the model coming up with an entire plan and running it. And not only do I get an answer to my initial question about the data set, but the AI agent also produces a fully functional collab notebook based on your instructions. And you can share it with others, allowing for real time collaboration on code, new visualizations, and all of the results. It's available today at labs.google slash code and let us know what you think about it. Now you see why I'm so excited today. You see, we're committed to empowering you to build with genitive AI boldly and responsibly from Gemini power assistance in your favorite IDEs to easy access for our most powerful Gemini model. And it comes with flexibility to use Gemma open models. You have an opportunity to reach anyone anywhere through Google's ecosystem of devices, app experiences, and platform. We continued investing resources to give your projects a boost. Google accelerators and the Google developer portfolio is just a couple of great examples. Our accelerators have guided over 1,300 startups in 84 countries with countless success stories. And that includes 20 startups that have grown to become unicorns. You Genie AI founded by Dr. Sudeth Boyd, it's a recent graduate of our climate change accelerator. You Genie was born out of Dr. Childry's desire to use AI for good. His team helps manufacturing companies to dramatically reduce their carbon emissions. And you Genie's algorithms apply Google DeepMind research which has helped to optimize energy use in Google's own data centers. You Genie was able to streamline their model development and their analytics with accurate emissions data through Google Earth Engine. We love working alongside startups to tackle big problems and invite all of you to apply to an upcoming accelerator soon. If you have one of the 15 million Google developer profiles, you enjoy invitations for early access programs, a chance to participate in research and opportunities to attend special events. Beginning today, your profile is evolving into something even better. Introducing the Google Developer Program. Members of the program can now access new benefits at no cost and I'm going to take you through them. First, you can access Gemini for learning and searching and chatting with documentation. Second, for IDX users, we're now enabling you to go from two work stations up to five. Third, if you're in the Google Cloud Innovators community, you get credits for interactive labs on Google Cloud Skills Boost. Sign up for the Google Developer Program today or if you have an existing profile, you're all set. More benefits are coming soon so be on the lookout. That brings us to the close of today's developer keynote but we're not quite done. There's a lot more to Google I.O than just the keynotes. If you're with us in person, attend the sessions, the workshops, and demos both today and tomorrow. If you're online, you'll find all the sessions available on IO.Google starting this Thursday morning, Mountain View Time. Last year, we kicked off the series of IO connect events to bring the in-person experience from Shoreline to more places that are closer to you. Well, we heard just how much you loved them so we've got three of them lined up this year. Starting in Berlin on June 27th and then we're in Bangalore in July and Beijing in August. And I'll mention, I'll be at all of them so I hope to see you there too. If you can join us at IO connect, come to one of the many community-led IO extended event happening in your local area. Now, I like to share something really cool with you. We had Project Astra watch the keynote this morning. Yes. Let's see what it says. Hey, my name is Michael Chang and we're watching the Google IO keynote while it's happening with Project Astra. Let's take a look. Hey, Gemini, currently watching Google IO keynote right now. You want to watch it with me? Sure, I'd love to watch it with you. What's happening in the keynote? What does it look like? What does it look like they're announcing right now? It looks like they're announcing Gemini 1.5 Flash. The speaker is talking about its speed and efficiency in comparison to the pro model. What are the other features? It looks like Gemini 1.5 Flash is optimized for tasks where low latency and high efficiency matter most. Cool. The screen also shows. Oh, yeah, sorry, I interrupted you. Keep going.","Project Astra is an AI-powered universal agent introduced during the Google I/O keynote. It aims to assist users with everyday tasks by providing a seamless and intuitive interaction with technology. The project focuses on capabilities such as identifying objects and providing relevant information based on what it sees, as demonstrated in a live demo where Astra identified a neighborhood and explained details about it.

Key capabilities that Project Astra aims to develop include:

1. **Task Assistance**: Helping users with scheduling, finding information, and controlling smart home devices.
2. **Object Recognition**: Identifying objects in the environment, as shown when Astra scanned a computer screen and explained the code.
3. **User Interaction**: Engaging with users in a conversational manner, answering questions, and providing insights based on the context of the discussion.
4. **Integration with Daily Life**: Creating a universal agent that can integrate into various aspects of users' daily routines, enhancing their interaction with technology.

Overall, Project Astra is positioned as a forward-looking initiative to revolutionize how AI can assist individuals in their everyday activities."
What is the significance of the Gemini 1.5 Pro model introduced at Google I/O 2024?,"Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
A Google we are fully in our Gemini era. Today all of our two billion user products use Gemini. Gemini 1.5 Pro is available today in Workspace Labs. Let's see how this comes to life with Google Workspace. People are always searching their emails and Gmail. We are working to make it much more powerful with Gemini. Now we can ask Gemini to summarize all recent emails from the school. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. People love using photos to search across their life. With Gemini, you're making that a whole lot easier. And ask photos can also help you search your memories in a deeper way. For example, you might be reminiscing about your daughter Lucius, early milestones. You can ask photos, show me how Lucius swimming has progressed. Here Gemini goes beyond a simple search, recognizing different contexts and photos packages it up all together in a summary. Unlocking knowledge across formats is why we build Gemini to be multimodal from the ground up. It's one model with all the modalities built in. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. So today, we are expanding the context window to two million tokens. So far, we've talked about two technical advances, multimodality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. But what if we could go even further? That's one of the opportunities we see with AI agents. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Here's a video of our prototype, which we'll see has two parts. Each part was captured in a single take in real time. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector, IV. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. Give me a banner name for this duo. Golden Stripes. Nice. Thanks, Gemini. Today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context. There's one more area I'm really excited to share with you. Our teams have made some incredible progress in genitive video. Today, I'm excited to announce our newest, most capable genitive video model, called VO. VO creates high quality 1080p videos from text, image and video prompts. It can capture the details of your instructions in different visual and cinematic styles. For 25 years, we have invested in world-class technical infrastructure. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. Google Search is a generative AI at the scale of human curiosity, and it's our most exciting chapter of Search yet. All the advancements you'll see today are made possible by a new Gemini model, customized for Google Search. What really sets this apart is our three unique strengths. This is Search in the Gemini era. By the end of the year, AI overviews will come to over a billion people. We're making AI overviews even more helpful for your most complex questions. The types that are really more like 10 questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. I'm really excited to share that soon you'll be able to ask questions with video. Why will this not stay in place? In the near-instead, Google gives me an AI overview. I guess some reasons this might be happening and steps I can take to troubleshoot.","The Gemini 1.5 Pro model introduced at Google I/O 2024 is significant for several reasons:

1. **Multimodal Capabilities**: Gemini 1.5 Pro is designed to handle multiple types of inputs, including text, images, audio, and video. This allows developers to create applications that can interleave these modalities, enhancing the versatility of AI applications.

2. **Long Context Window**: The model features a massive context window of up to 2 million tokens, which enables it to process and understand larger amounts of information in a single interaction. This is particularly useful for complex tasks that require a deep understanding of context.

3. **Quality Improvements**: The 1.5 Pro model is optimized for high-quality responses, making it suitable for complex tasks where accuracy and detail are paramount. This is contrasted with the 1.5 Flash model, which is designed for speed.

4. **Developer Features**: The model introduces new developer features such as video frame extraction, parallel function calling, and context caching. These features aim to improve the efficiency and effectiveness of using the model in applications.

5. **Cost Efficiency**: The pricing structure for using the model is designed to be competitive, with costs significantly reduced for larger prompts, making it more accessible for developers.

6. **Integration with Google Cloud**: Gemini 1.5 Pro is integrated into Google Cloud services, allowing developers to scale their applications easily and access enterprise-grade features as their needs grow.

Overall, the introduction of Gemini 1.5 Pro represents a significant advancement in AI capabilities, providing developers with powerful tools to create innovative applications that leverage multimodal AI and extensive context understanding."
How many developers are currently using Gemini models across Google's tools?,"Welcome to the 16th Google I.O. I want to thank our fans, have developed our community all millions of you around the world for choosing to build with Google. Literally, whatever you can imagine, you can bring it to life because only Google's ecosystem gives you the potential to reach people on 3 billion Android devices and 2 billion Chrome and Chromium-based browsers. You've created millions of helpful apps with Firebase, Google Cloud, and generative AI models like Gemini and Gemma. At Google, we're on a mission to make generative AI accessible to every developer on the planet. It's transforming the very fundamentals of software development, including new techniques for how you build and new inspiration for what you build. AI helps you to be more productive by assisting you with many different development tasks, such as writing, debugging, and testing your code, or generating documentation, and understanding your code base. That's why Gemini is available to you, all developers to use in Android Studio, Chrome Dev Tools, Project IDX, CoLab, Beath Code, IntelliJ, and Firebase. CHEERING AND APPLAUSE Gemini is even more helpful with context such as app settings, performance data, your logs, and source code. This is just a glimpse of AI changing how we built. Now, let's go to what you built. Today, people just assumed that things will work great. Of course, any platform, any device that they use, literally anywhere they are, whether on the go, at home, or at work, in their local town, or on the other side of the world. At the same time, the tools that you use need to make that possible and they're constantly evolving. It's great to see the latest technologies come out, but that means more complexity, especially with so many different AI concepts to learn. We want to make your lives simpler. As code becomes content and coders become creators, whether you're building an AI-powered mobile app, a web app, or an interactive experience that works across all platforms, we're providing you with powerful, more integrated development tools, both from Google and the OpenEka system, and it comes with secure, easy access to Google's most capable AI models and global cloud infrastructure. Ultimately, it's about helping you to stay focused on what you want to build, amazing products and services, and not the plumbing, which takes me to our agenda for today. Up next, you'll hear more in how you can use the Gemini API in Google AI Studio to develop engaging and multimodal apps. Then we will take you through how Gemini helps you to create a whole new category of AI-powered experiences on Android and Web, with exceptional levels of productivity. We've also been hard at work creating the very best experienced, full-stack, multi-platform development. Our team will share what's now possible with Project IDX, Flutter, and Firebase. And we think that will dramatically expand what you can build. Then we will talk about how you can build and customize your own AI models with Gemma. You'll see how CoLab, Carrison Jacks, make it really easy. Finally, I'll be back up here to share one more thing before we wrap it up. Are you ready to jump in? OK. Awesome. Jacqueline, come join us. Our investment in our AI research and infrastructure allows us to put the power of AI directly into your hands. We make it easy to start building an AI app with a simple API and a tool that can be used to create a new product. It can be used to create an AI app with a simple API integration. So you can focus on building the best possible products for your users. Now, meeting many of you at Hackathons and Meetups, I am blown away by how you're pushing the boundaries with our new AI models. From new ideas using audio and video inputs, the creative use of longer context windows. It's really inspiring to see. And like you heard about earlier today, balance across quality, cost, and speed. That's why I'm thrilled that Gemini 1.5 Flash is officially open to all developers to start building with today. You can get started using the Gemini API in Google AI Studio, which is now available in more than 200 countries and territories. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. But what I'm seeing more and more these days is that it isn't just about selecting the right model, but also the right techniques to bring your ideas to life.
Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in monks than is happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. So, we are in the very early days of the AI platform shift. We see so much opportunity ahead for creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too, including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries, as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. Applause There's so much innovation happening in search. Thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.",More than 1.5 million developers are currently using Gemini models across Google's tools.
What new feature was introduced in Google Photos that utilizes Gemini's capabilities?,"One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. The so much innovation happening in search, thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago. Since then, people have used it to organize their most important memories. Today, that amounts to more than 6 billion photos and videos uploaded every single day. And people are using photos to search across their life. With Gemini, you're making that a whole lot easier. Say you're at a parking station ready to pay, but you can't recall your license plate number. Before you could search photos for keywords, then scroll through years worth of photos looking for the right one. Now, you can simply ask photos. It knows the cards that appear often, it triangulates which one is yours, and just tells you the license plate number. And ask photos can also help you search your memories in a deeper way. For example, you might be reminiscing about your daughter Lucius early milestones. You can ask photos when did Lucius learn to swim. You can even fall up with something more complex. Show me how Lucius swimming has progressed. Here, Gemini goes beyond a simple search, recognizing different contexts from doing laps in the pool, to snorkeling in the ocean, to the texture and dates on her swimming certificates, and photos packages it up all together in a summary. You can really take it all in and relive amazing memories all over again. We are rolling out asked photos this summer with more capabilities to come. APPLAUSE Unlocking knowledge across formats is why we build Gemini to be multi-model from the ground up. It's one model with all the modalities built in. So not only does it understand each type of input, it finds connections between them. Multi-modality radically expands the questions we can ask, and the answers we will get back. Long context takes this a step further, enabling us to bring in even more information, hundreds of pages of text, hars of audio, a full hour of video, or entire code repos. Or if you want, roughly 96 cheesecake factory menus. For that many menus, you need a one-million token context window, now possible with Gemini 1.5 Pro. Developers have been using it in super interesting ways. Let's take a look. I remember the announcement, the one-million token context window, and my first reaction was, there's no way they were able to achieve this. I wanted to test this technical skills. So I uploaded a line chart. It was temperatures between Tokyo and Berlin, and how they were across the 12 months of the year. So I got in there, and I threw in the Python library that I was really struggling with. And I just asked it a simple question. And it nailed it. It could find specific references, the comments in the code, and specific requests that people had made, and other issues that people had had, but then suggest a fix for it, that related to what I was working on. I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model.
Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Oops! Reacts. Things got bigger. Like way bigger. And it wasn't all just for him, or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. Still, we are in the very early days of the AI platform shift. We see so much opportunity ahead. For creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too. Including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. The so much innovation happening in search, thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.","The new feature introduced in Google Photos that utilizes Gemini's capabilities is called ""Ask Photos."" This feature allows users to search for specific information within their photos by asking questions. For example, users can inquire about their license plate number or ask about their child's milestones, and Gemini can recognize different contexts and provide relevant information from the user's photo library."
What are the two key technical advances discussed at the event that enhance Gemini's capabilities?,"We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example. My sister is a self-employed photographer, and her inbox is full of appointment bookings, receipts, client feedback on photos, and so much more. Now, if you're a freelancer or a small business, you really want to focus on your craft and not on bookkeeping and logistics. So, let's go to her inbox and take a look. Lots of unread emails. Let's click on the first one. It's got a PDF. That's an attachment from a hotel as a receipt, and I see a suggestion in the side panel. Help me organize and track my receipts. Let's click on this prompt. The side panel now will show me more details about what that really means. And as you can see, there's two steps here. Step one. Create a drive folder and put this receipt and 37 others it's found into that folder. Make sense. Step two. Extract the relevant information from those receipts in that folder into a new spreadsheet. Now, this sounds useful. Why not? I also have the option to edit these actions, or just hit OK. So, let's hit OK. Gemini will now complete the two steps described above, and this is where it gets even better. Gemini offers you the option to automate this so that this particular workflow is run on all future emails, keeping your drive folder and expense sheet up to date with no effort from you. Now, we know that creating a complex spreadsheet like this can be daunting for most people. But with this automation, Gemini does the hard work of extracting all the right information from all the files and in that folder and generates this sheet for you. So, let's take a look. OK, it's super well organized and it even has a category for expense type. Now, we have the sheet. Things can get even more fun. We can ask Gemini questions, questions like show me where the money spent. Gemini not only analyzes the data from the sheet, but also creates a nice visual to help me see the complete breakdown by category. You can imagine how this extends to all sorts of use cases in your inbox like travel expenses, shopping, remodeling projects, you name it. All of that information in Gmail can be put to good use and help you work, plan, and play better. Now, this particular... I know. This particular ability to organize your attachments and drive and generate a sheet and do data analysis via Q&A will be rolling out to labs users this September. And it's just one of the many automations that we're working on in workspace. Workspace in the Gemini era will continue to unlock new ways of getting things done. We're building advanced agent of experiences, including customizing how you use Gemini. Now, as we look to 2025 and beyond, we're exploring entirely new ways of working with AI. Now, with Gemini, you have an AI-powered assistant always at your side. But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs, or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and context to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives.","The two key technical advances discussed at the event that enhance Gemini's capabilities are:

1. **Multimodal Interaction**: Gemini is natively multimodal, allowing users to interact using text, voice, or their phone's camera. This enables a more natural way of expressing oneself and enhances the user experience.

2. **Live Interaction with Real-Time Understanding**: Gemini can engage in live conversations using voice, leveraging Google's latest speech models to better understand users and respond naturally. This includes the ability to adapt to users' speech patterns and respond to their surroundings in real time when using the camera."
What is the ultimate goal of Google's AI initiatives as presented at the event?,"This is the end of the video. Thank you. Listening to everything that has been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context, and agents are the cutting edge of what this technology can do, take it to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners, and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting-edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines, and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine three NVO create more realistic imagery and videos. We must also consider how they might be misused to spread misinformation. To help last year, we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today, we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Gemini, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making. And in the coming months, we'll be open sourcing Synth ID text watermarking. This will be available in our updated responsible Gerritive AI Toolkit, which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supposed to appear collaborating with Adobe, Microsoft, Startups, and many others to build that implement standards that improve the transparency of digital media. Now, let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today, our AI advances are helping to solve real-world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases, helping predict floods in more than 80 countries, and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube.
We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.","The ultimate goal of Google's AI initiatives, as presented at the event, is to build AI responsibly while maximizing its benefits for people and society. This involves addressing the risks associated with AI technology, improving models to protect against misuse, and enhancing the accuracy and reliability of AI systems. Additionally, Google's AI advancements aim to solve real-world problems, such as aiding scientists, predicting natural disasters, and making information universally accessible for learning. The introduction of tools like LearnLam, which personalizes learning experiences, exemplifies their commitment to using AI to enhance education and empower individuals. Overall, the focus is on leveraging AI to improve lives while ensuring safety and ethical considerations are prioritized."
What is the purpose of Google's new Gemini Developer Contest?,"Hey! You may recognize me for my acting work, but my real passion is programming. When I'm in the zone, I'm unstoppable. I don't know if you know this about me, but I'm really excited about the future. And Google's Gemini models are a tech that can get it there. And you know I'm submitting to Google's new Gemini Developer Contest, which is so cool, it'll blow your mind back to beta. A chance to win for the most creative, useful and remarkable apps. If you're a dev or a dev team that wants to submit great, but you'll have some catching up to do. I just drank three cups of hydrochol brew, and I'm ready to crank out some serious. Time for the wormhole!
Welcome to the 16th Google I.O. I want to thank our fans, have developed our community all millions of you around the world for choosing to build with Google. Literally, whatever you can imagine, you can bring it to life because only Google's ecosystem gives you the potential to reach people on 3 billion Android devices and 2 billion Chrome and Chromium-based browsers. You've created millions of helpful apps with Firebase, Google Cloud, and generative AI models like Gemini and Gemma. At Google, we're on a mission to make generative AI accessible to every developer on the planet. It's transforming the very fundamentals of software development, including new techniques for how you build and new inspiration for what you build. AI helps you to be more productive by assisting you with many different development tasks, such as writing, debugging, and testing your code, or generating documentation, and understanding your code base. That's why Gemini is available to you, all developers to use in Android Studio, Chrome Dev Tools, Project IDX, CoLab, Beath Code, IntelliJ, and Firebase. CHEERING AND APPLAUSE Gemini is even more helpful with context such as app settings, performance data, your logs, and source code. This is just a glimpse of AI changing how we built. Now, let's go to what you built. Today, people just assumed that things will work great. Of course, any platform, any device that they use, literally anywhere they are, whether on the go, at home, or at work, in their local town, or on the other side of the world. At the same time, the tools that you use need to make that possible and they're constantly evolving. It's great to see the latest technologies come out, but that means more complexity, especially with so many different AI concepts to learn. We want to make your lives simpler. As code becomes content and coders become creators, whether you're building an AI-powered mobile app, a web app, or an interactive experience that works across all platforms, we're providing you with powerful, more integrated development tools, both from Google and the OpenEka system, and it comes with secure, easy access to Google's most capable AI models and global cloud infrastructure. Ultimately, it's about helping you to stay focused on what you want to build, amazing products and services, and not the plumbing, which takes me to our agenda for today. Up next, you'll hear more in how you can use the Gemini API in Google AI Studio to develop engaging and multimodal apps. Then we will take you through how Gemini helps you to create a whole new category of AI-powered experiences on Android and Web, with exceptional levels of productivity. We've also been hard at work creating the very best experienced, full-stack, multi-platform development. Our team will share what's now possible with Project IDX, Flutter, and Firebase. And we think that will dramatically expand what you can build. Then we will talk about how you can build and customize your own AI models with Gemma. You'll see how CoLab, Carrison Jacks, make it really easy. Finally, I'll be back up here to share one more thing before we wrap it up. Are you ready to jump in? OK. Awesome. Jacqueline, come join us. Our investment in our AI research and infrastructure allows us to put the power of AI directly into your hands. We make it easy to start building an AI app with a simple API and a tool that can be used to create a new product. It can be used to create an AI app with a simple API integration. So you can focus on building the best possible products for your users. Now, meeting many of you at Hackathons and Meetups, I am blown away by how you're pushing the boundaries with our new AI models. From new ideas using audio and video inputs, the creative use of longer context windows. It's really inspiring to see. And like you heard about earlier today, balance across quality, cost, and speed. That's why I'm thrilled that Gemini 1.5 Flash is officially open to all developers to start building with today. You can get started using the Gemini API in Google AI Studio, which is now available in more than 200 countries and territories. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. But what I'm seeing more and more these days is that it isn't just about selecting the right model, but also the right techniques to bring your ideas to life.","The purpose of Google's new Gemini Developer Contest is to provide a chance for developers or development teams to submit creative, useful, and remarkable applications. The contest aims to encourage innovation and showcase the potential of Google's Gemini models in building applications that can impress and benefit users."
What are the key features of the Gemini models mentioned in the context?,"We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son and I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, she really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that of course means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes, the foundation for understanding how objects move and interact. Ah, yes, this is where multimodal really shines. Now it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah, can you give my son Jimmy a basketball example? Hey, Jimmy, that's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay, so first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say, the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity or Isaac Newton. But nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format. And it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Shar. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead. Work across software and systems. All to get something done on your behalf. And most importantly, under your supervision. We are still in the early days and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes. And a lot less fun to return them when they don't fit. Imagine if Gemini could do all the steps for you.","The key features of the Gemini models mentioned in the context include:

1. **Multimodal Interaction**: Gemini allows users to interact using text, voice, or camera, enabling a more natural way to express themselves.

2. **Live Conversations**: Users can have in-depth conversations with Gemini using their voice, with the ability to interrupt and adapt to speech patterns.

3. **Customization with Gems**: Users can create personalized ""gems"" that act as personal experts on specific topics, allowing for tailored interactions.

4. **Dynamic Trip Planning**: Gemini can assist in planning trips by gathering information from various sources (like Gmail and maps) and creating personalized itineraries based on user preferences.

5. **Audio Overviews**: In educational contexts, Gemini can generate audio discussions based on text materials, making learning more engaging and interactive.

6. **Intelligent Systems**: Gemini models are designed to show reasoning, planning, and memory, enabling them to think multiple steps ahead and work across different software and systems.

7. **Integration with Workspace**: The models are integrated into Workspace Labs, enhancing productivity and creativity for users.

These features highlight Gemini's capabilities in providing personalized, interactive, and intelligent assistance across various tasks and contexts."
What is the significance of the phrase 'blow your mind back to beta' in relation to the contest?,"Hey! You may recognize me for my acting work, but my real passion is programming. When I'm in the zone, I'm unstoppable. I don't know if you know this about me, but I'm really excited about the future. And Google's Gemini models are a tech that can get it there. And you know I'm submitting to Google's new Gemini Developer Contest, which is so cool, it'll blow your mind back to beta. A chance to win for the most creative, useful and remarkable apps. If you're a dev or a dev team that wants to submit great, but you'll have some catching up to do. I just drank three cups of hydrochol brew, and I'm ready to crank out some serious. Time for the wormhole!
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The phrase ""blow your mind back to beta"" in relation to the contest signifies that the contest is expected to showcase innovative and groundbreaking ideas that are reminiscent of the excitement and creativity often associated with the early stages of technology development, particularly during the beta phase of software. It suggests that the submissions to the contest could be so impressive and transformative that they evoke a sense of nostalgia for the early, experimental days of tech innovation, where new ideas were rapidly tested and developed. This phrase emphasizes the potential for creativity and remarkable applications that participants can bring to the contest, aligning with the overall theme of pushing boundaries and exploring new possibilities in technology."
What does the speaker imply about their readiness to participate in the contest?,"Hey! You may recognize me for my acting work, but my real passion is programming. When I'm in the zone, I'm unstoppable. I don't know if you know this about me, but I'm really excited about the future. And Google's Gemini models are a tech that can get it there. And you know I'm submitting to Google's new Gemini Developer Contest, which is so cool, it'll blow your mind back to beta. A chance to win for the most creative, useful and remarkable apps. If you're a dev or a dev team that wants to submit great, but you'll have some catching up to do. I just drank three cups of hydrochol brew, and I'm ready to crank out some serious. Time for the wormhole!
Sabak Give me the hell out of here. Get me the hell out of this cup. Get me out. Yeah! Google! Google! Google! Google! Google! Ha ha ha! We all ready to do a little googling. Everyone get out your computers. It's time to have ourselves a nice Google. Siri Navigate to google.com. Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Are you awake? Have you had your coffee? Are you awake? Don't worry baby. Google's gonna wake you right up. Don't worry baby. Google! Google! Google! Google's gonna wake you right up. Bleep bloop, bloop, bloop, rappy bloop, bloop, bloop. Up time to get up. You silly little nerd's wake up. okay Google.com. What is it? What does it mean? Why are we here? No one knows. And you're not, you're not going to find out. Not today. Those answers? Not part of the presentation. Buckle up. I'm Mark Rabier. It's nice to meet you, nice to see you, nice to be here. In beautiful Honolulu Hawaii. Here for Google I.O. If you have no idea who I am, that's perfectly I would expect that. Not many people do, but I'm an improvisational musician. I make stuff up from scratch. And that's exactly what we're going to be doing this entire show. We have nothing planned. That includes the rest of the conference. None of this has been rehearsed. Sundar's going to come out, say some stuff. No, we don't know what's going to happen here today. They're making up these visuals right now. Someone's back there. So what I'm here to talk about is music. I'm here to talk to you about music. And in particular, a very exciting new technology that we've been developing here at Google.com. Music FX. DJ. That's right. No more do you need to haul around the case of records. No, the computer will take care of it. Music FX DJ is a generative artificial intelligence tool that will from scratch generate from nothing. A series of sounds as it interprets your prompts. I'll demonstrate that to you now. Utilizing technology at my fingertips. And utilizing the colossal power of Google's machine. Connecting to Google's machine now. Music. We'll start with a viola. How about that? Everyone loves a viola. And if you don't, I don't care. I don't care. You do now. Viola. No one wrote this. Google wrote this. Let's throw in something weird. Hip hop beat from the 808 drum machine. It's pretty tough actually. So we have the sort of melodic element of the viola still in there. We can pull that out. Chiptones. I think that's chiptones. Let's do it out. It's pretty tough. Pulling that out. The machine is in control now. And that's what it can do. And let me tell you something. Let me tell you something right now. That is just the beginning. You're going to be hearing that a lot at this keynote. That's just the beginning. Okay. Now you will choose the prompts and we'll leave it up to chance. As I said, everything here unscripted unplanned. I didn't know this was going to happen. What's this? I didn't even know you guys were going to be here. What are you guys doing here? Six props, bagpipes, Persian tar, jembe, dulcimer, flamenco guitar, Detroit techno. You know what those things are? You know what this is? You better. Shout them out. What do you want to hear? Flamenco guitar. Persian took bagpipes. No, no bagpipes. They said, no, get the bagpipes out of here. Okay. Persian tar, jembe, flamenco guitar. Jembe.","The speaker implies that they are very enthusiastic and prepared to participate in the contest. They express excitement about the future and the potential of Google's Gemini models, indicating a strong motivation to submit their work. The mention of having ""drank three cups of hydrochol brew"" suggests they are energized and ready to create, reinforcing their readiness to engage in the contest. Overall, the speaker conveys a sense of eagerness and determination to contribute creatively to the competition."
How does the speaker's background in acting relate to their passion for programming?,"I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!
Hey! You may recognize me for my acting work, but my real passion is programming. When I'm in the zone, I'm unstoppable. I don't know if you know this about me, but I'm really excited about the future. And Google's Gemini models are a tech that can get it there. And you know I'm submitting to Google's new Gemini Developer Contest, which is so cool, it'll blow your mind back to beta. A chance to win for the most creative, useful and remarkable apps. If you're a dev or a dev team that wants to submit great, but you'll have some catching up to do. I just drank three cups of hydrochol brew, and I'm ready to crank out some serious. Time for the wormhole!","The speaker, who is recognized for their acting work, expresses that their real passion lies in programming. They highlight that when they are in the zone with programming, they feel unstoppable, indicating a strong enthusiasm and commitment to the craft. This connection suggests that their background in acting may contribute to their creativity and performance skills, which can be advantageous in programming, especially in developing innovative and engaging applications. The speaker's excitement about the future and the potential of Google's Gemini models further emphasizes their desire to merge their creative talents from acting with their technical skills in programming."
What is the total number of Android devices and Chrome browsers mentioned in the keynote?,"You'll see these and work coming to hundreds of billions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, on-device foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel, later this year, we'll be expanding what's possible with our latest model Gemini Nano with multi-modality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed Talkback an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Caro, who uses Talkback will typically come across 90 unlabeled images per day. Thankfully, Talkback makes them accessible. And now we're taking that to the next level with the multi-modal capabilities of Gemini Nano. So when someone sends Caro a photo, she'll get a richer and clearer description of what's happening. Or let's say Caro is shopping online for an outfit. Now she can get a crystal clear description of the styling cut to find the perfect look. Running Gemini Nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to Talkback are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scandal continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a non-known caller right in the middle of my presentation. Hello. I am calling from the Safe Morphing Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make an authorized charges. Oh yeah, what kind of charges? I can't give you specifics over the phone, but to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. And look at this. My phone gives me a warning that this call might be a stab. Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone. So the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone, thanks to the on device model. So remember that pickleball example earlier? Gemini at Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools like Gemini Nano and Gemini in Android Studio. Also, stay tuned tomorrow for our upcoming Android 15 updates, which we can't wait to share with you. As we said at the outset, we're reimagining Android with Gemini at the core from your favorite apps to the OS itself. We're bringing the power of AI to every aspect of the smartphone experience. And with that, let me head over to Josh to share more on our use for developers. Thank you.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The provided context does not specify the total number of Android devices or Chrome browsers mentioned in the keynote. It discusses features and improvements related to Android and AI but does not provide specific numerical data regarding the number of devices or browsers. Therefore, the answer is that the total number is not mentioned in the context."
What new feature was announced for Gemini 1.5 Flash regarding its availability for developers?,"Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response. And it's OK if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to aisstudio.google.com, log in with your Google account, and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit run. Now, in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, it starts streaming it back. Now, this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from Notebook LM, like we showed earlier. Now, what's great about this is that you can take something like this in AI Studio, prototype tier in 10 seconds. And with one click in the upper left, get an API key, or over here in the upper right, just tap get code. And you've got all of the model configurations, the safety settings, ready to go, straight into your IDE. Now, over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way, you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. OK. OK. APPLAUSE Now, let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now, since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. APPLAUSE It's optimized for a range of image captioning, visual Q&A, and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model, but it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on NextGen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. APPLAUSE So wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look.","A new feature announced for Gemini 1.5 Flash is the ability for developers to use it to learn from customer feedback about their products, thanks to its low latency. Developers can input feedback from customer forums into Flash, load a prompt, and run it to quickly process and stream back relevant information. This feature allows for efficient extraction of themes and insights from large amounts of data, enhancing the development process."
What are the benefits of using Gemini models in Android Studio as highlighted in the keynote?,"It's important for us to be available on any formfactor so that people can listen to SoundCloud in any situation. Our team's mobile Compost guilds transfer directly to other formfactors, given that its concepts and most APIs are the same. We build our various and TV apps from the ground up in Compost in just 4 months, which would have been unimaginable with the old D system. Some parts of our foldable and tablet layouts and the whole login experience of our car are built in Compost. Being available across formfactors has allowed us to reach new people and markets, which ultimately increases our global reach. It is so exciting to see how millions of developers create amazing experiences with Android APIs and Android Studio that ultimately touch billions of people and their devices. We care about incorporating AI to accelerate your productivity on Android to make things easier for you. Last year we introduced StudioBunt as your AI coding companion for Android. Thanks to your feedback, we evolved our models, expanded to over 200 countries and territories, released at the stable and brought it into the Gemini family of products last month. We introduced the introduction of Gemini and Android Studio. Today, we will show you how the latest Gemini 1.5 Pro models can take your development to the next level. Our vision is to responsibly use AI so you can spend more time on aspects of development that you like and make the harder tasks easier. For example, the crash-litx integration in Android Studio is even more helpful than ever before. Because on top of showing crashes and errors and the context of our source code, we now use Gemini to analyze and provide recommendations on how to fix those issues. Now, as you venture into more AI features, you ultimately have control of what you share or don't share with AI privacy controls built right into Android Studio. So, speaking of Android Studio, let's jump into the IDE to see a few more examples of Gemini and Action. So, here I am in the Jet Snack app, one of our open source composed samples. Just shopping up that cool, shared element in the integration that Maroo shared earlier with us. One of the cool things that Dubai enables is that it makes it easier to optimize my code. So, for example, here I have a conditional statement that can be a bit more efficient and idiomatic in Kotlin. To fix this, I can actually use Gemini to optimize this code so that it's easier to understand and maintain. So, all I have to do is right click on my selected code, navigate to Gemini, click on Transform, and let me simply input, simplify and make the idiomatic with this quick action. And press Enter. And what's happening is that I'm seeing the code snippet to the model and getting a result back. So, here I have a more optimized code submit going from an if-else structure to more efficient wind structure in Kotlin. Now, what is also great is that I can do additional modifications to this result. So, for instance, it's good practice to add documentation, right? So, let me use Gemini to help with that. All I need to do is click on the Refine button. Let's say, add some documentation and press Enter. Consider it's a model. And now I have augmented... The code changed with the help of Gemini. So, let me accept these changes. Now, I have optimized the code for readability and even added some code documentation, all of the help of Gemini. Nice. Now, Gemini can do pretty cool things on code itself, but it also does some amazing things that are perfect for Android app development. So, for example, we have the screen here that needs a little bit of translation. So, looking at my strings resource file, my colleague left me out to do the Transite to Spanish, but I don't know Spanish that well, but Gemini does. So, let me highlight a few strings here. And I can navigate right-click and navigate the Gemini. Click on Transform. In this case, let me type in Transite to Spanish. Let me see this question and strings to the model. I get a response back. It looks pretty good. And note, Gemini kept my app name in English. Nice. So, let me do one more section for completeness. Again, I hide the strings. Right-click, go to Gemini, click on Transform. Let me translate that to Spanish for this quick action. Results look pretty good. Let me accept those.
A chance to win for the most creative, useful, and remarkable apps. If you're a dev or a dev team that wants to submit, great! But you'll have some catching up to do. I just drank three cups of hydro cold brew and I'm ready to crank out some serious times of the world poll. Now we'll look at examples of how AI unlocks excellent experiences and how we can get the best results. Jacqueline talked about how Gemini models benefit all developers. Now we'll look at examples of how AI unlocks excellent experiences on Android, the world's most popular operating system. We're reimagining Android with AI at the core, enabling you to build a whole new class of apps on mobile. This unlocks experiences that were not even possible just a few years ago. So let's dive into the ways that Gemini can power building the next generation of apps on Android. Using Google AI Studio, you can integrate the Gemini API directly into your app to leverage Gemini models. When using Google Cloud, you can also access the powerful capabilities of Gemini through Vertex AI. This is Google's fully managed development platform designed for building and deploying generative AI. Whether you simply need world-class inference capabilities or want to build end-to-end AI workflows with Vertex, the Gemini API is a great solution. Next, let's look at Gemini Nano, our most efficient model for on-device tasks. It runs directly on mobile, enabling low latency responses and data privacy. This helps you implement features like suggested replies and messaging apps while knowing that the data won't leave the device. Gemini Nano also guarantees AI model availability even when there's no cellular network coverage. Gemini Nano, inference, is made possible by AI Core, a system service managing on-device foundation models. It removes the developer burden of packaging and distributing large language models. Gemini Nano and AI Core are currently available on the Pixel 8 Pro in Samsung Galaxy S24 series and, as you heard earlier, will be expanding their reach to additional devices later this year. Gemini Nano gives you the best path to running a generative AI model on device. And yet, we know that being able to customize and work with open models is important too. So we also built MediaPipe LLAM to give you a complete solution. Last year, we announced an Early Access program for Gemini Nano and we got an overwhelming amount of responses. We're now working closely with a few early adopters to ensure we're creating the best APIs that unlock the most innovative experiences. We're now on the path to opening access even wider in the coming months. One of these trailblazers is Patreon. They're building an experience that helps creators and fans rapidly catch up on unread messages in their community chats. And another example is Grammarly. Their team has been able to easily experiment with Gemini Nano while building their smart suggestions feature. And yet, these amazing examples are just the beginning. We're excited to see what you'll build with Gemini. Next up, Mara will share a bit about developer productivity and how Android is making it faster and easier to build excellent experiences across form factors. We know how much you all love Kotlin. It's been so fun to see the growth and energy in the community. Today, I'm excited to announce that we're taking a significant step forward with towards the future with first class tooling and library support for Kotlin Multiplotform on Android. We've been adding Kotlin Multiplotform support to Jetpack libraries like Datastore and now we're expanding it to your favorite libraries like Room. At Google, we've been migrating the Google Docs app to use Kotlin Multiplotform to share business logic across Android, iOS and Web. The workspace team is excited to continue to invest in using Kotlin Multiplotform across the rest of their apps in the future. We've been learning a lot from our work with Google Apps and we're collaborating with Jetbrings and the Kotlin Developer community to bring the benefits of KMP to all of you. Kotlin Multiplotform will boost your productivity by sharing business logic across platforms. On the UI layer, Compost helps you build excellent experiences and today 40% of the top 1,000 apps like Google Drive, Threads and SoundCloud are taking advantage of the benefits Compost brings. We're continuing to invest in Compost with more performance improvements and easier ways to build beautiful UI across form factors. Compost continues to be faster with each release and combined with R8 and baseline profiles can now significantly boost your apps performance. For an even more delightful user experience, you can now use shared element transitions with Compost.","The benefits of using Gemini models in Android Studio, as highlighted in the keynote, include:

1. **Enhanced Productivity**: Gemini acts as an AI coding companion that helps developers accelerate their productivity by simplifying coding tasks.

2. **Code Optimization**: Gemini can analyze code and provide recommendations for optimization, making it easier to understand and maintain. For example, it can transform conditional statements into more efficient structures.

3. **Documentation Assistance**: Gemini can help developers add documentation to their code, promoting good coding practices.

4. **Translation Capabilities**: Gemini can assist in translating strings within the app, which is particularly useful for localization efforts.

5. **Privacy Controls**: Developers have control over what information they share with AI, thanks to built-in privacy controls in Android Studio.

6. **Integration with Google AI Studio**: Developers can integrate the Gemini API directly into their apps, leveraging its capabilities for various tasks.

7. **On-Device Efficiency**: With Gemini Nano, developers can run AI models directly on mobile devices, ensuring low latency responses and data privacy.

8. **Support for Kotlin Multiplatform**: Gemini supports Kotlin Multiplatform, allowing developers to share business logic across Android, iOS, and web applications, thereby boosting productivity.

Overall, these features enable developers to create more efficient, user-friendly, and innovative applications on the Android platform."
What is the purpose of the new context-caching feature announced for Gemini models?,"As context windows grow, we're discovering new ways to interact with models to solve tasks. Huge context windows unlock a new set of AI applications, but they're also more computationally expensive, which is why I'm excited to announce the new context-caching feature coming next month. So how does it work? Well, let's say a large part of your prompt doesn't change. You can cache that once and easily call it unsubssequent turns for a fraction of the price. So imagine a major analyst trying to figure out what to write about next. To help me brainstorm ideas, I might want to gather all the content I've already created. Previous articles, some video clips, maybe some voice memos. Using context-caching, I'll send all this content at once, which let's say it will cost me about 20 cents for that first turn, along with the regular input price for the rest of my prompts. Since I cache that content that I'm referencing, each following turn will only cost me 10 cents, plus the cost of any new prompts. Of course, this concept is extensible to so many different applications that you might want to build. These could involve things like legal documents, research papers, school assignments, or employee training materials. The sky is the limit. Gemini models are empowering developers worldwide to transform workflows, enhancing efficiency, and also adding a touch of user delight. Take, for example, front-end development. Locify.ai takes a Figma design and uses their own AI models to generate code. But what's really unique is how they're pairing with Gemini's 1 million token context window. You see, oftentimes, front-end designs contain hundreds of layers with nondescript names such as rectangle one or group 76. Locify sends this entire design to 1.5 pro to generate meaningful names in the front-end code. Because Gemini understands the purpose behind rectangle one, it's actually able to rename it in code as something more useful like header background. Another one that I love is how AI models are unlocking new abilities. Invision is powering people with low vision to better understand their immediate environment through an app or smart glasses and ask specific questions. Because of the speed of 1.5 flash, users will be able to get quick responses. Another developer, Zapier, is enabling anybody to seamlessly connect different apps and automate repetitive tasks like podcast editing. They built a workflow to automatically remove speaker artifacts like OZ and OMS from audio tracks that's triggered as soon as a user uploads a file to Google Drive. As developers, the ideas you have and the code you write will help transform AI into practical applications for the world. We're announcing our Gemini API developer competition for a chance to win the grand prize, a custom electric de lorean. Woo! Woo! Woo! Woo! Woo! Woo! For those of you in the audience, be sure to check it out at the AI Pavilion, maybe grab a picture, and visit our website for more details. And now I'm going to leave you with my favorite developer to tell you a little bit more about it. Hey! You may recognize me for my acting work, but my real passion is programming. When I'm in the zone, I'm unstoppable. I don't know if you know this about me, but I'm really excited about the future and Google's Gemini models are tech that can get it there. And you know I'm going to be a great developer that you know I'm submitting to Google's new Gemini developer contest, which is so cool, it'll blow your mind back to beta. A chance to win for the most creative, useful, and remarkable apps. If you're a dev or a dev team that wants to submit, great! But you'll have some catching up to do. I just drank three cups of hydro cold brew and I'm ready to crank out some serious times of the world poll. Now we'll look at examples of how AI unlocks excellent experiences and how we can get the best results. Jacqueline talked about how Gemini models benefit all developers. Now we'll look at examples of how AI unlocks excellent experiences on Android, the world's most popular operating system. We're reimagining Android with AI at the core, enabling you to build a whole new class of apps on mobile. This unlocks experiences that were not even possible just a few years ago. So let's dive into the ways that Gemini can power building the next generation of apps on Android.
Now, meeting many of you at Hackathons and Meetups, I am blown away by how you're pushing the boundaries with our new AI models. From new ideas using audio and video inputs, the creative use of longer context windows. It's really inspiring to see. And like you heard about earlier today, balance across quality, cost, and speed. That's why I'm thrilled that Gemini 1.5 Flash is officially open to all developers to start building with today. You can get started using the Gemini API in Google AI Studio, which is now available in more than 200 countries and territories. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. But what I'm seeing more and more these days is that it isn't just about selecting the right model, but also the right techniques to bring your ideas to life. Let's say you want to personalize the responses you get. Now, one way to do that is by tuning the model. In fact, Google AI Studio lets you do that in minutes. And now, with our 2 million token context window, you have another compelling option. Let me show you how in Google AI Studio. So here I'm already signed in. We've made it easier for you to start prompting right away. Or if you want to get straight to code, you can quickly grab an API key here to use with one of our many SDKs. If you're looking for some inspiration, we also have a Gemini API cookbook, which is a great resource. But today, I actually want to take you through one of my own personal workflows that has unlocked a new way of getting things done. Now, if you're anything like me, ideas always seem to strike when I'm away from the computer. So now, when an idea comes to mind, I quickly grab my phone and I capture a voice memo to myself. This could be at the park with my daughters or while I'm out walking. So let's actually jump into a saved prompt that I have right now. This is what I use to help you write new blog posts. As you can see, I've already added some system instructions, which tell the model it's an experienced writer. I've also added a few of my previous blog posts so that the output is going to better mirror my tone, style, and word choice. Next, I'm going to add a bunch of voice memos of some half-big thoughts I've had recently for an upcoming post. So we'll click in here, go into my drive, and add these two voice memos. Now, while those are loading, I'm actually going to play you a clip from one of those just so you have an idea of exactly what I've sent into Gemini. OK, so recently, I made the connection about two pretty prominent moments in my career. The first. OK, so as you can clearly hear, I was at the park with my daughters pushing them on the swings. Now that these have loaded, I'm going to add a simple prompt. Turn these into a blog post. I'll hit run. And what you're about to see is Gemini 1.5 Flash take all of that content. My previous blog post, the system instructions, my voice memos, along with that prompt, and turn it into what will hopefully be a compelling first draft of my next blog post. From throwaway work to breakthrough ideas or to breakthroughs, embracing the power of right to think. Not bad for a working title. I think I can probably take it from there. As context windows grow, we're discovering new ways to interact with models to solve tasks. Huge context windows unlock a new set of AI applications, but they're also more computationally expensive, which is why I'm excited to announce the new context-caching feature coming next month. So how does it work? Well, let's say a large part of your prompt doesn't change. You can cache that once and easily call it unsubssequent turns for a fraction of the price. So imagine a major analyst trying to figure out what to write about next. To help me brainstorm ideas, I might want to gather all the content I've already created. Previous articles, some video clips, maybe some voice memos. Using context-caching, I'll send all this content at once, which let's say it will cost me about 20 cents for that first turn, along with the regular input price for the rest of my prompts.","The new context-caching feature announced for Gemini models is designed to optimize the use of large context windows by allowing users to cache a significant portion of their prompt that doesn't change. This means that once the initial content is sent, it can be reused in subsequent interactions for a reduced cost. For example, if a user sends a large amount of content (like previous articles, video clips, or voice memos) in the first turn, they would pay a higher price for that initial input. However, for the following turns, they would only incur a lower cost, as the cached content can be referenced without needing to resend it. This feature aims to enhance efficiency and reduce computational expenses when interacting with the models."
What significant updates were made to Kotlin Multiplatform as discussed in the keynote?,"The workspace team is excited to continue to invest in using Kotlin Multiplotform across the rest of their apps in the future. We've been learning a lot from our work with Google Apps and we're collaborating with Jetbrings and the Kotlin Developer community to bring the benefits of KMP to all of you. Kotlin Multiplotform will boost your productivity by sharing business logic across platforms. On the UI layer, Compost helps you build excellent experiences and today 40% of the top 1,000 apps like Google Drive, Threads and SoundCloud are taking advantage of the benefits Compost brings. We're continuing to invest in Compost with more performance improvements and easier ways to build beautiful UI across form factors. Compost continues to be faster with each release and combined with R8 and baseline profiles can now significantly boost your apps performance. For an even more delightful user experience, you can now use shared element transitions with Compost. We've built a simple and powerful system that helps you choreograph beautiful transitions across screens that share content. It handles size changes and animates text changes and it works great with predictive apps right out of the box. Developers have seen significantly increased engagement on apps that look and feel great across the diverse range of Android devices. Rather than thinking about individual screen sizes, we want you to think adaptive. So write once and adapt across screen sizes and configurations. To make this journey easier, we've built a new set of Compost APIs for building adaptive layouts using material guidance. Early adopters are already integrating this layout into their Compost projects, significantly reducing development time. We've also been making improvements to stylus, mouse and keyboard support for Compost to make it easier to adapt to the input device in your user's hand. Take advantage of the new stylus handwriting recognition, which using the power of AI transforms handwriting into text when using views or Compost. Widgets are another high user engagement surface with Jetpack Lamps, you can now build widgets that seamlessly adapt to all screen sizes. And Android 15 brings support for generated previews to improve user discoverability in the widget picker. Next is testing. We're working with Samsung and Pixel and other OEMs to connect their device labs into the new Android device streaming powered by Firebase now in beta. So you can now validate your adaptive app on a wider array of real devices, including the latest foldable and tablets. We're also continuing to invest in the resizable emulator, which helps you quickly test your app across different device types, as well as building a new Compost UI check mode, which helps you identify and provide solutions to adaptive and accessibility issues. Let's hear from SoundCloud about how Compost accelerated their UI development to create excellent user experiences across devices. Then Jamal will join us to share more about how AI can supercharge your productivity. SoundCloud is an ex-generation music entertainment company powered by an ecosystem of artists, fans and curators. My name is Svita Sautna. I'm a super passionate about music and I'm doing Android development at SoundCloud where I'm leading the adoption of Jetpack Compost. We started using Jetpack Compost when we were still in early preview. Our goal was to overcome the robustity that's associated with manipulating XMLVs. We were able to save up to 45% of lines of code when migrating screens from XMLVs to Compost. We quickly realized how easy and natural it was to create reusable UI components. Now we've migrated around 50% of our design system to Jetpack Compost. We're investing heavily in writing our base filling box and Compost so that our engineers can write new features with it and make it their first choice. It's important for us to be available on any formfactor so that people can listen to SoundCloud in any situation. Our team's mobile Compost guilds transfer directly to other formfactors, given that its concepts and most APIs are the same. We build our various and TV apps from the ground up in Compost in just 4 months, which would have been unimaginable with the old D system. Some parts of our foldable and tablet layouts and the whole login experience of our car are built in Compost. Being available across formfactors has allowed us to reach new people and markets, which ultimately increases our global reach. It is so exciting to see how millions of developers create amazing experiences with Android APIs and Android Studio that ultimately touch billions of people and their devices. We care about incorporating AI to accelerate your productivity on Android to make things easier for you. Last year we introduced StudioBunt as your AI coding companion for Android.
A chance to win for the most creative, useful, and remarkable apps. If you're a dev or a dev team that wants to submit, great! But you'll have some catching up to do. I just drank three cups of hydro cold brew and I'm ready to crank out some serious times of the world poll. Now we'll look at examples of how AI unlocks excellent experiences and how we can get the best results. Jacqueline talked about how Gemini models benefit all developers. Now we'll look at examples of how AI unlocks excellent experiences on Android, the world's most popular operating system. We're reimagining Android with AI at the core, enabling you to build a whole new class of apps on mobile. This unlocks experiences that were not even possible just a few years ago. So let's dive into the ways that Gemini can power building the next generation of apps on Android. Using Google AI Studio, you can integrate the Gemini API directly into your app to leverage Gemini models. When using Google Cloud, you can also access the powerful capabilities of Gemini through Vertex AI. This is Google's fully managed development platform designed for building and deploying generative AI. Whether you simply need world-class inference capabilities or want to build end-to-end AI workflows with Vertex, the Gemini API is a great solution. Next, let's look at Gemini Nano, our most efficient model for on-device tasks. It runs directly on mobile, enabling low latency responses and data privacy. This helps you implement features like suggested replies and messaging apps while knowing that the data won't leave the device. Gemini Nano also guarantees AI model availability even when there's no cellular network coverage. Gemini Nano, inference, is made possible by AI Core, a system service managing on-device foundation models. It removes the developer burden of packaging and distributing large language models. Gemini Nano and AI Core are currently available on the Pixel 8 Pro in Samsung Galaxy S24 series and, as you heard earlier, will be expanding their reach to additional devices later this year. Gemini Nano gives you the best path to running a generative AI model on device. And yet, we know that being able to customize and work with open models is important too. So we also built MediaPipe LLAM to give you a complete solution. Last year, we announced an Early Access program for Gemini Nano and we got an overwhelming amount of responses. We're now working closely with a few early adopters to ensure we're creating the best APIs that unlock the most innovative experiences. We're now on the path to opening access even wider in the coming months. One of these trailblazers is Patreon. They're building an experience that helps creators and fans rapidly catch up on unread messages in their community chats. And another example is Grammarly. Their team has been able to easily experiment with Gemini Nano while building their smart suggestions feature. And yet, these amazing examples are just the beginning. We're excited to see what you'll build with Gemini. Next up, Mara will share a bit about developer productivity and how Android is making it faster and easier to build excellent experiences across form factors. We know how much you all love Kotlin. It's been so fun to see the growth and energy in the community. Today, I'm excited to announce that we're taking a significant step forward with towards the future with first class tooling and library support for Kotlin Multiplotform on Android. We've been adding Kotlin Multiplotform support to Jetpack libraries like Datastore and now we're expanding it to your favorite libraries like Room. At Google, we've been migrating the Google Docs app to use Kotlin Multiplotform to share business logic across Android, iOS and Web. The workspace team is excited to continue to invest in using Kotlin Multiplotform across the rest of their apps in the future. We've been learning a lot from our work with Google Apps and we're collaborating with Jetbrings and the Kotlin Developer community to bring the benefits of KMP to all of you. Kotlin Multiplotform will boost your productivity by sharing business logic across platforms. On the UI layer, Compost helps you build excellent experiences and today 40% of the top 1,000 apps like Google Drive, Threads and SoundCloud are taking advantage of the benefits Compost brings. We're continuing to invest in Compost with more performance improvements and easier ways to build beautiful UI across form factors. Compost continues to be faster with each release and combined with R8 and baseline profiles can now significantly boost your apps performance. For an even more delightful user experience, you can now use shared element transitions with Compost.","The significant updates made to Kotlin Multiplatform, as discussed in the keynote, include:

1. **First-Class Tooling and Library Support**: There is a significant step forward in providing first-class tooling and library support for Kotlin Multiplatform on Android.

2. **Integration with Jetpack Libraries**: Kotlin Multiplatform support is being added to Jetpack libraries, including Datastore, and is expanding to other popular libraries like Room.

3. **Migration of Google Apps**: The Google Docs app has been migrated to use Kotlin Multiplatform to share business logic across Android, iOS, and Web platforms.

4. **Collaboration with the Kotlin Developer Community**: The workspace team is collaborating with JetBrains and the Kotlin Developer community to bring the benefits of Kotlin Multiplatform to developers.

5. **Productivity Boost**: Kotlin Multiplatform is expected to boost productivity by allowing developers to share business logic across different platforms.

These updates indicate a strong commitment to enhancing Kotlin Multiplatform's capabilities and its integration into the broader Android development ecosystem."
What significant advancements were introduced with the Gemini 1.5 Pro model?,"Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.","The Gemini 1.5 Pro model introduced several significant advancements:

1. **Quality Improvements**: The model received a series of quality enhancements across various tasks, including translation, coding, and reasoning.

2. **Multimodal Capabilities**: Gemini 1.5 Pro is natively multimodal, allowing users to interleave text, images, audio, and video as inputs. This capability enhances the model's versatility in handling different types of data.

3. **Expanded Context Window**: The context window was expanded to 2 million tokens, which opens up new possibilities for processing larger amounts of information and providing more comprehensive responses.

4. **Developer Features**: New features were added for developers, including:
   - **Video Frame Extraction**: This feature allows developers to extract frames from videos for analysis.
   - **Parallel Function Calling**: This enables the model to return multiple function calls simultaneously.
   - **Context Caching**: This feature allows users to send all files to the model once, reducing the need to resend them repeatedly.

5. **Accessibility**: The model is available globally for developers and can be accessed through platforms like AI Studio and Vertex AI, making it easier for users to integrate and utilize its capabilities.

6. **Pricing Structure**: The pricing for using the model was made more affordable, with specific rates for different token usages, such as $7 per 1 million tokens and reduced rates for larger prompts.

These advancements collectively enhance the functionality and usability of the Gemini 1.5 Pro model for developers and users alike."
How many developers are currently using Gemini models across Google's tools?,"Welcome to the 16th Google I.O. I want to thank our fans, have developed our community all millions of you around the world for choosing to build with Google. Literally, whatever you can imagine, you can bring it to life because only Google's ecosystem gives you the potential to reach people on 3 billion Android devices and 2 billion Chrome and Chromium-based browsers. You've created millions of helpful apps with Firebase, Google Cloud, and generative AI models like Gemini and Gemma. At Google, we're on a mission to make generative AI accessible to every developer on the planet. It's transforming the very fundamentals of software development, including new techniques for how you build and new inspiration for what you build. AI helps you to be more productive by assisting you with many different development tasks, such as writing, debugging, and testing your code, or generating documentation, and understanding your code base. That's why Gemini is available to you, all developers to use in Android Studio, Chrome Dev Tools, Project IDX, CoLab, Beath Code, IntelliJ, and Firebase. CHEERING AND APPLAUSE Gemini is even more helpful with context such as app settings, performance data, your logs, and source code. This is just a glimpse of AI changing how we built. Now, let's go to what you built. Today, people just assumed that things will work great. Of course, any platform, any device that they use, literally anywhere they are, whether on the go, at home, or at work, in their local town, or on the other side of the world. At the same time, the tools that you use need to make that possible and they're constantly evolving. It's great to see the latest technologies come out, but that means more complexity, especially with so many different AI concepts to learn. We want to make your lives simpler. As code becomes content and coders become creators, whether you're building an AI-powered mobile app, a web app, or an interactive experience that works across all platforms, we're providing you with powerful, more integrated development tools, both from Google and the OpenEka system, and it comes with secure, easy access to Google's most capable AI models and global cloud infrastructure. Ultimately, it's about helping you to stay focused on what you want to build, amazing products and services, and not the plumbing, which takes me to our agenda for today. Up next, you'll hear more in how you can use the Gemini API in Google AI Studio to develop engaging and multimodal apps. Then we will take you through how Gemini helps you to create a whole new category of AI-powered experiences on Android and Web, with exceptional levels of productivity. We've also been hard at work creating the very best experienced, full-stack, multi-platform development. Our team will share what's now possible with Project IDX, Flutter, and Firebase. And we think that will dramatically expand what you can build. Then we will talk about how you can build and customize your own AI models with Gemma. You'll see how CoLab, Carrison Jacks, make it really easy. Finally, I'll be back up here to share one more thing before we wrap it up. Are you ready to jump in? OK. Awesome. Jacqueline, come join us. Our investment in our AI research and infrastructure allows us to put the power of AI directly into your hands. We make it easy to start building an AI app with a simple API and a tool that can be used to create a new product. It can be used to create an AI app with a simple API integration. So you can focus on building the best possible products for your users. Now, meeting many of you at Hackathons and Meetups, I am blown away by how you're pushing the boundaries with our new AI models. From new ideas using audio and video inputs, the creative use of longer context windows. It's really inspiring to see. And like you heard about earlier today, balance across quality, cost, and speed. That's why I'm thrilled that Gemini 1.5 Flash is officially open to all developers to start building with today. You can get started using the Gemini API in Google AI Studio, which is now available in more than 200 countries and territories. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. But what I'm seeing more and more these days is that it isn't just about selecting the right model, but also the right techniques to bring your ideas to life.
Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in monks than is happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. So, we are in the very early days of the AI platform shift. We see so much opportunity ahead for creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too, including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries, as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. Applause There's so much innovation happening in search. Thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.",More than 1.5 million developers are currently using Gemini models across Google's tools.
What new features are being rolled out in Google Photos with the integration of Gemini?,"Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in monks than is happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. So, we are in the very early days of the AI platform shift. We see so much opportunity ahead for creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too, including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries, as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. Applause There's so much innovation happening in search. Thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.
Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Oops! Reacts. Things got bigger. Like way bigger. And it wasn't all just for him, or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. Still, we are in the very early days of the AI platform shift. We see so much opportunity ahead. For creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too. Including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. The so much innovation happening in search, thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.","The integration of Gemini into Google Photos is expected to bring new and powerful search experiences. While specific features are not detailed in the provided text, it mentions that Gemini's capabilities will enhance how users interact with Google Photos, likely allowing for more advanced search functionalities and improved user satisfaction. This could include features such as searching with photos and generating more relevant results based on complex queries. The overall goal is to leverage Gemini's advancements to create a more intuitive and effective user experience within Google Photos."
What is the purpose of the Google Developer Program launched at the event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The purpose of the Google Developer Program launched at the event is to provide developers with updates and new tools to help them build the best applications across various services. This includes leveraging advancements in AI, such as the Gemini 1.5 Pro, to improve and accelerate development processes. The program also features hands-on workshops, breakout sessions, and competitions to engage developers and enhance their skills in using Google's technologies."
What is the expected impact of the new AI overviews feature in Google Search?,"Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago we created search to help people make sense of the waves of information moving online. With each platform shift we have delivered breakthroughs to help answer your questions better. On mobile we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities are high bar for information quality, our decades of experience connecting you to the richness of the web. The result is a product that does the work for you. Google search is generative AI at the scale of human curiosity. And it's our most exciting chapter of search yet. To tell you more here's Liz. Thanks Sundar. With each of these platform shifts we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI search will do more for you than you ever imagined. So whatever is on your mind and whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real time information with over a trillion facts about people, places and things. Second, our in parallel ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly. Complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub questions and get an AI overview in seconds. To make this possible, we're introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates studios in Boston and show you details on their inter offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there.
To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly, complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today, with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. You make this possible for introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates to use in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you've got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there. So since you're asking about places, it taps into Google's index of information about the real world with over 250 million places and updated in real time, including their ratings, reviews, business hours, and more. Research that might have taken you minutes or even hours Google can now do on your behalf in just seconds. Next, let me show you another way multi-step reasoning in Google search can make your life that much easier. Take planning, for example. Planning of trips and meal plans can be fun. But doing the work of actually figuring it all out, no thank you. With Gemini in search, Google does the planning with you. Planning is really hard for AI to get right. It's the type of problem that takes advance reasoning and logic. After all, if you're meal planning, you probably don't want mac and cheese for breakfast, lunch, and dinner. Okay, my tics might, but say you're looking for a bit more variety. Now, you can ask search to create a three-day meal plan for a group that's easy to prepare. And here you get a plan with a wide range of recipes from across the web. This one for overnight oats looks particularly interesting. And you can easily head over to the website to learn how to prepare them. If you want to get more veggies in, you can simply ask search to swap in a vegetarian dish. And just like that, search customizes your meal plan.","The expected impact of the new AI overviews feature in Google Search is significant. It aims to enhance the user experience by allowing users to receive instant answers to their questions, complete with a range of perspectives and links for deeper exploration. This feature is designed to simplify the process of gathering information, as users can ask complex questions with multiple sub-questions and receive comprehensive AI-generated overviews in seconds. 

Additionally, the AI overviews will roll out to over a billion users by the end of the year, making advanced search capabilities more accessible. The integration of multi-step reasoning will enable Google to break down larger questions into manageable parts, allowing it to conduct thorough research on behalf of the user. This means that tasks that previously took considerable time and effort, such as planning trips or meal plans, can now be accomplished quickly and efficiently through Google Search. Overall, the AI overviews are expected to dramatically expand the possibilities of what users can achieve with Google Search, making it a more powerful tool for information retrieval and decision-making."
What new technology related to music was introduced at Google I/O 2024?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
Sabak Give me the hell out of here. Get me the hell out of this cup. Get me out. Yeah! Google! Google! Google! Google! Google! Ha ha ha! We all ready to do a little googling. Everyone get out your computers. It's time to have ourselves a nice Google. Siri Navigate to google.com. Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Are you awake? Have you had your coffee? Are you awake? Don't worry baby. Google's gonna wake you right up. Don't worry baby. Google! Google! Google! Google's gonna wake you right up. Bleep bloop, bloop, bloop, rappy bloop, bloop, bloop. Up time to get up. You silly little nerd's wake up. okay Google.com. What is it? What does it mean? Why are we here? No one knows. And you're not, you're not going to find out. Not today. Those answers? Not part of the presentation. Buckle up. I'm Mark Rabier. It's nice to meet you, nice to see you, nice to be here. In beautiful Honolulu Hawaii. Here for Google I.O. If you have no idea who I am, that's perfectly I would expect that. Not many people do, but I'm an improvisational musician. I make stuff up from scratch. And that's exactly what we're going to be doing this entire show. We have nothing planned. That includes the rest of the conference. None of this has been rehearsed. Sundar's going to come out, say some stuff. No, we don't know what's going to happen here today. They're making up these visuals right now. Someone's back there. So what I'm here to talk about is music. I'm here to talk to you about music. And in particular, a very exciting new technology that we've been developing here at Google.com. Music FX. DJ. That's right. No more do you need to haul around the case of records. No, the computer will take care of it. Music FX DJ is a generative artificial intelligence tool that will from scratch generate from nothing. A series of sounds as it interprets your prompts. I'll demonstrate that to you now. Utilizing technology at my fingertips. And utilizing the colossal power of Google's machine. Connecting to Google's machine now. Music. We'll start with a viola. How about that? Everyone loves a viola. And if you don't, I don't care. I don't care. You do now. Viola. No one wrote this. Google wrote this. Let's throw in something weird. Hip hop beat from the 808 drum machine. It's pretty tough actually. So we have the sort of melodic element of the viola still in there. We can pull that out. Chiptones. I think that's chiptones. Let's do it out. It's pretty tough. Pulling that out. The machine is in control now. And that's what it can do. And let me tell you something. Let me tell you something right now. That is just the beginning. You're going to be hearing that a lot at this keynote. That's just the beginning. Okay. Now you will choose the prompts and we'll leave it up to chance. As I said, everything here unscripted unplanned. I didn't know this was going to happen. What's this? I didn't even know you guys were going to be here. What are you guys doing here? Six props, bagpipes, Persian tar, jembe, dulcimer, flamenco guitar, Detroit techno. You know what those things are? You know what this is? You better. Shout them out. What do you want to hear? Flamenco guitar. Persian took bagpipes. No, no bagpipes. They said, no, get the bagpipes out of here. Okay. Persian tar, jembe, flamenco guitar. Jembe.","The new technology related to music introduced at Google I/O 2024 is called ""Music FX DJ."" It is a generative artificial intelligence tool that can create a series of sounds from scratch based on user prompts. This technology allows users to generate music without needing to carry around physical records, as the computer takes care of the sound generation."
Who hosted the I-O Pre-Show and what unique feature did they demonstrate?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The I-O Pre-Show was hosted by musician Mark Rubier, who demonstrated the new DJ mode in Google's music FX tool to host an improvisational set."
What was the atmosphere described during the Google I/O 2024 event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The atmosphere during the Google I/O 2024 event was described as buzzing with excitement, with attendees making their way into the Shoreline Amphitheater. The event was characterized as a unique opportunity for the company to share its story, with a sense of enthusiasm surrounding the return to a developer festival atmosphere. There were about 4,000 developers in attendance, and the program included various engaging activities such as sandboxes, demos, and hands-on workshops. The excitement was palpable, especially during the keynote where significant announcements were made, and the audience was energized by the innovations and live demos presented throughout the event."
What instruments were mentioned as part of the improvisational music demonstration?,"That's just the beginning. Okay. Now you will choose the prompts and we'll leave it up to chance. As I said, everything here unscripted unplanned. I didn't know this was going to happen. What's this? I didn't even know you guys were going to be here. What are you guys doing here? Six props, bagpipes, Persian tar, jembe, dulcimer, flamenco guitar, Detroit techno. You know what those things are? You know what this is? You better. Shout them out. What do you want to hear? Flamenco guitar. Persian took bagpipes. No, no bagpipes. They said, no, get the bagpipes out of here. Okay. Persian tar, jembe, flamenco guitar. Jembe. What I'm going to do is pop these in here and then I'm going to use them as samples and I'm going to make something with this. Ideally, I want it to be simple so that I can use it as an element in like a fuller composition. Do you understand what I'm saying? It's okay if you don't. I don't. Jembe. Flamenco guitar. Go ahead, baby. Give it a little more. Persian tar. Hey. I've now muted. reacts, deltarict. Monoyo. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. Shaburam. Shaburam. Shaburam. Go go go go go go go go go. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam.
Sabak Give me the hell out of here. Get me the hell out of this cup. Get me out. Yeah! Google! Google! Google! Google! Google! Ha ha ha! We all ready to do a little googling. Everyone get out your computers. It's time to have ourselves a nice Google. Siri Navigate to google.com. Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Google! Are you awake? Have you had your coffee? Are you awake? Don't worry baby. Google's gonna wake you right up. Don't worry baby. Google! Google! Google! Google's gonna wake you right up. Bleep bloop, bloop, bloop, rappy bloop, bloop, bloop. Up time to get up. You silly little nerd's wake up. okay Google.com. What is it? What does it mean? Why are we here? No one knows. And you're not, you're not going to find out. Not today. Those answers? Not part of the presentation. Buckle up. I'm Mark Rabier. It's nice to meet you, nice to see you, nice to be here. In beautiful Honolulu Hawaii. Here for Google I.O. If you have no idea who I am, that's perfectly I would expect that. Not many people do, but I'm an improvisational musician. I make stuff up from scratch. And that's exactly what we're going to be doing this entire show. We have nothing planned. That includes the rest of the conference. None of this has been rehearsed. Sundar's going to come out, say some stuff. No, we don't know what's going to happen here today. They're making up these visuals right now. Someone's back there. So what I'm here to talk about is music. I'm here to talk to you about music. And in particular, a very exciting new technology that we've been developing here at Google.com. Music FX. DJ. That's right. No more do you need to haul around the case of records. No, the computer will take care of it. Music FX DJ is a generative artificial intelligence tool that will from scratch generate from nothing. A series of sounds as it interprets your prompts. I'll demonstrate that to you now. Utilizing technology at my fingertips. And utilizing the colossal power of Google's machine. Connecting to Google's machine now. Music. We'll start with a viola. How about that? Everyone loves a viola. And if you don't, I don't care. I don't care. You do now. Viola. No one wrote this. Google wrote this. Let's throw in something weird. Hip hop beat from the 808 drum machine. It's pretty tough actually. So we have the sort of melodic element of the viola still in there. We can pull that out. Chiptones. I think that's chiptones. Let's do it out. It's pretty tough. Pulling that out. The machine is in control now. And that's what it can do. And let me tell you something. Let me tell you something right now. That is just the beginning. You're going to be hearing that a lot at this keynote. That's just the beginning. Okay. Now you will choose the prompts and we'll leave it up to chance. As I said, everything here unscripted unplanned. I didn't know this was going to happen. What's this? I didn't even know you guys were going to be here. What are you guys doing here? Six props, bagpipes, Persian tar, jembe, dulcimer, flamenco guitar, Detroit techno. You know what those things are? You know what this is? You better. Shout them out. What do you want to hear? Flamenco guitar. Persian took bagpipes. No, no bagpipes. They said, no, get the bagpipes out of here. Okay. Persian tar, jembe, flamenco guitar. Jembe.","The instruments mentioned as part of the improvisational music demonstration are:

1. Bagpipes
2. Persian tar
3. Jembe
4. Dulcimer
5. Flamenco guitar
6. Detroit techno"
What was the overall theme of Mark Rubier's presentation at Google I/O 2024?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The overall theme of Mark Rubier's presentation at Google I/O 2024 was centered around the use of the new DJ mode in Google's music FX tool, which allowed him to host an improvisational set. The presentation highlighted the technology's capability to summon and mix various sounds, showcasing its potential as a companion for creating unique and specific sounds for larger compositions. This theme emphasized creativity and innovation in music technology during the event."
"What is the context window size for Gemini 1.5 Pro, and how does it compare to previous versions?","I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM.
Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.","The context window size for Gemini 1.5 Pro is 1 million tokens, which has been expanded to 2 million tokens for developers in private preview. This represents an increase from previous versions, allowing for greater capacity and the ability to handle more extensive inputs compared to earlier iterations of the Gemini models."
"What are the two main technical advances highlighted in the presentation, and how do they enhance Gemini's capabilities?","So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example. My sister is a self-employed photographer, and her inbox is full of appointment bookings, receipts, client feedback on photos, and so much more. Now, if you're a freelancer or a small business, you really want to focus on your craft and not on bookkeeping and logistics. So, let's go to her inbox and take a look. Lots of unread emails. Let's click on the first one. It's got a PDF. That's an attachment from a hotel as a receipt, and I see a suggestion in the side panel. Help me organize and track my receipts. Let's click on this prompt. The side panel now will show me more details about what that really means. And as you can see, there's two steps here. Step one. Create a drive folder and put this receipt and 37 others it's found into that folder. Make sense. Step two. Extract the relevant information from those receipts in that folder into a new spreadsheet. Now, this sounds useful. Why not? I also have the option to edit these actions, or just hit OK. So, let's hit OK. Gemini will now complete the two steps described above, and this is where it gets even better. Gemini offers you the option to automate this so that this particular workflow is run on all future emails, keeping your drive folder and expense sheet up to date with no effort from you. Now, we know that creating a complex spreadsheet like this can be daunting for most people. But with this automation, Gemini does the hard work of extracting all the right information from all the files and in that folder and generates this sheet for you. So, let's take a look. OK, it's super well organized and it even has a category for expense type. Now, we have the sheet. Things can get even more fun. We can ask Gemini questions, questions like show me where the money spent. Gemini not only analyzes the data from the sheet, but also creates a nice visual to help me see the complete breakdown by category. You can imagine how this extends to all sorts of use cases in your inbox like travel expenses, shopping, remodeling projects, you name it. All of that information in Gmail can be put to good use and help you work, plan, and play better. Now, this particular... I know. This particular ability to organize your attachments and drive and generate a sheet and do data analysis via Q&A will be rolling out to labs users this September. And it's just one of the many automations that we're working on in workspace. Workspace in the Gemini era will continue to unlock new ways of getting things done. We're building advanced agent of experiences, including customizing how you use Gemini. Now, as we look to 2025 and beyond, we're exploring entirely new ways of working with AI. Now, with Gemini, you have an AI-powered assistant always at your side. But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs, or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and context to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives.
Gemini understood the back and forth in that thread and that Jeff was ready to start so offers me a few customized options based on that context. So here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay so one of the really neat things about Workspace Apps like Gmail, Drive, Docs, Calendar is how well they work together. In our daily lives we often have information that flows from one app to another like say adding a calendar entry from Gmail or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely. So let me show you what I mean with a real life example. My sister is a self-employed photographer and her inbox is full of appointment bookings, feeds, client feedback on photos and so much more. Now if you're a freelancer or a small business you really want to focus on your craft and not on bookkeeping and logistics. So let's go to her inbox and take a look. Lots of unread emails, let's click on the first one. It's got a PDF, that's an attachment from a hotel as a receipt and I see a suggestion in the side panel. Talk me, organize and track my receipts. Let's click on this prompt. The side panel now will show me more detailed about what that really means. And as you can see there's two steps here. Step one, create a drive folder and put this receipt and 37 others it's found into that folder. Make sense. Step two, extract the relevant information from those receipts in that folder into a new spreadsheet. Now this sounds useful, why not. I also have the option to edit these actions or just hit OK. So let's hit OK. Gemini will now complete the two steps described above and this is where it gets even better. Gemini offers you the option to automate this so that this particular workflow is run on all future emails, keeping your drive folder and expand sheet up to date with no effort from you. Now we know that creating a complex spreadsheet like this can be daunting for most people. But with this automation Gemini does the hard work of extracting all the right information from all the files and in that folder and generates this sheet for you. So let's take a look. OK, it's super well organized and it even has a category for expense type. Now we have the sheet, things can get even more fun. We can ask Gemini questions, questions like, show me where the money is spent. Gemini not only analyzes the data from the sheet but also creates a nice visual to help me see the complete breakdown by category. And you can imagine how this extends to all sorts of use cases in your inbox like travel expenses, shopping, remodeling projects, you name it. All of that information in Gmail can be put to good use and help you work, plan and play better. Now this particular, I know. This particular ability to organize your attachments in drive and generate a sheet and do data analysis via Q&A will be rolling out to labs users this September. And it's just one of the many automations that we're working on in workspace. Workspace in the Gemini era will continue to unlock new ways of getting things done. We're building advanced agent of experiences including customizing how you use Gemini. Now as we look to 2025 and beyond, we're exploring entirely new ways of working with AI. Now with Gemini you have an AI power dissistant always at your side. But what if you could expand how you interact with AI? For example, when we work with other people, we mention them in comments and docs or we send them emails, we have group chats with them, etc. And it's not just how we collaborate with each other, but we each have a specific role to play in the team. And as the team works together, we build a set of collective experiences and contacts to learn from each other. We have the combined set of skills to draw from when we need help. So how could we introduce AI into this mix and build on this shared expertise? Well, here's one way. We're prototyping a virtual Gemini-powered teammate. This teammate has an identity, a workspace account, along with the specific role and objectives. Let me bring Tony up to show you what I mean. Hey, Tony. Hey, partner.","The two main technical advances highlighted in the presentation regarding Gemini's capabilities are:

1. **Automation of Workflows**: Gemini introduces the ability to automate tasks related to managing emails and organizing attachments. For example, it can automatically create a Drive folder for receipts and extract relevant information into a spreadsheet. This automation allows users, especially freelancers and small businesses, to focus on their core work rather than on bookkeeping and logistics. By automating these workflows, Gemini keeps the user's Drive folder and expense sheet up to date with minimal effort.

2. **Data Analysis and Visualization**: Gemini enhances its capabilities by allowing users to ask questions about the data in their spreadsheets and providing visual representations of that data. For instance, users can inquire about their spending and receive a breakdown by category, which helps in better understanding and managing expenses. This feature not only simplifies data analysis but also makes it more accessible and useful for various applications, such as travel expenses or project management.

Together, these advances make Gemini a powerful assistant that streamlines tasks and provides insightful analysis, ultimately improving productivity and efficiency for users."
"What is Project Astra, and what is its goal in relation to AI assistance?","Yes. Let's see what it says. Hey, my name is Michael Chang and we're watching the Google IO keynote while it's happening with Project Astra. Let's take a look. Hey, Gemini, currently watching Google IO keynote right now. You want to watch it with me? Sure, I'd love to watch it with you. What's happening in the keynote? What does it look like? What does it look like they're announcing right now? It looks like they're announcing Gemini 1.5 Flash. The speaker is talking about its speed and efficiency in comparison to the pro model. What are the other features? It looks like Gemini 1.5 Flash is optimized for tasks where low latency and high efficiency matter most. Cool. The screen also shows. Oh, yeah, sorry, I interrupted you. Keep going. That's all right. What were we talking about? Well, we were talking about the capabilities of 1.5 Flash. Do you remember? Yes, we were discussing the new Gemini model and its efficiency and speed. The presentation seems to have moved on though. What would you like to talk about now? Well, I'm curious about what he's going to talk about next. He seems to be moving on to the topic of model training and feedback. Perhaps they will talk about how user input helps train the models. It looks like he just announced a new project. What is that project called? They are calling that project, Project Astra. It looks like they are calling that project Astra. It looks like they are calling that project Astra. It looks like they are calling that project Astra. It looks to focus on the future of AI assistance. What do you think a project Astra will be helpful for? It sounds like a very ambitious project. It will be interesting to see what kind of AI assistant they create and how it integrates with our daily lives. They seem to say it's a universal agent. What would you use a universal agent for? A universal agent that can help with everyday tasks would be helpful. It could potentially be useful for things like scheduling, finding information, or even controlling smart home devices. Oh, that would be super cool. I agree it sounds amazing. Having a seamless and intuitive AI agent could revolutionize how we interact with technology. Yeah, can you summarize what he showed so far? The speaker introduced Project Astra, an AI-powered universal agent for everyday tasks that is still under development. They discussed the challenges of building this technology and how their team is working toward making Astra helpful and intuitive for users. Wow, amazing. And what did he talk about before Project Astra? He was discussing the Gemini 1.5 model, particularly Gemini 1.5 Flash, which focuses on efficiency and speed. The presentation was focused on the future direction of AI models and user feedbacks roll in training those models. It looks like they're now doing a demo of Project Astra. What does it look like they're doing right now? In the video, someone is holding a phone and panning across the table. The phone is running Project Astra, which can identify a speaker and a cup of crayons. And what is it doing now? It appears to be scanning a computer screen displaying code. Astra then explains what the highlighted section of the code is doing. Wow, that's super cool. Thanks for explaining all of these things to me, Gemini. You're very welcome. Is there anything else I can explain about the presentation? Let's just keep on watching. What's currently happening? In the video, Astra is identifying the King's Cross Neighborhood of London using the phone's camera view. It's also providing some facts about the area. Wow. I can't believe this is...this technology exists right now. It does look quite impressive. Astra seems very advanced at interpreting what it sees and providing relevant information. That was a lot of fun to watch. I'm looking forward to Project Astra as I'm sure all of you are too. Thank you for being here. Thank you for tuning in. Let's go build something amazing!
Thanks, and it's so great to be here. Ever since I was a kid playing chess for the England Junior Team, I'd been thinking about the nature of intelligence. I was captivated by the idea of a computer that could think like a person. It's ultimately why it became a programmer and studied neuroscience. I co-founded DeepMind in 2010 with the goal of one day building AGI, Artificial General Intelligence, a system that has human level cognitive capabilities. I've always believed that if we could build this technology responsibly, its impact would be truly profound and it could benefit humanity in incredible ways. Last year, we reached a milestone on that path when we formed Google DeepMind, combining AI talent from across the company into one super unit. Since then, we built AI systems that can do an amazing range of things, from turning language and vision into action for robots, navigating complex virtual-threading environments, solving a limpiav level math problems, and even discovering thousands of new materials. Just last week, we announced our next-generation alpha-fold model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. All of this was made possible with the best infrastructure for the AI era, including our highly optimized tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multi-modal because that's how we interact with and understand the world around us. We've built a variety of models for different use cases, you've seen how powerful Gemini 1.5 Pro is, but we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale while still featuring multi-modal reasoning capabilities and breakthrough-long context. Flash is optimized for tasks where low latency and efficiency matter most. Starting today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI, and developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. You'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models, but we're always striving to push the state of the art even further. At any one time, we have many different models in training, and we use our very large and powerful ones to help teach and train our production-ready models. Together with user feedback, this cutting-edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multi-modal AI. And this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multi-modal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action, and it would have to be proactive, teachable and personal, so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multi-modal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called?","Project Astra is an initiative aimed at developing a universal AI agent that can be truly helpful in everyday life. The goal of Project Astra is to create an AI system that understands and responds to the complex and dynamic world in a manner similar to humans. This agent is designed to take in and remember what it sees, understand context, and take action accordingly. It should be proactive, teachable, and personal, allowing for natural conversations without lag or delay. The project focuses on enhancing the interaction quality and speed, making it feel more conversational and intuitive for users."
"What is the significance of the new generative video model called VO, and what capabilities does it offer?","We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.
This is what excites me. As a hip-hop producer, we dug in the crates. We play these vinyls, and the part with it is no vocal, we pull it, we sample it, and we create an entire song around that. Right now we dig in an infinite crate. It's endless. Where I found the AI really useful for me, this way to fill in the sparser elements of my loops. Okay, let's try Bongo's. We're going to put Viola. We're going to put Rhythnic clapping. And we're going to see what happens there. And it makes it sound ironically at the end of the day a little more human. So then this is entirely Google's loops right here. These are glutes. It's like having like this weird friend that's just like, try this, try that. And then you're like, oh, okay, yeah, no, that's pretty dope. It's a group in, group in, group in, group in, up, up, up. The tools are capable, speeding up the process of what's in my head getting it out. You're able to move light speed with your creativity. This is amazing. Right there. I think this really shows what's possible when we work with the artist's community on the future of music. You can find some brand new songs from these acclaimed artists and songwriters on their YouTube channels now. There's one more area I'm really excited to share with you. Our teams have made some incredible progress in genitive video. Today, I'm excited to announce our newest, most capable genitive video model called VO. VO creates high quality 1080p videos from text, image and video prompts. It can capture the details of your instructions in different visual and cinematic styles. You can prompt for things like aerial shots of a landscape or time lapse and further edit your videos using additional prompts. You can use VO in our new experimental tool called Video FX. We're exploring features like storyboarding and generating longer scenes. VO gives you unprecedented creative control. Techniques for generating static images have come a long way, but generating video is a different challenge altogether. Not only is it important to understand where an object or subject should be in space, it needs to maintain this consistency over time, just like the car in this video. VO builds upon years of our pioneering genitive video model work, including GQN, FNACI, WALT, Video Poet, Lumiere and much more. We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into a video model. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it, right? As much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today.","The new generative video model called VO is significant because it represents a major advancement in the field of AI-generated video content. Developed by Google DeepMind, VO is designed to create high-quality 1080p videos from various prompts, including text, images, and existing video clips. This model builds upon years of research and development in generative video technology, incorporating techniques from previous models to enhance consistency, quality, and output resolution.

Key capabilities of VO include:

1. **High-Quality Video Generation**: VO can produce videos at 1080p resolution, ensuring that the output is visually appealing and suitable for various applications.

2. **Creative Control**: Users have unprecedented creative control over the video generation process. They can specify details such as cinematic styles, visual effects, and specific shots (e.g., aerial views or time-lapse sequences).

3. **Multi-Modal Capabilities**: By leveraging Gemini's multi-modal capabilities, VO can better understand and capture nuances from user prompts, allowing for more sophisticated and tailored video outputs.

4. **Speed and Efficiency**: The model enables users to visualize concepts and ideas much faster than traditional methods, significantly speeding up the creative process.

5. **Accessibility for Creators**: Features of VO will soon be available to select creators through an experimental tool called Video FX, which includes functionalities like storyboarding and generating longer scenes.

Overall, VO aims to democratize video creation, allowing more people to express their creativity and tell stories through video, thereby enhancing communication and understanding among individuals."
"How does Gemini Advanced enhance the user experience for trip planning, and what features does it include?","Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. Over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now Gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now based on my flight information, Gemini knows that I needed two and a half day, itinerary. You can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens.
We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.","Gemini Advanced enhances the user experience for trip planning by integrating advanced reasoning and intelligence capabilities that allow it to create personalized vacation plans. Here are the key features it includes:

1. **Dynamic Graph Creation**: Gemini gathers information from various sources, such as search results and helpful extensions like maps and Gmail, to create a dynamic graph of possible travel options. This graph takes into account the user's priorities and constraints.

2. **Personalized Itinerary**: Based on the user's input, such as flight information and preferences (e.g., interests in art and seafood), Gemini generates a tailored itinerary. It considers factors like timing and location to optimize the travel experience.

3. **Interactive Adjustments**: Users can interact with the itinerary in real-time. For example, if a user wants to change the start time of a day’s activities, Gemini can adjust the entire itinerary accordingly, moving activities around and suggesting new options.

4. **Time Efficiency**: The system significantly reduces the time and effort required for trip planning. It automates the process of checking multiple sources and figuring out schedules, which would typically take hours.

5. **User-Friendly Interface**: The personalized vacation plan is presented in a new dynamic user interface, making it easy for users to navigate and understand their travel options.

Overall, these features make Gemini Advanced a powerful tool for users looking to plan trips efficiently and effectively, adapting to their specific needs and preferences."
What is the significance of the tools mentioned in the event?,"Let's click on the first one It's got a PDF that's an attachment from a hotel as a receipt and I see a suggestion in the side panel Help me organize and track my receipts. Let's click on this prompt The side panel now show me more detailed about what that really means and as you can see There's two steps here step one Create a drive folder and put this receipt and 37 others. It's found into that folder. Make sense Step two extract the relevant information from those receipts in that folder into a new spreadsheet now This sounds useful why not I also have the option to edit these actions or just hit OK So let's hit OK Gemini will now complete the two steps described above and this is where it gets even better Gemini offers you the option to automate this so that this particular workflow is run on all future emails Keeping your drive folder an expense sheet up to date with no effort from you Now Now we know that creating a complex spreadsheet like this can be daunting for most people But with this automation Gemini does the hard work of extracting all the right information from all the files and in that folder and Generates this sheet for you. So let's take a look. Okay. It's super well organized and it even has a category for expense type now We have the sheet things can get even more fun We can ask Gemini questions questions like show me where the money spent Gemini not only analyzes the data from the sheet But also creates a nice visual to help me see the complete breakdown by category And you can imagine how this extends to all sorts of use cases in your inbox like travel expenses Shopping remodeling projects you name it all of that information and Gmail can be put to good use and help you Work plan and play better now this particular This particular ability to organize your attachments and drive and generate a sheet and do data analysis via Q&A We'll be rolling out to labs users this September and it's just one of the many automations that we're working on in workspace Work space in the Gemini era will continue to unlock new ways of getting things done We're building advanced agent of experiences including customizing how you use Gemini Now as we look to 2025 and beyond we're exploring entirely new ways of working with AI Now with Gemini you have an AI power assistant always at your side But what if you could expand how you interact with AI For example when we work with other people we mention them in comments and docs or we send them emails We have group chats with them etc And it's not just how we collaborate with each other But we each have a specific role to play in the team and as the team works together We build a set of collective experiences and contacts to learn from each other We have the combined set of skills to draw from when we need help So how could we introduce AI into this mix and build on this shared expertise Well, here's one way We're prototyping a virtual Gemini powered teammate This teammate has an identity a workspace account along with a specific role and objectives Let me bring Tony up to show you what I mean Hey Tony Hey everyone okay So let me start by showing you how we set up this virtual teammate As you can see the teammate has this very own account and we can go ahead and give it a name. We'll do something fun like Chip Chips been given a specific job role at the set of descriptions on how to be helpful for the team You see that here and some of the jobs are to monitor and track projects We've listed a few out to organize information and provide context and a few more things Now that we've configured our virtual teammate Let's go ahead and see chip in action to do that. I'll switch us over here to Google chat First when planning for an event like IO we have a ton of chat rooms for various purposes Luckily for me chip is in all of them To quickly catch up I might ask a question like Anyone know if our IO Storyboards are approved Because we've instructed chip to track this project chip searches across all the conversations and knows respond with an answer There it is simple but very helpful Now as the team adds chip to more group chats more files more email threads Chip builds a collective memory of our work together Let's look at an example to show you all switch over to a different room. How about project sapphire over here And here we are discussing a product release coming up and as usual many pieces are still in flight so I can go ahead and ask Are we on track?
Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.","The tools mentioned in the Google I/O 2024 event, particularly those related to the Gemini models, signify a major advancement in how AI can enhance productivity and collaboration. Here are some key points regarding their significance:

1. **Automation of Workflows**: The ability of Gemini to automate tasks, such as organizing receipts and generating spreadsheets, demonstrates how AI can simplify complex processes, making them more accessible to users who may find them daunting.

2. **Data Analysis and Visualization**: Gemini's capability to analyze data and create visual representations allows users to gain insights quickly and effectively. This feature can be particularly useful for managing expenses, tracking projects, and making informed decisions based on data.

3. **Enhanced Collaboration**: The introduction of a virtual Gemini-powered teammate illustrates a shift towards integrating AI into team dynamics. This virtual assistant can track projects, provide context, and facilitate communication, thereby enhancing collaboration among team members.

4. **Multi-modal Capabilities**: The advancements in Gemini's multi-modal capabilities, such as describing images for accessibility, highlight the importance of inclusivity in technology. This feature can significantly improve user experience for individuals with visual impairments.

5. **Developer Support**: The updates announced for tools like Android Studio, which leverage Gemini 1.5 Pro, indicate a commitment to empowering developers with advanced resources to build innovative applications. This support can lead to the creation of more sophisticated and user-friendly applications.

Overall, these tools signify a broader trend of utilizing AI to improve efficiency, enhance user experience, and foster collaboration in both personal and professional settings."
What does the introduction of 'Gemini' signify in the context of the event?,"We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
Chercalm7 Allow me to reuse myself My name is Sodieneres Okay When all of these tools come together, it's a powerful combination. It's amazing. That's amazing. It's an entire suite of different kinds of possibilities. Hi, I'm Gemini. What neighborhood do you think I'm in? This appears to be the campus cross area of London. Together we're creating a new era.","The introduction of 'Gemini' at the event signifies a major advancement in AI technology, specifically aimed at enhancing user interaction and productivity. Gemini is presented as a personal AI assistant that integrates Google's latest AI models, allowing users to engage with it through various modalities such as text, voice, and camera. This development reflects a shift towards more intuitive and personalized AI experiences, enabling users to customize their interactions and create ""gems"" for specific tasks or needs. Additionally, Gemini's capabilities include advanced features like live conversations and dynamic trip planning, showcasing its potential to assist users in more complex and personalized ways. Overall, Gemini represents a step forward in making AI more accessible and useful in everyday life."
In which location is the 'Gemini' character suggesting it is situated?,"We're excited about where this is headed, so stay tuned. And as Gemini and its capabilities continue to evolve, we're diligently bringing that power directly into workspace to make all our users more productive and creative, both at home and at work. And now, over to Sissy to tell you more about Gemini app. Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. And over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists, and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now, gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months, and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now, we all know that chatbots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art. My husband loves seafood. And our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now, based on my flight information, Gemini knows that I needed two and a half day, itinerary. And you can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now, on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules.
Chercalm7 Allow me to reuse myself My name is Sodieneres Okay When all of these tools come together, it's a powerful combination. It's amazing. That's amazing. It's an entire suite of different kinds of possibilities. Hi, I'm Gemini. What neighborhood do you think I'm in? This appears to be the campus cross area of London. Together we're creating a new era.",The 'Gemini' character is suggesting that it is situated in the campus cross area of London.
What overarching theme does the event convey regarding technological advancements?,"At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features are really good. They're a great way to create a new world. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. And they're really good. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways, and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AGI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensed processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Anthropic have trained their models on TPUs as well. Today we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs we are proud to offer CPUs and GPUs to support any workload. That includes the new Axion processes we announced last month. Our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge blackwell GPUs available in the market. We are fortunate to have a long standing partnership with NVIDIA, excited to bring blackwells capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better.
That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways. And thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AGI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases tenfold. Google was built for this. For 25 years, we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensor processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Amtropic have trained their models on TPUs as well. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs, we are proud to offer CPUs and GPUs to support any workload that includes the new Axion processes we announced last month our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge Blackwell GPUs available in early 2025. We are fortunate to have a long-standing partnership with NVIDIA excited to bring Blackwell's capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying the raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago we created search to help people make sense of the waves of information moving online. With each platform shift we have delivered breakthroughs to help answer your questions better. On mobile we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk.","The overarching theme conveyed by the event regarding technological advancements is the transformative power of AI and machine learning in enhancing creativity, communication, and problem-solving. The discussions emphasize how innovations, particularly in AI models and computing infrastructure, are enabling individuals to express their creativity more effectively and share their stories. Additionally, the event highlights the significant investments in advanced hardware, such as TPUs and GPUs, and the development of new technologies that improve computational efficiency. This reflects a broader vision of leveraging AI to not only advance technology but also to foster understanding and connection among people."
How does the speaker describe the combination of tools presented?,"I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. I got the news, dude. I got the news, dude. I got therada, I got theirdoooo. I got the news, dude. I got the news, dude. I got the news, dudes. I got the news, dude. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Me Organize, Since We Launched. And now, we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini-for-Works-based to the organization, and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5-Private Pro unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now, we've been trying to find a contractor to fix our roofs, and with Work Travel, I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read, and luckily for me, I can simply tap the summarized option up top and skip reading this long back and forth. Now, Gemini pulls up this helpful mobile card as an overlay, and this is where I can read a nice summary of all the salient information that I need to know. So, I see here that we have a quote from Jeff at Green Roofing, and he's ready to start. Now, I know we had other bids, and I don't remember the details. Previously, I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now, I can simply type out my question right here in the mobile card and say something like, compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox, for example, when am I shoes arriving, or what time the doors open for the next game, without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway, back to my roof. It looks like Gemini has found details that I got from two other contractors in completely different email threads, and I have this really nicely organized summary, and I can do a quick comparison. So, it seems like Jeff's quote was right in the middle, and he can start immediately, so Green Roofing it is. I'll open that last email from Jeff and confirm the project, and look at that. I see some suggested replies from Gemini. Now, what is really, really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread, and that Jeff was ready to start, so offers me a few customized options based on that context. So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example.
There's a parents group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multimodal model, we can do so much more. To show you an early demo of an audio output in notebook LM, here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son. And I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, he really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that, of course, means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes. The foundation for understanding how objects move and interact. Ah, yes. This is where multimodal really shines. Now, it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah. Can you give my son Jimmy a basketball example? Hey, Jimmy. That's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay. So first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now, what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity, or Isaac Newton, but nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format, and it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Jor. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning, and memory are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision. We are still in the early days, and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes, and a lot less fun to return them when they don't fit.","The speaker describes the combination of tools presented as a seamless and integrated experience that enhances productivity and user interaction. They highlight how tools like Gmail, Drive, Docs, and Calendar work together effectively, allowing for smooth transitions of information across applications. The speaker emphasizes the potential of Gemini to automate these processes, making tasks easier and more efficient for users. They illustrate this with examples, such as how Gemini can summarize email threads, compare bids, and generate contextual replies, thereby streamlining workflows and improving overall user experience. The speaker also mentions the multimodal capabilities of Gemini, which allow for personalized and interactive outputs, further enhancing the utility of the tools in real-life scenarios."
What is the biggest challenge faced by teachers today according to the excerpt?,"What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.
And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube. In fact, you've already seen LearnLam on stage today, when it helps to meer with his son's homework on Android. Now, let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems, which will be available in the Gemini app and web experience, including one called Learning Coach. With Learning Coach, you can get step-by-step study guidance, along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features that Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy, to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use Generative AI. We're also working directly with educators to build more helpful Generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning, and enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real, how much it is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors, and how much more is still to come. This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar.","According to the excerpt, the biggest challenge faced by teachers today is finding a way to fit everything that's required of educators within the limited timeframe they feel they have. This includes managing and meeting the needs and expectations of students, supervisors, the school, and the community."
How is generative AI being applied in education according to the document?,"What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.
And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube. In fact, you've already seen LearnLam on stage today, when it helps to meer with his son's homework on Android. Now, let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems, which will be available in the Gemini app and web experience, including one called Learning Coach. With Learning Coach, you can get step-by-step study guidance, along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features that Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy, to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use Generative AI. We're also working directly with educators to build more helpful Generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning, and enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real, how much it is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors, and how much more is still to come. This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar.","According to the document, generative AI is being applied in education in several ways:

1. **Personalized Learning**: Generative AI tools, such as LearnLM, are being used to make learning experiences more active and tailored to individual students' needs. This allows teachers to differentiate instruction better and find appropriate resources for diverse learners.

2. **Simplifying Lesson Planning**: A pilot program in Google Classroom is being developed to help educators simplify the lesson planning process. This includes discovering new ideas, unique activities, and engaging materials, enabling teachers to adjust their lessons to meet every student's needs.

3. **Interactive Learning Tools**: Generative AI is being integrated into platforms like YouTube to make educational videos more interactive. This allows students to ask clarifying questions, receive helpful explanations, and take quizzes, enhancing their learning experience.

4. **Personal AI Tutors**: The document mentions the potential for every student to have their own personal AI tutor, which could assist them in learning any topic, thereby making education more accessible.

5. **Collaborations with Educational Institutions**: Google is partnering with experts and institutions like Columbia Teachers College and Khan Academy to test and improve the capabilities of generative AI models for learning.

Overall, generative AI is seen as a transformative tool that can enhance the educational experience by making it more personalized, engaging, and efficient for both teachers and students."
What pilot program is mentioned in the excerpt and what is its purpose?,"There's a parents group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multimodal model, we can do so much more. To show you an early demo of an audio output in notebook LM, here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son. And I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, he really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that, of course, means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes. The foundation for understanding how objects move and interact. Ah, yes. This is where multimodal really shines. Now, it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah. Can you give my son Jimmy a basketball example? Hey, Jimmy. That's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay. So first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now, what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity, or Isaac Newton, but nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format, and it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Jor. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning, and memory are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision. We are still in the early days, and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes, and a lot less fun to return them when they don't fit.
Let's click on the first one It's got a PDF that's an attachment from a hotel as a receipt and I see a suggestion in the side panel Help me organize and track my receipts. Let's click on this prompt The side panel now show me more detailed about what that really means and as you can see There's two steps here step one Create a drive folder and put this receipt and 37 others. It's found into that folder. Make sense Step two extract the relevant information from those receipts in that folder into a new spreadsheet now This sounds useful why not I also have the option to edit these actions or just hit OK So let's hit OK Gemini will now complete the two steps described above and this is where it gets even better Gemini offers you the option to automate this so that this particular workflow is run on all future emails Keeping your drive folder an expense sheet up to date with no effort from you Now Now we know that creating a complex spreadsheet like this can be daunting for most people But with this automation Gemini does the hard work of extracting all the right information from all the files and in that folder and Generates this sheet for you. So let's take a look. Okay. It's super well organized and it even has a category for expense type now We have the sheet things can get even more fun We can ask Gemini questions questions like show me where the money spent Gemini not only analyzes the data from the sheet But also creates a nice visual to help me see the complete breakdown by category And you can imagine how this extends to all sorts of use cases in your inbox like travel expenses Shopping remodeling projects you name it all of that information and Gmail can be put to good use and help you Work plan and play better now this particular This particular ability to organize your attachments and drive and generate a sheet and do data analysis via Q&A We'll be rolling out to labs users this September and it's just one of the many automations that we're working on in workspace Work space in the Gemini era will continue to unlock new ways of getting things done We're building advanced agent of experiences including customizing how you use Gemini Now as we look to 2025 and beyond we're exploring entirely new ways of working with AI Now with Gemini you have an AI power assistant always at your side But what if you could expand how you interact with AI For example when we work with other people we mention them in comments and docs or we send them emails We have group chats with them etc And it's not just how we collaborate with each other But we each have a specific role to play in the team and as the team works together We build a set of collective experiences and contacts to learn from each other We have the combined set of skills to draw from when we need help So how could we introduce AI into this mix and build on this shared expertise Well, here's one way We're prototyping a virtual Gemini powered teammate This teammate has an identity a workspace account along with a specific role and objectives Let me bring Tony up to show you what I mean Hey Tony Hey everyone okay So let me start by showing you how we set up this virtual teammate As you can see the teammate has this very own account and we can go ahead and give it a name. We'll do something fun like Chip Chips been given a specific job role at the set of descriptions on how to be helpful for the team You see that here and some of the jobs are to monitor and track projects We've listed a few out to organize information and provide context and a few more things Now that we've configured our virtual teammate Let's go ahead and see chip in action to do that. I'll switch us over here to Google chat First when planning for an event like IO we have a ton of chat rooms for various purposes Luckily for me chip is in all of them To quickly catch up I might ask a question like Anyone know if our IO Storyboards are approved Because we've instructed chip to track this project chip searches across all the conversations and knows respond with an answer There it is simple but very helpful Now as the team adds chip to more group chats more files more email threads Chip builds a collective memory of our work together Let's look at an example to show you all switch over to a different room. How about project sapphire over here And here we are discussing a product release coming up and as usual many pieces are still in flight so I can go ahead and ask Are we on track?","The pilot program mentioned in the excerpt is ""Gemini 1.5 Pro,"" which is available in Workspace Labs. Its purpose is to enhance tools like notebook LM by providing features such as generating personalized audio overviews and study guides based on the materials provided. This program aims to make learning more interactive and tailored to individual needs, particularly for students, by allowing them to engage with educational content in a more dynamic way."
What is described as the 'magic' that only a teacher can provide?,"What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.
And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube. In fact, you've already seen LearnLam on stage today, when it helps to meer with his son's homework on Android. Now, let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems, which will be available in the Gemini app and web experience, including one called Learning Coach. With Learning Coach, you can get step-by-step study guidance, along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features that Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy, to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use Generative AI. We're also working directly with educators to build more helpful Generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning, and enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real, how much it is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors, and how much more is still to come. This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar.","The 'magic' that only a teacher can provide is described as the fundamental connection, the human connection between a teacher and a student, which is essential to learning."
What is the teacher's favorite part of their job as mentioned in the excerpt?,"What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.
And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube. In fact, you've already seen LearnLam on stage today, when it helps to meer with his son's homework on Android. Now, let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems, which will be available in the Gemini app and web experience, including one called Learning Coach. With Learning Coach, you can get step-by-step study guidance, along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features that Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation, or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy, to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use Generative AI. We're also working directly with educators to build more helpful Generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning, and enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real, how much it is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors, and how much more is still to come. This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar.","The teacher's favorite part of their job, as mentioned in the excerpt, is the feeling of having made something better in their classroom."
What is the significance of Gemma's tokenizer in relation to language diversity in India?,"The language is a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navrasa. Navrasa is a model that's trained for Indic languages. It's a fine tune model based on Google's Gemma. We built Navrasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. So we should use AI to make sure that no one is following and no one is using it. Today, the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of Generative AI that we want to bring to every corner of India and the entire world.
Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now since it launched less than three months ago it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, Polly Gemma, our first vision language open model and it's available right now. It's optimized for a range of image captioning visual Q&A and other image labeling tasks so go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model but it's still got a fitness size that's easy for all of you to use. So in a few weeks we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on next-gen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. To wrap up I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. Languages are a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navarasa. Navarasa is a model that's trained for Indic languages. It's a fine tuned model based on Google's Gemma. We built Navarasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. Today the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of generative AI that we want to bring to every corner of India and the entire world.","Gemma's tokenizer is significant in relation to language diversity in India because it enables the model to handle a vast array of words, symbols, and characters across multiple alphabets and language systems. Given India's linguistic diversity, where languages can change every few kilometers, a powerful tokenizer is essential for accurately processing and understanding the nuances of different languages. This capability allows Gemma to adapt effectively to projects like Navrasa, which is specifically designed for Indic languages. By utilizing this extensive vocabulary, Gemma can create culturally rooted language models that facilitate communication in native languages, thereby addressing the unique linguistic challenges present in India. The ultimate goal is to ensure that people can engage with technology in their own languages, making generative AI accessible to all corners of the country."
What is the purpose of the Navrasa model?,"The language is a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navrasa. Navrasa is a model that's trained for Indic languages. It's a fine tune model based on Google's Gemma. We built Navrasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. So we should use AI to make sure that no one is following and no one is using it. Today, the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of Generative AI that we want to bring to every corner of India and the entire world.
So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on NextGen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. APPLAUSE So wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. The language is a very trusting problem to solve, actually. And given India has a huge variety of languages and it changes every five kilometers. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navarasa. Navarasa is a model that's trained for Indic languages. It's a fine tuned model based on Google's Gemma. We built Navarasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. So we need AI to use this technology that we don't want to ask for any of these. Today, the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of generative AI that we want to bring to every corner of India and the entire world. Lifting to everything that's been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context, and agents are the cutting edge of what this technology can do. Take it to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles, and we're learning from our users, partners, and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines, and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine three NVO create more realistic imagery and videos. We must also consider how they might be misused to spread misinformation. To help last year, we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today, we're expanding Synth ID to two new modalities, text, and video.","The purpose of the Navrasa model is to create large language models that are culturally rooted, allowing people to communicate in their native languages and receive responses in those same languages. It is specifically trained for Indic languages and aims to expand access to 15 different Indic languages, ensuring that technology can understand and address the linguistic diversity present in India. The ultimate goal is to include everyone from all corners of India in this technological advancement."
What is the ultimate goal mentioned for the language model being developed?,"This size is optimized by Nvidia to run on next gen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. To wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. Languages are a very trusting problem to solve actually. And given India has a huge variety of languages and it changes every 5 kilometres. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navarasa. Navarasa is a model that's trained for Indic languages. It's a fine tune model based on Google's Gemma. We built Navarasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. We should use AI to make it a technology that no one is following and no one is using it. Today, the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of Generative AI that we want to bring to every corner of India and the entire world. Listening to everything that's been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context and agents are the cutting edge of what this technology can do. It's a technology that's taken to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here, I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like CAM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation, make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine 3NVO create more realistic imagery and videos. We must also consider how there might be misuse to spread misinformation. To help last year we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Jemeni, Imagine, Luria, and VR models.
The language is a very trusting problem to solve actually and given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navrasa. Navrasa is a model that's trained for Indic languages. It's a fine tune model based on Google's Gemma. We built Navrasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. So we should use AI to make sure that no one is following and no one is using it. Today, the language that you speak in could be the tool and the technology that you use for solving your real world problems. And that's the power of Generative AI that we want to bring to every corner of India and the entire world.","The ultimate goal mentioned for the language model being developed, specifically Navarasa, is to create a model that is culturally rooted, allowing people to communicate in their native languages and receive responses in those same languages. The aspiration is to include everyone from all corners of India and to use AI to empower individuals to solve real-world problems using their native languages."
How does the speaker envision the role of AI in solving real-world problems?,"This is the end of the video. Thank you. Listening to everything that has been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context, and agents are the cutting edge of what this technology can do, take it to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners, and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting-edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines, and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine three NVO create more realistic imagery and videos. We must also consider how they might be misused to spread misinformation. To help last year, we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today, we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Gemini, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making. And in the coming months, we'll be open sourcing Synth ID text watermarking. This will be available in our updated responsible Gerritive AI Toolkit, which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supposed to appear collaborating with Adobe, Microsoft, Startups, and many others to build that implement standards that improve the transparency of digital media. Now, let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today, our AI advances are helping to solve real-world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases, helping predict floods in more than 80 countries, and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube.
This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning, and memory are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision. We are still in the early days, and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes, and a lot less fun to return them when they don't fit. Imagine if Gemini could do all the steps for you, searching your inbox for the receipt, locating the order number from your email, filling out a return form, and even scheduling a pickup. That's much easier, right? Applause Let's take another example that's a bit more complex. Say you just moved to Chicago. You can imagine Gemini and Chrome working together to help you do a number of things to get ready, organizing, reasoning, synthesizing on your behalf. For example, you will want to explore the city and find services nearby, from dry cleaners to dog walkers. You'll have to update your new address across dozens of websites. Gemini can work across these tasks and will prompt you for more information when needed, so you're always in control. That part is really important as we prototype these experiences. We are thinking hard about how to do it in a way that's private, secure, and works for everyone. These are simple use cases, but they give you a good sense of the types of problems you want to solve. By building intelligence systems, the think ahead, reason, and plan all on your behalf. The power of Gemini with multi-modality, long context, and agents, brings us closer to our ultimate goal, making AI helpful for everyone. We see this as how we will make the most progress against our mission, organizing the world's information across every input, making it accessible via any output, and combining the world's information with the information in your world in a way that's truly useful for you. To fully realize the benefits of AI will continue to break new ground. Google DeepMind is hard at work. To share more, please welcome for the first time on the IOTage, Sir Demis.","The speaker envisions AI playing a significant role in solving real-world problems by leveraging advanced technologies to assist in various domains. They highlight several key applications:

1. **Scientific Research**: AI is helping accelerate the work of scientists globally, such as through tools like AlphaFold, which aids in addressing neglected diseases.

2. **Disaster Prediction**: AI is being utilized to predict floods in numerous countries, showcasing its potential in disaster management and response.

3. **Global Goals Tracking**: Organizations like the United Nations are using AI to track progress on the world's 17 sustainable development goals, indicating its utility in monitoring and achieving global objectives.

4. **Personalized Learning**: The introduction of AI models like LearnLam aims to enhance educational experiences by making them more personal and engaging, potentially providing every learner with a personal AI tutor.

5. **Improving Accessibility**: The speaker emphasizes the goal of making information universally accessible and useful, suggesting that AI can transform how knowledge is disseminated and utilized in learning environments.

Overall, the speaker believes that AI innovations can address complex challenges while maximizing benefits for society, emphasizing responsible development and collaboration in the ecosystem."
What is the broader impact of Generative AI as discussed in the excerpt?,"This is the end of the video. Thank you. Listening to everything that has been announced today, it's clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context, and agents are the cutting edge of what this technology can do, take it to a whole new level, it's capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners, and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting-edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines, and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine three NVO create more realistic imagery and videos. We must also consider how they might be misused to spread misinformation. To help last year, we introduced Synth ID, a tool that adds imperceptible watermarks to our AI-generated images and audio so that they're easier to identify. Today, we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Gemini, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making. And in the coming months, we'll be open sourcing Synth ID text watermarking. This will be available in our updated responsible Gerritive AI Toolkit, which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supposed to appear collaborating with Adobe, Microsoft, Startups, and many others to build that implement standards that improve the transparency of digital media. Now, let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today, our AI advances are helping to solve real-world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases, helping predict floods in more than 80 countries, and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now, Gerritive AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day. And Gerritive AI is opening up new possibilities, allowing us to ask questions like, what if everyone, everywhere, could have their own personal AI tutor on any topic? Or, what if every educator could have the role assistant in the classroom? Today, Mark's a new chapter for learning and education at Google. I'm excited to introduce LearnLam, our new family of models based on Gemini and fine-tuned for learning. LearnLam is grounded in educational research, making learning experiences more personal and engaging. And it's coming to the products you use every day, like search, Android, Gemini, and YouTube.
What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.","The broader impact of Generative AI, as discussed in the excerpt, includes several key aspects:

1. **Enhancing Learning and Education**: Generative AI is being utilized to create more personalized and engaging learning experiences. It allows for tailored educational resources that meet the diverse needs of students, thereby improving the overall quality of education.

2. **Supporting Educators**: Tools like LearnLM are designed to assist teachers in lesson planning and resource discovery, enabling them to better differentiate instruction and meet the individual needs of their students. This support can lead to more effective teaching and improved student outcomes.

3. **Addressing Real-World Problems**: The advancements in AI are being applied to solve significant global challenges, such as aiding scientists in research, predicting natural disasters, and tracking progress on sustainable development goals.

4. **Responsible AI Development**: The excerpt emphasizes the importance of building AI responsibly by addressing risks and maximizing benefits. This includes practices like red teaming to identify weaknesses in AI models and developing tools to prevent misuse, such as watermarking AI-generated content.

5. **Collaboration and Ecosystem Engagement**: There is a commitment to working with various stakeholders, including academia and industry partners, to establish standards that enhance the transparency and safety of digital media.

Overall, Generative AI is positioned as a transformative technology that can significantly improve educational practices, contribute to societal benefits, and necessitate responsible development and deployment to mitigate risks."
What are the key highlights of the event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
And this is not just the model completing the code. It's the model coming up with an entire plan and running it. And not only do I get an answer to my initial question about the data set, but the AI agent also produces a fully functional collab notebook based on your instructions. And you can share it with others, allowing for real time collaboration on code, new visualizations, and all of the results. It's available today at labs.google slash code and let us know what you think about it. Now you see why I'm so excited today. You see, we're committed to empowering you to build with genitive AI boldly and responsibly from Gemini power assistance in your favorite IDEs to easy access for our most powerful Gemini model. And it comes with flexibility to use Gemma open models. You have an opportunity to reach anyone anywhere through Google's ecosystem of devices, app experiences, and platform. We continued investing resources to give your projects a boost. Google accelerators and the Google developer portfolio is just a couple of great examples. Our accelerators have guided over 1,300 startups in 84 countries with countless success stories. And that includes 20 startups that have grown to become unicorns. You Genie AI founded by Dr. Sudeth Boyd, it's a recent graduate of our climate change accelerator. You Genie was born out of Dr. Childry's desire to use AI for good. His team helps manufacturing companies to dramatically reduce their carbon emissions. And you Genie's algorithms apply Google DeepMind research which has helped to optimize energy use in Google's own data centers. You Genie was able to streamline their model development and their analytics with accurate emissions data through Google Earth Engine. We love working alongside startups to tackle big problems and invite all of you to apply to an upcoming accelerator soon. If you have one of the 15 million Google developer profiles, you enjoy invitations for early access programs, a chance to participate in research and opportunities to attend special events. Beginning today, your profile is evolving into something even better. Introducing the Google Developer Program. Members of the program can now access new benefits at no cost and I'm going to take you through them. First, you can access Gemini for learning and searching and chatting with documentation. Second, for IDX users, we're now enabling you to go from two work stations up to five. Third, if you're in the Google Cloud Innovators community, you get credits for interactive labs on Google Cloud Skills Boost. Sign up for the Google Developer Program today or if you have an existing profile, you're all set. More benefits are coming soon so be on the lookout. That brings us to the close of today's developer keynote but we're not quite done. There's a lot more to Google I.O than just the keynotes. If you're with us in person, attend the sessions, the workshops, and demos both today and tomorrow. If you're online, you'll find all the sessions available on IO.Google starting this Thursday morning, Mountain View Time. Last year, we kicked off the series of IO connect events to bring the in-person experience from Shoreline to more places that are closer to you. Well, we heard just how much you loved them so we've got three of them lined up this year. Starting in Berlin on June 27th and then we're in Bangalore in July and Beijing in August. And I'll mention, I'll be at all of them so I hope to see you there too. If you can join us at IO connect, come to one of the many community-led IO extended event happening in your local area. Now, I like to share something really cool with you. We had Project Astra watch the keynote this morning. Yes. Let's see what it says. Hey, my name is Michael Chang and we're watching the Google IO keynote while it's happening with Project Astra. Let's take a look. Hey, Gemini, currently watching Google IO keynote right now. You want to watch it with me? Sure, I'd love to watch it with you. What's happening in the keynote? What does it look like? What does it look like they're announcing right now? It looks like they're announcing Gemini 1.5 Flash. The speaker is talking about its speed and efficiency in comparison to the pro model. What are the other features? It looks like Gemini 1.5 Flash is optimized for tasks where low latency and high efficiency matter most. Cool. The screen also shows. Oh, yeah, sorry, I interrupted you. Keep going.","The key highlights of the Google I/O 2024 event include:

1. **Developer Conference**: Google I/O is described as the biggest developer conference, featuring around 4,000 developers in attendance and a two-day program filled with sandboxes, demos, and hands-on workshops.

2. **Keynote Presentations**: Sundar Pichai kicked off the keynote, discussing Google's progress over the past year and advancements in AI, particularly through the Gemini models. The keynote attracted over a million viewers from 186 countries.

3. **New Product Announcements**: Several new products and features were announced, including:
   - **Gemini 1.5 Pro** and **Gemini 1.5 Flash**: Highlighted for their performance and capabilities.
   - **LearnAlem** and **Imagine Three**: New tools for image generation.
   - **Gemini Live**, **Asphoto**, and **Gemini Nano**: Additional features aimed at enhancing user experience.

4. **Accessibility Features**: The event showcased how Gemini's multi-modal capabilities can improve accessibility, particularly in describing images for visually impaired users.

5. **Developer Tools and Resources**: Updates to Android Studio were announced, along with new tools to help developers build applications more effectively. The Google Developer Program was introduced, offering new benefits and access to resources.

6. **Community Engagement**: The event emphasized community involvement with sessions, workshops, and demos available both in-person and online. There are also plans for IO connect events in various global locations.

7. **Project Astra**: A demonstration of Project Astra, which allows users to interact with AI during the keynote, was showcased, highlighting its capabilities in real-time engagement.

Overall, the event focused on innovation in AI, developer support, and community engagement, with a strong emphasis on building helpful products and features."
Who were the presenters at the event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The presenters at the event included:

1. **Mark Rubier** - He hosted the I-O Pre-Show and used the new DJ mode in the music FX tool for an improvisational set.
2. **Sundar Pichai** - He kicked off the Google keynote, sharing progress over the past year and advancements in Gemini models.
3. **Janine Banks** - She led the Developer X and Core team and announced updates and new tools for developers during the developer keynote.
4. **Seneca Meeks** - A software engineer on the Quantum AI team, who demonstrated a problem solved by their quantum computer.
5. **Josh Woodward** - VP of Google Labs, who discussed the Googlers-only demo slam and the experience of presenting live demos.

Additionally, there were American Sign Language interpreters who were rehearsing for the keynote."
What new features were introduced for developers at Google I/O 2024?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","At Google I/O 2024, several new features and updates were introduced for developers, including:

1. **Updates to Android Studio**: Enhancements were made to help developers leverage Gemini 1.5 Pro to improve and accelerate their development processes.

2. **Real-time Code Generation**: Developers were able to take a mock-up of a screen and generate the corresponding code in Android Studio, as well as create an interactive app in real-time.

3. **AI and ML Technologies**: New easy-to-use AI and machine learning technologies were showcased, including Multimodal Search.

4. **Developer Tools**: Tools like Compose were introduced to facilitate building applications across various form factors.

5. **Cloud Solutions**: Reliable solutions on cloud, including new summarization tools for workspace, were highlighted.

6. **Experiential Demos**: Six demos showcased projects built on the latest AI models, including Gemini's image recognition and storytelling capabilities.

7. **Quantum Computing Demonstration**: A demonstration of quantum computing capabilities was presented, showcasing the ability to solve complex problems much faster than classical computers.

8. **Googlers-only Demo Slam**: A special event where Googlers presented live demos across various labs and technologies, fostering engagement and innovation.

These features aimed to enhance the development experience and showcase the latest advancements in AI and technology at Google I/O 2024."
How many startups have been supported by Google accelerators globally?,"I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!
We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision, we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course, search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities are high bar for information quality, our decades of experience connecting you to the richness of the web, the result is a product that does the work for you. Google search is a generative AI at the scale of human curiosity and it's our most exciting chapter of search yet to tell you more. Here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. Now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind and whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. To make this possible, we're introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates studios in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning.","The provided context does not contain information regarding the number of startups supported by Google accelerators globally. Therefore, I cannot answer that question based on the given text."
What was the atmosphere of the event described as?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The atmosphere of the event was described as buzzing with excitement, particularly as attendees made their way into the Shoreline Amphitheater for Google I/O. The event was characterized as a unique opportunity for the company to come together and create a significant moment, with a vibrant vibe reminiscent of a developer festival. There was a sense of energy and enthusiasm, especially during the keynote and various demonstrations, indicating a lively and engaging environment."
What core technology is being utilized for the short film being created at Google Farms?,"Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it, right? As much as you wish. And so we've been hearing that feedback is that it allows for more... ...multinality, more iteration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want if you enter the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. Those are really enabling us to be more creative and to share that creativity with each other.
We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.","The core technology being utilized for the short film being created at Google Farms is Google DeepMind's generative video model, which has been trained to convert input text into output video."
How does the generative video model enhance the filmmaking process according to the excerpt?,"Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it, right? As much as you wish. And so we've been hearing that feedback is that it allows for more... ...multinality, more iteration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want if you enter the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. Those are really enabling us to be more creative and to share that creativity with each other.
We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.","The generative video model enhances the filmmaking process by allowing filmmakers to bring ideas to life that were previously not possible, enabling visualization on a time scale that is 10 to 100 times faster than traditional methods. It facilitates more functionality, inspiration, and improvisation, allowing creators to make mistakes faster, which is essential in the artistic process. The model captures nuances from prompts, incorporates cinematic techniques and visual effects, and provides total creative control. This democratizes filmmaking, suggesting that everyone can become a director and share their stories, ultimately fostering greater creativity and understanding among people."
What benefits does Gemini's multi-modal capabilities provide in the context of model training?,"There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son and I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, she really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that of course means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes, the foundation for understanding how objects move and interact. Ah, yes, this is where multimodal really shines. Now it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah, can you give my son Jimmy a basketball example? Hey, Jimmy, that's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay, so first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say, the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity or Isaac Newton. But nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format. And it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Shar. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead. Work across software and systems. All to get something done on your behalf. And most importantly, under your supervision. We are still in the early days and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes. And a lot less fun to return them when they don't fit. Imagine if Gemini could do all the steps for you.
There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son and I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, she really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that of course means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes, the foundation for understanding how objects move and interact. Ah, yes, this is where multimodal really shines. Now it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah, can you give my son Jimmy a basketball example? Hey, Jimmy, that's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay, so first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say, the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity or Isaac Newton. But nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format. And it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Shar. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead. Work across software and systems. All to get something done on your behalf. And most importantly, under your supervision. We are still in the early days and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes. And a lot less fun to return them when they don't fit.","Gemini's multi-modal capabilities offer several benefits in the context of model training:

1. **Personalization**: The ability to tailor outputs based on user input allows for a more engaging and relevant learning experience. For instance, it can generate content that is age-appropriate and contextually relevant, as demonstrated with the example of explaining physics concepts through basketball, which resonates with the user's interests.

2. **Interactive Learning**: Users can interact with the model in real-time, steering conversations and discussions in directions that are meaningful to them. This interactivity enhances the learning process by allowing users to ask questions and receive tailored responses.

3. **Diverse Input and Output Formats**: The model can handle various types of inputs (text, audio, etc.) and produce outputs in multiple formats. This flexibility enables the creation of rich educational materials, such as study guides, FAQs, and quizzes, which cater to different learning styles.

4. **Enhanced Engagement**: By providing lively discussions and relatable examples, the model can capture and maintain users' attention, making learning more enjoyable and effective.

5. **Efficiency in Learning**: The model can quickly synthesize information from multiple sources and present it in a coherent manner, saving time for users who might otherwise spend hours searching for and organizing educational content.

6. **Support for Complex Concepts**: The ability to connect various concepts and present them in an understandable way helps users grasp complex ideas more easily, facilitating deeper learning.

Overall, Gemini's multi-modal capabilities enhance the training process by making it more personalized, interactive, and efficient, ultimately leading to better learning outcomes."
What is the significance of storytelling as mentioned in the excerpt?,"What do you think is the biggest challenge that you face as a teacher today? Finding a way to fit everything that's required of educators in the timeframe I feel like we're given managing, meeting everyone's needs and expectations. You're own your students, your supervisors, the school, the community. Today with generative AI, the scope of the tools that are possible has increased dramatically. With LearnLM, we are applying generative AI to make learning more active and more tailored to how students learn. AI means that I can differentiate better. It allows a teacher to find that one resource and make it appropriate for other students. Through a new pilot program in Google Classroom, we're working closely with hundreds of educators to create tools that simplify the lesson planning process. We are learning how to help teachers discover new ideas and unique activities, find engaging materials and easily adjust their lessons to meet every student where they are. These tools are only going to get better, so if this is what it will do to allow us to slow down and do the job the way we want to, it's going to make for better learning experience. The fundamental connection, the human connection between a teacher and a student is essential to learning. That's the magic that only a teacher can provide. When I go home just to have that part behind me that I made something better in my classroom, that would be my favorite part.
That's just the beginning. Okay. Now you will choose the prompts and we'll leave it up to chance. As I said, everything here unscripted unplanned. I didn't know this was going to happen. What's this? I didn't even know you guys were going to be here. What are you guys doing here? Six props, bagpipes, Persian tar, jembe, dulcimer, flamenco guitar, Detroit techno. You know what those things are? You know what this is? You better. Shout them out. What do you want to hear? Flamenco guitar. Persian took bagpipes. No, no bagpipes. They said, no, get the bagpipes out of here. Okay. Persian tar, jembe, flamenco guitar. Jembe. What I'm going to do is pop these in here and then I'm going to use them as samples and I'm going to make something with this. Ideally, I want it to be simple so that I can use it as an element in like a fuller composition. Do you understand what I'm saying? It's okay if you don't. I don't. Jembe. Flamenco guitar. Go ahead, baby. Give it a little more. Persian tar. Hey. I've now muted. reacts, deltarict. Monoyo. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. I'm going to do a little bit of the same thing. Shaburam. Shaburam. Shaburam. Go go go go go go go go go. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam. Shaburam.","The excerpt emphasizes the importance of storytelling in education, highlighting that the fundamental connection between a teacher and a student is essential to learning. Storytelling serves as a means to create engaging and relatable learning experiences, allowing teachers to connect with their students on a deeper level. This connection is described as the ""magic"" that only a teacher can provide, suggesting that storytelling not only aids in conveying information but also fosters a supportive and interactive learning environment. By utilizing generative AI tools, teachers can enhance their storytelling capabilities, making lessons more tailored and effective in meeting the diverse needs of their students."
What does the speaker imply about the future of directing with the advancements in technology?,"We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it right as much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories, the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google and the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here and we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar.
Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into output video. It looks good. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it, right? As much as you wish. And so we've been hearing that feedback is that it allows for more... ...multinality, more iteration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want if you enter the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this, it's just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. Those are really enabling us to be more creative and to share that creativity with each other.","The speaker implies that with advancements in technology, particularly through tools like Google DeepMind's generative video model, the role of directing will become more accessible to everyone. They suggest that these technologies will empower more individuals to become directors, as the creative process will be enhanced, allowing for greater functionality, inspiration, and improvisation. The speaker emphasizes that storytelling is at the heart of this development, indicating that as technology evolves, it will enable more people to share their stories and creativity, ultimately fostering a deeper understanding among individuals."
What are the key highlights of the Google I/O 2024 event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
And this is not just the model completing the code. It's the model coming up with an entire plan and running it. And not only do I get an answer to my initial question about the data set, but the AI agent also produces a fully functional collab notebook based on your instructions. And you can share it with others, allowing for real time collaboration on code, new visualizations, and all of the results. It's available today at labs.google slash code and let us know what you think about it. Now you see why I'm so excited today. You see, we're committed to empowering you to build with genitive AI boldly and responsibly from Gemini power assistance in your favorite IDEs to easy access for our most powerful Gemini model. And it comes with flexibility to use Gemma open models. You have an opportunity to reach anyone anywhere through Google's ecosystem of devices, app experiences, and platform. We continued investing resources to give your projects a boost. Google accelerators and the Google developer portfolio is just a couple of great examples. Our accelerators have guided over 1,300 startups in 84 countries with countless success stories. And that includes 20 startups that have grown to become unicorns. You Genie AI founded by Dr. Sudeth Boyd, it's a recent graduate of our climate change accelerator. You Genie was born out of Dr. Childry's desire to use AI for good. His team helps manufacturing companies to dramatically reduce their carbon emissions. And you Genie's algorithms apply Google DeepMind research which has helped to optimize energy use in Google's own data centers. You Genie was able to streamline their model development and their analytics with accurate emissions data through Google Earth Engine. We love working alongside startups to tackle big problems and invite all of you to apply to an upcoming accelerator soon. If you have one of the 15 million Google developer profiles, you enjoy invitations for early access programs, a chance to participate in research and opportunities to attend special events. Beginning today, your profile is evolving into something even better. Introducing the Google Developer Program. Members of the program can now access new benefits at no cost and I'm going to take you through them. First, you can access Gemini for learning and searching and chatting with documentation. Second, for IDX users, we're now enabling you to go from two work stations up to five. Third, if you're in the Google Cloud Innovators community, you get credits for interactive labs on Google Cloud Skills Boost. Sign up for the Google Developer Program today or if you have an existing profile, you're all set. More benefits are coming soon so be on the lookout. That brings us to the close of today's developer keynote but we're not quite done. There's a lot more to Google I.O than just the keynotes. If you're with us in person, attend the sessions, the workshops, and demos both today and tomorrow. If you're online, you'll find all the sessions available on IO.Google starting this Thursday morning, Mountain View Time. Last year, we kicked off the series of IO connect events to bring the in-person experience from Shoreline to more places that are closer to you. Well, we heard just how much you loved them so we've got three of them lined up this year. Starting in Berlin on June 27th and then we're in Bangalore in July and Beijing in August. And I'll mention, I'll be at all of them so I hope to see you there too. If you can join us at IO connect, come to one of the many community-led IO extended event happening in your local area. Now, I like to share something really cool with you. We had Project Astra watch the keynote this morning. Yes. Let's see what it says. Hey, my name is Michael Chang and we're watching the Google IO keynote while it's happening with Project Astra. Let's take a look. Hey, Gemini, currently watching Google IO keynote right now. You want to watch it with me? Sure, I'd love to watch it with you. What's happening in the keynote? What does it look like? What does it look like they're announcing right now? It looks like they're announcing Gemini 1.5 Flash. The speaker is talking about its speed and efficiency in comparison to the pro model. What are the other features? It looks like Gemini 1.5 Flash is optimized for tasks where low latency and high efficiency matter most. Cool. The screen also shows. Oh, yeah, sorry, I interrupted you. Keep going.","The key highlights of the Google I/O 2024 event include:

1. **Developer Conference**: Google I/O is described as the biggest developer conference, featuring around 4,000 developers in attendance and a variety of programs over two days.

2. **Keynote Address**: Sundar Pichai kicked off the keynote by discussing Google's progress over the past year and how advancements in Gemini models are enhancing product capabilities.

3. **New Announcements**: Several new products and features were announced during the keynote, including:
   - **Gemini 1.5 Pro** and **Gemini 1.5 Flash**: These models are noted for their performance, with Gemini 1.5 Flash optimized for low latency and high efficiency.
   - **LearnAlem** and **Imagine Three**: New tools for image generation.
   - **Fio**, **VideoFX**, **Gemini Live**, **Asphoto**, and **Gemini Nano**: Additional features and products aimed at improving user experience.

4. **Accessibility Features**: The keynote highlighted the multi-modal capabilities of Gemini, particularly in enhancing accessibility features for describing images.

5. **Developer Tools and Competitions**: Updates to Android Studio were announced, along with a Gemini developer competition offering a DeLorean car as a grand prize.

6. **Google Developer Program**: Introduction of the Google Developer Program, which provides new benefits and access to tools for developers.

7. **Community Engagement**: The event included opportunities for in-person attendees to participate in sessions, workshops, and demos, as well as online access to all sessions starting the following Thursday.

8. **Project Astra**: A demonstration of Project Astra, which allows users to interact with the keynote in real-time, showcasing the capabilities of Gemini.

Overall, the event emphasized Google's commitment to innovation in AI and developer support, with a focus on collaboration and community engagement."
Who were the presenters at the Google I/O 2024 event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The presenters at the Google I/O 2024 event included:

1. **Sundar Pichai** - He kicked off the keynote, sharing progress over the past year and discussing technical advances in Gemini models.
2. **Mark Rubier** - He hosted the I-O Pre-Show and used the new DJ mode in the music FX tool.
3. **Janine Banks** - She led the Developer X and Core team and announced updates and new tools for developers during the developer keynote.
4. **Seneca Meeks** - A software engineer on the Quantum AI team, he demonstrated a problem-solving capability of their quantum computer.
5. **Josh Woodward** - VP of Google Labs, he discussed the Googlers-only demo slam and the concept behind it.

Additionally, there were various presenters involved in the demos themed by Stack across web, mobile, cloud, and AI."
What new tools were announced for developers during the event?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","During the event, several new tools were announced for developers, including updates to Android Studio that help developers leverage Gemini 1.5 Pro to improve and accelerate their development. Additionally, there was a feature that allowed developers to take a mock-up of a screen and generate the code for it in Android Studio, enabling them to create an interactive app in real-time. Other tools highlighted included easy-to-use AI ML technologies like Multimodal Search, reliable solutions on cloud, and new summarization tools on Workspace."
How many startups have been supported by Google accelerators?,"I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!
We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We will keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago, we created search to help people make sense of the waves of information moving online. With each platform shift, we have delivered breakthroughs to help answer your questions better. On mobile, we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision, we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course, search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities are high bar for information quality, our decades of experience connecting you to the richness of the web, the result is a product that does the work for you. Google search is a generative AI at the scale of human curiosity and it's our most exciting chapter of search yet to tell you more. Here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. Now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind and whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. To make this possible, we're introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates studios in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning.","The provided context does not contain information regarding the number of startups supported by Google accelerators. Therefore, I am unable to answer that question based on the given text."
What was the atmosphere of the Google I/O 2024 event described as?,"Everyone, welcome to TLDR. We're here at Shoreline Ampitheater for Google I-O. Google I-O is our biggest developer conference, and over the next two days, we'll share the latest innovations from across Google. We're here to get an up-close look at what this year's Google I-O has to offer. Let's go! Shoreline is buzzing with excitement as attendees make their way in. We ran into marketing lead Marvin Chao to tell us more about this year's event. Google I-O is such a unique opportunity to tell the entire company story. It's when everyone comes together to really do one big moment for the company. We're back to a developer festival now. We've got about 4,000 developers in the audience. We have two days of program. We have sandboxes. We've got demos. We're really bringing that vibe back of developers who are going to build this next generation of applications using AI, so it's super exciting. First up, the I-O Pre-Show hosted by musician Mark Rubier. Yeah! Google! He used the new DJ mode in our music FX tool to host an improvisational set. Complete with a giant coffee cup and rainbow robes for the crowd. We are here with Mark Rubier. First I think we've got to get dressed. Oh, you're right. We've got to get dressed. All right, much better. Can you tell us a bit about music FX DJ? It's a technology that I was just introduced to while we were preparing for this gig and basically just allows you to summon up any number of sounds, mix them together, and sometimes it'll do surprising things. After some maturing, I could see using it as like a companion. Basically how I did on stage, which is like getting it to make very specific sounds that I want but don't already have so that I can incorporate them into like bigger, fuller compositions. Time to get up, you silly little nerds! Wake up! With that, it's time for the main event, our Google keynote. Sundar kicked things off, sharing our progress over the past year, and how technical advances in Gemini models are powering new capabilities across our products. And throughout the keynote, we shared how we're building more helpful products and features with AI, including improvements across search, workspace, photos, Android, and more. More than a million people in over 186 countries tuned in live. There were a lot of new announcements in the keynote. Including... Gemini 1.5 Pro coming to Gemini Advanced, Gemini 1.5 Flash Project Vesta Trillium, which is the most performant TV to date, LearnAlem, Imagine Three Our Highest Quality Image Generation Model Yet, Fio, VideoFX, Gemini Live, Asphoto, Gemini Nano coming to talk back, just kidding. You can catch the full list on the Google Keyword blog. Behind the scenes, we saw where presenters get final touch-ups, the production team is hard at work, and the American Sign Language interpreters were rehearsing for the next keynote. We ran into VP's Dave Burke and Rosiel, who shared more about their experience on stage. We showed how Gemini with its multi-modal capability can really enhance talk back, so it's an accessibility feature in particular for describing images. You might be shopping and you get an image, and if you have a tough time being able to see the image, we'd like to be able to describe it in like ton of detail. It's actually a very simple but very powerful feature. I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development.
I would love to tell you about search of videos, and I love the feature because there's so many things that you can't capture with just a photo, right? We're definitely thinking about what it means to bring search into the real world. It was nerve-wracking doing a live demo on-state. We have all kinds of programming for developers throughout Shoreline. Breakouts and dialogue sessions, hands-on workshops, and product specialist office hours. A Gemini developer competition with a grand prize of a DeLoring car, and a Google store pop-up with devices to purchase in the entire Pixel ecosystem on display. In our developer keynote, we announced updates and new tools to help developers build the best apps across services. I'm Janine Banks, and I lead Developer X and Core. We unveiled some amazing updates to Android Studio, which helps developers to leverage Gemini 1.5 Pro to improve and accelerate their development. But what was super exciting, and I think the audience got super pumped up about it, was when they were able to take a mock-up of a screen and generate the code for it in Android Studio, and even generate an app that you could interact with in real-time. I feel proud of the team. I feel energized to go talk to our community. We don't just announce amazing innovations in AI advances. We let developers, reporters, and partners experience some of them in action for the very first time. There were 16 demos themed by Stack across web, mobile, cloud, and AI. The demo showed products and features highlighted in the developer keynote and breakout sessions. From the latest easy-to-use AI ML technologies like Multimodal Search, to Android developer tools like Compose to build across all form factors, reliable solutions on cloud, including new summarization tools on workspace, powerful web tools for Chrome, and more. In our AI sandbox, six experiential demos showcased projects built on our latest AI models. We got an up-close look at the power of Gemini's image recognition and storytelling. Imagine two generated images from the world of Alice in Wonderland and the style of five visual artists. With visual reasoning and conversation skills, Gemini 1.5 Pro analyzed accuracy, power, and style to share tips to improve your penalty kick. We got to live out our DJ dreams with music FX DJ mode. And our final stop was the Quantum Lab. Hi, my name is Seneca Meeks, and I'm a software engineer on the Quantum AI team. So here we have a miniature version of our lab in Santa Barbara, and we have scaled models of our quantum computer. Here we have software that enables us to program our quantum computer. We were able to demonstrate a problem that would have taken a classical computer a billion years to compute on its own. And we were able to do that in the magnitude of minutes. This year, we extended I.O. with a special Googlers-only demo slam. Googlers took the I.O. stage to present live demos across labs, Gemini, Search, Android, and more. Each demo had four minutes before a dinosaur warded them off the stage. We chatted with VP of Google Labs, Josh Woodward. I started to Google as an intern, where people would come up, they would plug in their laptop, and just cast things from a terminal screen. We thought, what if it'd be fun if we could do that? And then what if we could get, I don't know, 5,000, 6,000 Googlers around the I.O. village, just experiencing it? That was where the idea came from. And people said, yes, so we're just doing it. Thanks for tuning in. Chris, I have to say, this has been an eye-opening experience. AI would have to agree. Bye!","The atmosphere of the Google I/O 2024 event was described as buzzing with excitement, with attendees making their way into the Shoreline Amphitheater. It was characterized as a unique opportunity for the company to come together, resembling a developer festival with about 4,000 developers in the audience. The event featured a lively pre-show with musician Mark Rubier and included a range of programming, hands-on workshops, and a sense of energy and enthusiasm among participants. Overall, it conveyed a vibrant and engaging environment focused on innovation and collaboration in technology."
What was the initial reaction of the speaker to the announcement of the 1 million token context window?,"I remember at the announcement the 1 million token context window and my first reaction was there's no way they were able to achieve this. I wanted to test this technical skills so I uploaded a line chart. It was temperatures between Tokyo and Berlin and how they were across the 12 months of year. So I got in there and I threw in the Python library that I was really struggling with. And I just asked it a simple question. And it nailed it. It could find specific references, the comments in the code and specific requests that people had made and other issues that people had had but then suggest a fix for it that related to what I was working on. I immediately tried to kind of crash it so I took you know four or five research papers out on my desktop and it's in my mulling experience when you add so much text and then you see the kind of amount of tokens your ad is not even at half the capacity. It felt a little bit like Christmas because you saw things kind of peppered up to the top of your feet about like oh wow I built this thing or oh it's doing this and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf and I shot a video just panning my camera along the bookshelf and I fed the video into the model. It gave me the titles and authors of the books even though the authors weren't visible on those book spines and on the bookshelf there was a squirrel nutcracker sat in front of the book truncating the title you could just see the word site C and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click like this is it. I thought like I had like a super part in my hands. It was poetry it was beautiful I was so happy it just this this is going to be amazing this is this is going to help people. This is kind of where the future of language models are going. Personalize to you not because you trained it to be personal to you but personal to you because you can give it such a fast understanding of who you are.
I immediately tried to kind of crash it. So I took, you know, four or five research papers out of the desktop, and it's a mind-blowing experience when you add so much text. And then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas because you saw things kind of peppered up to the top of your feed about like, oh, wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those book spines, and on the bookshelf there was a squirrel nut cracker sat in front of the book, truncating the title. You could just see the word site C, and it still gets the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry. It was beautiful. I was so happy. It just, this is going to be amazing. This is going to help people. This is kind of where the future of language models are going. Personalize to you not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been rolling out the bookshelf. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we have talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parents group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multimodal model, we can do so much more. To show you an early demo of an audio output in notebook LM, here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.","The initial reaction of the speaker to the announcement of the 1 million token context window was skepticism. They expressed disbelief, thinking, ""there's no way they were able to achieve this."""
What experiment did the speaker conduct to test the capabilities of the model?,"Thanks, and it's so great to be here. Ever since I was a kid playing chess for the England Junior Team, I'd been thinking about the nature of intelligence. I was captivated by the idea of a computer that could think like a person. It's ultimately why it became a programmer and studied neuroscience. I co-founded DeepMind in 2010 with the goal of one day building AGI, Artificial General Intelligence, a system that has human level cognitive capabilities. I've always believed that if we could build this technology responsibly, its impact would be truly profound and it could benefit humanity in incredible ways. Last year, we reached a milestone on that path when we formed Google DeepMind, combining AI talent from across the company into one super unit. Since then, we built AI systems that can do an amazing range of things, from turning language and vision into action for robots, navigating complex virtual-threading environments, solving a limpiav level math problems, and even discovering thousands of new materials. Just last week, we announced our next-generation alpha-fold model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. All of this was made possible with the best infrastructure for the AI era, including our highly optimized tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multi-modal because that's how we interact with and understand the world around us. We've built a variety of models for different use cases, you've seen how powerful Gemini 1.5 Pro is, but we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale while still featuring multi-modal reasoning capabilities and breakthrough-long context. Flash is optimized for tasks where low latency and efficiency matter most. Starting today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI, and developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. You'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models, but we're always striving to push the state of the art even further. At any one time, we have many different models in training, and we use our very large and powerful ones to help teach and train our production-ready models. Together with user feedback, this cutting-edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multi-modal AI. And this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multi-modal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action, and it would have to be proactive, teachable and personal, so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multi-modal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called?
There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son and I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, she really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that of course means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes, the foundation for understanding how objects move and interact. Ah, yes, this is where multimodal really shines. Now it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah, can you give my son Jimmy a basketball example? Hey, Jimmy, that's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay, so first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say, the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity or Isaac Newton. But nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format. And it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Shar. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead. Work across software and systems. All to get something done on your behalf. And most importantly, under your supervision. We are still in the early days and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes. And a lot less fun to return them when they don't fit.","The speaker conducted an experiment to test the capabilities of the model by demonstrating a prototype that involved two parts captured in real-time. During the test, the speaker asked the model to identify when it saw something that makes sound. The model responded by identifying a speaker and naming a part of it, showcasing its ability to process visual and auditory information in a conversational context. This experiment highlighted the model's capacity for multi-modal understanding and interaction."
What was the outcome when the speaker tested the model with multiple research papers?,"I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.
I immediately tried to kind of crash it. So I took, you know, four or five research papers out of the desktop, and it's a mind-blowing experience when you add so much text. And then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas because you saw things kind of peppered up to the top of your feed about like, oh, wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those book spines, and on the bookshelf there was a squirrel nut cracker sat in front of the book, truncating the title. You could just see the word site C, and it still gets the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry. It was beautiful. I was so happy. It just, this is going to be amazing. This is going to help people. This is kind of where the future of language models are going. Personalize to you not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been rolling out the bookshelf. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we have talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parents group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multimodal model, we can do so much more. To show you an early demo of an audio output in notebook LM, here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.","The speaker described the experience as ""mind-blowing"" when they tested the model with multiple research papers. They noted that even after adding a significant amount of text, the number of tokens used was not even at half the model's capacity. This led to a feeling of excitement, akin to ""Christmas,"" as they observed the model's capabilities and unexpected results, such as being able to identify book titles and authors from a video of their bookshelf, even when some information was not visible. Overall, the outcome was a realization of the model's potential and a sense of happiness about its future applications."
How did the model perform when the speaker fed it a video of their bookshelf?,"I immediately tried to kind of crash it. So I took, you know, four or five research papers out of the desktop, and it's a mind-blowing experience when you add so much text. And then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas because you saw things kind of peppered up to the top of your feed about like, oh, wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those book spines, and on the bookshelf there was a squirrel nut cracker sat in front of the book, truncating the title. You could just see the word site C, and it still gets the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry. It was beautiful. I was so happy. It just, this is going to be amazing. This is going to help people. This is kind of where the future of language models are going. Personalize to you not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been rolling out the bookshelf. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we have talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parents group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multimodal model, we can do so much more. To show you an early demo of an audio output in notebook LM, here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.
I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.","The model performed exceptionally well when the speaker fed it a video of their bookshelf. It was able to identify the titles and authors of the books, even though the authors were not visible on the book spines. Additionally, it correctly identified a book despite part of the title being obscured by a squirrel nutcracker in front of it. The speaker described this experience as ""mind-blowing"" and expressed a sense of joy and amazement, indicating that the model's capabilities felt like having a ""super part"" in their hands and that it opened up almost unlimited possibilities."
What did the speaker mean by saying the model felt like a 'super part' in their hands?,"I immediately tried to kind of crash it. So I took, you know, four or five research papers out of the desktop, and it's a mind-blowing experience when you add so much text. And then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas because you saw things kind of peppered up to the top of your feed about like, oh, wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those book spines, and on the bookshelf there was a squirrel nut cracker sat in front of the book, truncating the title. You could just see the word site C, and it still gets the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry. It was beautiful. I was so happy. It just, this is going to be amazing. This is going to help people. This is kind of where the future of language models are going. Personalize to you not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been rolling out the bookshelf. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we have talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parents group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multimodal model, we can do so much more. To show you an early demo of an audio output in notebook LM, here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.
I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you.","The speaker's reference to the model feeling like a ""super part"" in their hands suggests that they perceived it as a powerful and advanced tool that significantly enhances their capabilities. It implies a sense of excitement and empowerment, as if they were holding something extraordinary that could enable them to achieve remarkable things. This metaphor indicates that the speaker felt a strong connection to the model, viewing it as a transformative technology that could facilitate creative and innovative tasks, much like having a superpower at their disposal."
What significant changes have occurred in the past year according to the document?,"Google's ambitions and artificial intelligence. Who wants a Gemini? AI is rolling out to work so it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas. And new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never to old bird-tree house. We trained for things. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started.
I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. I got the news, dude. I got the news, dude. I got therada, I got theirdoooo. I got the news, dude. I got the news, dude. I got the news, dudes. I got the news, dude. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Me Organize, Since We Launched. And now, we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini-for-Works-based to the organization, and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5-Private Pro unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now, we've been trying to find a contractor to fix our roofs, and with Work Travel, I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read, and luckily for me, I can simply tap the summarized option up top and skip reading this long back and forth. Now, Gemini pulls up this helpful mobile card as an overlay, and this is where I can read a nice summary of all the salient information that I need to know. So, I see here that we have a quote from Jeff at Green Roofing, and he's ready to start. Now, I know we had other bids, and I don't remember the details. Previously, I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now, I can simply type out my question right here in the mobile card and say something like, compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox, for example, when am I shoes arriving, or what time the doors open for the next game, without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway, back to my roof. It looks like Gemini has found details that I got from two other contractors in completely different email threads, and I have this really nicely organized summary, and I can do a quick comparison. So, it seems like Jeff's quote was right in the middle, and he can start immediately, so Green Roofing it is. I'll open that last email from Jeff and confirm the project, and look at that. I see some suggested replies from Gemini. Now, what is really, really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread, and that Jeff was ready to start, so offers me a few customized options based on that context. So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example.","According to the document, significant changes in the past year include:

1. The rollout of artificial intelligence (AI) technologies, specifically mentioning Gemini, which is changing the way people work.
2. New beginnings and the discovery of new ideas and solutions to longstanding problems.
3. An increase in productivity, as exemplified by a local business that improved its customer support team's productivity by over 30% through the use of Gemini.
4. The introduction of new capabilities in tools like Gmail, which enhance user experience by providing features such as automatic language detection, real-time captions, and a Q&A feature that simplifies information retrieval from emails.
5. The overall sentiment that a lot has happened in a short time, with more advancements expected to come."
What metaphor is used to describe the pace of change over the past months?,"Google's ambitions and artificial intelligence. Who wants a Gemini? AI is rolling out to work so it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas. And new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never to old bird-tree house. We trained for things. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started.
Our model even uses contextual factors like the time of the year. So since it's warm in Dallas, you can get rooftop patios as an idea. And it pulls everything together into a dynamic whole page experience. You'll start to see this new AI organized search results page when you look for inspiration, starting with dining and recipes, and coming to movies, music, books, hotels, shopping, and more. Today you've seen how you can bring any question to search and Google takes the work out of searching. But your questions aren't limited to words in a text box and sometimes even that picture can't tell the whole story. Earlier, Demis showed you our latest advancements in video understanding. And I'm really excited to share that soon you'll be able to ask questions with video right in Google search. Let me introduce Rose to show you this in a live demo. Thank you, Liz. I have always known a record player. And I got this one and some finals that you're still recently. But when I go to play it, this thing keeps sliding off. I have no idea how to fix it. Well, where to even start? Before, I would have pieced together a bunch of searches to try to figure this out. Like, what make is this record player? What's the model? And what does this thing actually call? But now, I can just ask of a video. So let's try it. Let's do a live demo. I'm going to take a video and ask Google why will this not stay in place? In the near instant, Google gives me an AI overview. I get some reasons this might be happening and steps I can take to troubleshoot. So it's like, first, this is called a toner. Very helpful. And it looks like it may be unbalanced and there's some really helpful steps here. And I love that because I'm new to all this. I can check out this helpful link from AudioTechnica to learn even more. So that was pretty quick. There. Let me walk you through what just happened. Thanks to your combination of our steady art speech models, our deep visual understanding, and our custom Gemini model, and our search was able to understand the question I asked out loud, break down the video frame by frame. Each frame was fed into Gemini's long context window you heard about earlier today. So this search could then pinpoint the exact make a model of my record player and make sense of the motion across frames, so identify that the toner was drifting. Search fan down and comb the web to find relevant insights from articles, forums, videos, and more. And it stitched all of this together into my AI overview. The result was music to my ears. Back to your lives. Applause Everything you saw today is just a glimpse of how we're reimagining Google Search in the Gemini era. We're taking the very best of what makes Google Google. All the reasons why billions of people turn to Google Search and have relied on us for decades. And we're bringing in the power of Gemini's agent of capabilities. So Google will do the searching, the researching, the planning, the brainstorming, and so much more. All you need to do is just ask. You'll start to see these features rolling out in search in the coming weeks. Opt into search labs to be among the first to try them out. Now let's take a look at how this all comes together in Google Search this year. I got that hush, that straight up swing. I got that, no, no, no, no, no, no, no, damn. Do my thing, do my thing. I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. You know, you know, you know, you know, you know, you know. I got the news, dude. I got the new game. Yeah, you know? Yeah, you know? I got the news, dude. I got the news, no. No, no, no, no. No, no, no, no, no. I got the news, dude. I got the news, dude.","The metaphor used to describe the pace of change over the past months is ""We filled days like they were weeks."" This suggests that a lot has happened in a short amount of time, emphasizing the rapid pace of change and development."
What does the phrase 'Cannonball!' signify in the context of the document?,"Google's ambitions and artificial intelligence. Who wants a Gemini? AI is rolling out to work so it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas. And new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never to old bird-tree house. We trained for things. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started.
I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. I got the news, dude. I got the news, dude. I got therada, I got theirdoooo. I got the news, dude. I got the news, dude. I got the news, dudes. I got the news, dude. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Me Organize, Since We Launched. And now, we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini-for-Works-based to the organization, and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5-Private Pro unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now, we've been trying to find a contractor to fix our roofs, and with Work Travel, I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read, and luckily for me, I can simply tap the summarized option up top and skip reading this long back and forth. Now, Gemini pulls up this helpful mobile card as an overlay, and this is where I can read a nice summary of all the salient information that I need to know. So, I see here that we have a quote from Jeff at Green Roofing, and he's ready to start. Now, I know we had other bids, and I don't remember the details. Previously, I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now, I can simply type out my question right here in the mobile card and say something like, compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox, for example, when am I shoes arriving, or what time the doors open for the next game, without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway, back to my roof. It looks like Gemini has found details that I got from two other contractors in completely different email threads, and I have this really nicely organized summary, and I can do a quick comparison. So, it seems like Jeff's quote was right in the middle, and he can start immediately, so Green Roofing it is. I'll open that last email from Jeff and confirm the project, and look at that. I see some suggested replies from Gemini. Now, what is really, really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread, and that Jeff was ready to start, so offers me a few customized options based on that context. So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example.","In the context of the document, the phrase 'Cannonball!' signifies a moment of enthusiasm and boldness, likely representing a leap into new experiences or challenges. It suggests a sense of excitement and readiness to embrace change, reflecting the overall theme of innovation and progress discussed in the text. The phrase captures the spirit of taking risks and diving into new opportunities, which aligns with the narrative of transformation and growth associated with Google's ambitions and advancements in artificial intelligence."
How does the document suggest the changes have impacted different groups of people?,"I just walk that, just walk now. I just do my thing. I just walk now. I just do my thing. I just walk now, do it like that. I got the news, dude. I got the news, dude. I got the news, dude. Why is the lever not moving all the way? I got the news, dude. I got the news, dude. I got the news, dude. I got therada, I got theirdoooo. I got the news, dude. I got the news, dude. I got the news, dudes. I got the news, dude. Since last May, we've been hard at work making Gemini for workspace even more helpful for businesses and consumers across the world. Tens of thousands of customers have been using Help Me Right, Help Me Visualize, Help Me Organize, Since We Launched. And now, we're really excited that the new Gemini-powered side panel will be generally available next month. One of our customers is a local favorite right here in California, sports basement. They rolled out Gemini-for-Works-based to the organization, and this has helped improve the productivity of their customer support team by more than 30%. Customers love how Gemini grows participation in meetings with automatic language detection and real-time captions now expanding to 68 languages. We are really excited about what Gemini 1.5-Private Pro unlocks for workspace and AI premiums customers. Let me start by showing you three new capabilities coming to Gmail Mobile. This is my Gmail account. Okay, there's an email up top from my husband. Help me sort out the roof repair thing please. Now, we've been trying to find a contractor to fix our roofs, and with Work Travel, I have clearly dropped the ball. It looks like there's an email thread on this with lots of emails that I haven't read, and luckily for me, I can simply tap the summarized option up top and skip reading this long back and forth. Now, Gemini pulls up this helpful mobile card as an overlay, and this is where I can read a nice summary of all the salient information that I need to know. So, I see here that we have a quote from Jeff at Green Roofing, and he's ready to start. Now, I know we had other bids, and I don't remember the details. Previously, I would have had to do a number of searches in Gmail and then remember and compare information across different emails. Now, I can simply type out my question right here in the mobile card and say something like, compare my roof repair bids by price and availability. This new Q&A feature makes it so easy to get quick answers on anything in my inbox, for example, when am I shoes arriving, or what time the doors open for the next game, without having to first search Gmail and open the email and then look for the specific information and attachments and so on. Anyway, back to my roof. It looks like Gemini has found details that I got from two other contractors in completely different email threads, and I have this really nicely organized summary, and I can do a quick comparison. So, it seems like Jeff's quote was right in the middle, and he can start immediately, so Green Roofing it is. I'll open that last email from Jeff and confirm the project, and look at that. I see some suggested replies from Gemini. Now, what is really, really neat about this evolution of smart reply is that it's contextual. Gemini understood the back and forth in that thread, and that Jeff was ready to start, so offers me a few customized options based on that context. So, you know, here I see I have declined the service, suggested new time. I'll choose proceed and confirm time. I can even see a preview of the full reply simply by long pressing. This looks reasonable, so I'll hit send. These new capabilities in Gemini and Gmail will start rolling out this month to labs users. Okay, so one of the really neat things about Workspace Apps, like Gmail, Drive, Docs, Calendar, is how well they work together. And in our daily lives, we often have information that flows from one app to another, like say adding a calendar entry from Gmail, or creating reminders from a spreadsheet tracker. But what if Gemini can make these journeys totally seamless, perhaps even automate them for you entirely? So, let me show you what I mean with a real life example.
Google's ambitions and artificial intelligence. Who wants a Gemini? AI is rolling out to work so it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas. And new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never to old bird-tree house. We trained for things. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started.","The document suggests that the changes brought about by Gemini have positively impacted various groups of people, particularly businesses and consumers. It highlights that tens of thousands of customers have been using Gemini features, which have improved productivity significantly—specifically mentioning a 30% improvement in the productivity of a customer support team at a local business. Additionally, it emphasizes that the new capabilities, such as automatic language detection and real-time captions, enhance participation in meetings for a diverse audience, expanding to 68 languages. This indicates that the changes are designed to benefit a wide range of users by making communication and information management more efficient and accessible."
What is implied by the statement 'we're just getting started'?,"Google's ambitions and artificial intelligence. Who wants a Gemini? AI is rolling out to work so it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas. And new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never to old bird-tree house. We trained for things. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started.
That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways. And thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AGI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we have made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases tenfold. Google was built for this. For 25 years, we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tensor processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Amtropic have trained their models on TPUs as well. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs, we are proud to offer CPUs and GPUs to support any workload that includes the new Axion processes we announced last month our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge Blackwell GPUs available in early 2025. We are fortunate to have a long-standing partnership with NVIDIA excited to bring Blackwell's capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying the raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We have been doing this for nearly a decade, long before it became state of the art for the industry. And today our total deployed fleet capacity for liquid cooling systems is nearly 1 gigawatt and growing. That is close to 70 times the capacity of any other fleet. Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago we created search to help people make sense of the waves of information moving online. With each platform shift we have delivered breakthroughs to help answer your questions better. On mobile we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk.","The statement ""we're just getting started"" implies that there is much more to come in terms of developments, innovations, or advancements, particularly in the context of artificial intelligence and technology. It suggests that the current progress is only the beginning of a larger journey, indicating optimism and the potential for significant future growth and breakthroughs. This phrase often conveys a sense of excitement about upcoming possibilities and the ongoing evolution of ideas and solutions."
What is the significance of the Gemini 1.5 Pro model introduced at Google I/O 2024?,"Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes so you all as developers can choose the one that works best for you. Throughout the morning you've heard a lot about our Gemini 1.5 series and I want to talk about the two models you can access today. 1.5 Pro which is getting a series of quality improvements that go out right about now and the brand new 1.5 Flash both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're a Google Cloud customer to give them a try. Now both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs and pack that massive 1 million token context window. And if you go to AI.googledev today you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API. Parallel function calling so you can return more than one function call at a time and my favorite context caching so you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now we're using Google's infrastructure to serve these models so developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens and I'm excited to share that for prompts up to 128K it'll be 50% less for $3.50 and 1.5 Flash will start at 35 cents per 1 million tokens. Now one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response and it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most and as a developer you can go try them both out today and see what works best for you. Now I'm going to show you how it works here in AI Studio the fastest way to build with Gemini and we'll pull it up here and you can see this is AI Studio. It's free to use you don't have to configure anything to get going. You just go to aisudio.google.com log in with your Google account and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt and hit run. Now in the background what it's going to do is it's going to go through that 93,000 token pile of information and you can see here start streaming it back. Now this is really helpful because it pulls out the themes for us it gives us all the right places where we can start to look and you can see this is from some of the benefits from Notebook LM like we showed earlier. Now what's great about this is that you can take something like this in AI Studio. Prototype tier in 10 seconds and with one click in the upper left get an API key or over here in the upper right just tap get code and you've got all of the model configurations the safety settings ready to go straight into your IDE. Now over time if you find that you need more enterprise grade features you can use the same Gemini 1.5 models and the same configurations right in vertex AI. That way you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash both of which are available today globally and you'll hear a lot more about them in the developer keynote later today. Now let's shift gears and talk about Gemma our family of open models which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes.
A Google we are fully in our Gemini era. Today all of our two billion user products use Gemini. Gemini 1.5 Pro is available today in Workspace Labs. Let's see how this comes to life with Google Workspace. People are always searching their emails and Gmail. We are working to make it much more powerful with Gemini. Now we can ask Gemini to summarize all recent emails from the school. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. People love using photos to search across their life. With Gemini, you're making that a whole lot easier. And ask photos can also help you search your memories in a deeper way. For example, you might be reminiscing about your daughter Lucius, early milestones. You can ask photos, show me how Lucius swimming has progressed. Here Gemini goes beyond a simple search, recognizing different contexts and photos packages it up all together in a summary. Unlocking knowledge across formats is why we build Gemini to be multimodal from the ground up. It's one model with all the modalities built in. We've been rolling out Gemini 1.5 Pro with long context and preview over the last few months. So today, we are expanding the context window to two million tokens. So far, we've talked about two technical advances, multimodality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. But what if we could go even further? That's one of the opportunities we see with AI agents. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead, work across software and systems, all to get something done on your behalf, and most importantly, under your supervision. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Here's a video of our prototype, which we'll see has two parts. Each part was captured in a single take in real time. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector, IV. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. Give me a banner name for this duo. Golden Stripes. Nice. Thanks, Gemini. Today, we're introducing Gemini 1.5 Flash. Flash is a lighter weight model compared to Pro. It's designed to be fast and cost-efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context. There's one more area I'm really excited to share with you. Our teams have made some incredible progress in genitive video. Today, I'm excited to announce our newest, most capable genitive video model, called VO. VO creates high quality 1080p videos from text, image and video prompts. It can capture the details of your instructions in different visual and cinematic styles. For 25 years, we have invested in world-class technical infrastructure. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. Google Search is a generative AI at the scale of human curiosity, and it's our most exciting chapter of Search yet. All the advancements you'll see today are made possible by a new Gemini model, customized for Google Search. What really sets this apart is our three unique strengths. This is Search in the Gemini era. By the end of the year, AI overviews will come to over a billion people. We're making AI overviews even more helpful for your most complex questions. The types that are really more like 10 questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. I'm really excited to share that soon you'll be able to ask questions with video. Why will this not stay in place? In the near-instead, Google gives me an AI overview. I guess some reasons this might be happening and steps I can take to troubleshoot.","The Gemini 1.5 Pro model introduced at Google I/O 2024 is significant for several reasons:

1. **Multimodal Capabilities**: Gemini 1.5 Pro is designed to handle multiple types of inputs, including text, images, audio, and video. This allows developers to create applications that can interleave these modalities, enhancing the versatility and functionality of AI applications.

2. **Large Context Window**: The model supports a massive context window of up to 2 million tokens, which enables it to process and understand extensive amounts of information in a single interaction. This is particularly useful for complex tasks that require a deep understanding of context.

3. **Quality Improvements**: The 1.5 Pro version includes a series of quality enhancements aimed at providing higher quality responses, making it suitable for complex tasks where accuracy and detail are paramount.

4. **Developer Features**: The model introduces new developer features such as video frame extraction, parallel function calling, and context caching. These features are designed to improve the efficiency and usability of the model for developers.

5. **Cost-Effective Pricing**: The pricing structure for using the model is competitive, with 1.5 Pro costing $7 per million tokens, and a reduced rate of $3.50 for prompts up to 128K tokens. This makes it accessible for a wide range of applications.

6. **Integration with Google Cloud**: Developers can easily integrate Gemini 1.5 Pro into their applications using Google Cloud services, allowing for scalability as enterprise needs grow.

Overall, the introduction of Gemini 1.5 Pro marks a significant advancement in AI capabilities, providing developers with powerful tools to create innovative applications that leverage multimodal AI technology."
How many developers are currently using Gemini models across Google's tools?,"Welcome to the 16th Google I.O. I want to thank our fans, have developed our community all millions of you around the world for choosing to build with Google. Literally, whatever you can imagine, you can bring it to life because only Google's ecosystem gives you the potential to reach people on 3 billion Android devices and 2 billion Chrome and Chromium-based browsers. You've created millions of helpful apps with Firebase, Google Cloud, and generative AI models like Gemini and Gemma. At Google, we're on a mission to make generative AI accessible to every developer on the planet. It's transforming the very fundamentals of software development, including new techniques for how you build and new inspiration for what you build. AI helps you to be more productive by assisting you with many different development tasks, such as writing, debugging, and testing your code, or generating documentation, and understanding your code base. That's why Gemini is available to you, all developers to use in Android Studio, Chrome Dev Tools, Project IDX, CoLab, Beath Code, IntelliJ, and Firebase. CHEERING AND APPLAUSE Gemini is even more helpful with context such as app settings, performance data, your logs, and source code. This is just a glimpse of AI changing how we built. Now, let's go to what you built. Today, people just assumed that things will work great. Of course, any platform, any device that they use, literally anywhere they are, whether on the go, at home, or at work, in their local town, or on the other side of the world. At the same time, the tools that you use need to make that possible and they're constantly evolving. It's great to see the latest technologies come out, but that means more complexity, especially with so many different AI concepts to learn. We want to make your lives simpler. As code becomes content and coders become creators, whether you're building an AI-powered mobile app, a web app, or an interactive experience that works across all platforms, we're providing you with powerful, more integrated development tools, both from Google and the OpenEka system, and it comes with secure, easy access to Google's most capable AI models and global cloud infrastructure. Ultimately, it's about helping you to stay focused on what you want to build, amazing products and services, and not the plumbing, which takes me to our agenda for today. Up next, you'll hear more in how you can use the Gemini API in Google AI Studio to develop engaging and multimodal apps. Then we will take you through how Gemini helps you to create a whole new category of AI-powered experiences on Android and Web, with exceptional levels of productivity. We've also been hard at work creating the very best experienced, full-stack, multi-platform development. Our team will share what's now possible with Project IDX, Flutter, and Firebase. And we think that will dramatically expand what you can build. Then we will talk about how you can build and customize your own AI models with Gemma. You'll see how CoLab, Carrison Jacks, make it really easy. Finally, I'll be back up here to share one more thing before we wrap it up. Are you ready to jump in? OK. Awesome. Jacqueline, come join us. Our investment in our AI research and infrastructure allows us to put the power of AI directly into your hands. We make it easy to start building an AI app with a simple API and a tool that can be used to create a new product. It can be used to create an AI app with a simple API integration. So you can focus on building the best possible products for your users. Now, meeting many of you at Hackathons and Meetups, I am blown away by how you're pushing the boundaries with our new AI models. From new ideas using audio and video inputs, the creative use of longer context windows. It's really inspiring to see. And like you heard about earlier today, balance across quality, cost, and speed. That's why I'm thrilled that Gemini 1.5 Flash is officially open to all developers to start building with today. You can get started using the Gemini API in Google AI Studio, which is now available in more than 200 countries and territories. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. Woo-hoo. But what I'm seeing more and more these days is that it isn't just about selecting the right model, but also the right techniques to bring your ideas to life.
Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in monks than is happened in years. Three eggs. Things got bigger. Like way bigger. And it wasn't all just for him or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. So, we are in the very early days of the AI platform shift. We see so much opportunity ahead for creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too, including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries, as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. Applause There's so much innovation happening in search. Thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.",More than 1.5 million developers are currently using Gemini models across Google's tools.
What are the new features introduced in Google Photos with the integration of Gemini?,"One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. The so much innovation happening in search, thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago. Since then, people have used it to organize their most important memories. Today, that amounts to more than 6 billion photos and videos uploaded every single day. And people are using photos to search across their life. With Gemini, you're making that a whole lot easier. Say you're at a parking station ready to pay, but you can't recall your license plate number. Before you could search photos for keywords, then scroll through years worth of photos looking for the right one. Now, you can simply ask photos. It knows the cards that appear often, it triangulates which one is yours, and just tells you the license plate number. And ask photos can also help you search your memories in a deeper way. For example, you might be reminiscing about your daughter Lucius early milestones. You can ask photos when did Lucius learn to swim. You can even fall up with something more complex. Show me how Lucius swimming has progressed. Here, Gemini goes beyond a simple search, recognizing different contexts from doing laps in the pool, to snorkeling in the ocean, to the texture and dates on her swimming certificates, and photos packages it up all together in a summary. You can really take it all in and relive amazing memories all over again. We are rolling out asked photos this summer with more capabilities to come. APPLAUSE Unlocking knowledge across formats is why we build Gemini to be multi-model from the ground up. It's one model with all the modalities built in. So not only does it understand each type of input, it finds connections between them. Multi-modality radically expands the questions we can ask, and the answers we will get back. Long context takes this a step further, enabling us to bring in even more information, hundreds of pages of text, hars of audio, a full hour of video, or entire code repos. Or if you want, roughly 96 cheesecake factory menus. For that many menus, you need a one-million token context window, now possible with Gemini 1.5 Pro. Developers have been using it in super interesting ways. Let's take a look. I remember the announcement, the one-million token context window, and my first reaction was, there's no way they were able to achieve this. I wanted to test this technical skills. So I uploaded a line chart. It was temperatures between Tokyo and Berlin, and how they were across the 12 months of the year. So I got in there, and I threw in the Python library that I was really struggling with. And I just asked it a simple question. And it nailed it. It could find specific references, the comments in the code, and specific requests that people had made, and other issues that people had had, but then suggest a fix for it, that related to what I was working on. I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model.
Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Oops! Reacts. Things got bigger. Like way bigger. And it wasn't all just for him, or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. Still, we are in the very early days of the AI platform shift. We see so much opportunity ahead. For creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too. Including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. The so much innovation happening in search, thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago.","The new features introduced in Google Photos with the integration of Gemini include:

1. **Enhanced Search Capabilities**: Users can now search for specific items, such as their license plate number, by simply asking the app instead of scrolling through years of photos. Gemini recognizes frequently appearing cars and can provide the license plate number directly.

2. **Contextual Memory Search**: Users can ask more complex questions about their memories, such as when a specific event occurred (e.g., ""When did Lucius learn to swim?"") and receive detailed contextual information. Gemini can recognize different contexts related to the query, such as various swimming activities and associated dates.

3. **Summarization of Memories**: Gemini can package together related memories and present them in a summary format, allowing users to relive their experiences more comprehensively.

These features leverage Gemini's multi-modal capabilities to create a more powerful and intuitive search experience within Google Photos."
What is the purpose of the Gemini Advanced subscription service announced at the event?,"Our vision for the Gemini app is to be the most helpful personal AI assistant by giving you direct access to Google's latest AI models. Gemini can help you learn, create, code, and anything else you can imagine. Over the past year, Gemini has put Google's AI in the hands of millions of people with experiences designed for your phone and the web. We also launched Gemini Advanced, our premium subscription for access to the latest AI innovations from Google. Today, we'll show you how Gemini is delivering our most intelligent AI experience. Let's start with the Gemini app, which is redefining how we interact with AI. It's natively multimodal, so you can use text, voice, or your phone's camera to express yourself naturally. And this summer, you can have an in-depth conversation with Gemini using your voice. We're calling this new experience live. Using Google's latest speech models, Gemini can better understand you and answer naturally. You can even interrupt while Gemini is responding, and it will adapt to your speech patterns. And this is just the beginning. We're excited to bring the speed gains and video understanding capabilities from Project Astra to the Gemini app. When you go live, you'll be able to open your camera so Gemini can see what you see and respond to your surroundings in real time. Now, the way I use Gemini isn't the way you use Gemini. So we're rolling out a new feature that lets you customize it for your own needs and create personal experts on any topic you want. We're calling these Gems. They're really simple to set up. Just tap to create a gem, write your instructions once, and come back whenever you need it. For example, here's a gem that I created that acts as a personal writing coach. It specializes in short stories with mysterious twists and it even builds on the story drafts in my Google Drive. I call it the cliffhanger curator. Now Gems are a great time saver when you have specific ways that you want to interact with Gemini again and again. Gems will roll out in the coming months and our trusted testers are already finding so many creative ways to put them to use. They can act as your yoga bestie, your personal sous chef, a brand new calculus tutor, a peer reviewer for your code, and so much more. Next, I'll show you how Gemini is taking a step closer to being a true AI assistant by planning and taking actions for you. Now we all know that chat bots can give you ideas for your next vacation, but there's a lot more that goes into planning a great trip. It requires reasoning that considers space, time logistics, and the intelligence to prioritize and make decisions. That reasoning and intelligence all come together in the new trip planning experience in Gemini advanced. Now, it all starts with a prompt. Okay, so here we go. We're going to Miami. My son loves art, my husband loves seafood, and our flight and hotel details are already in my Gmail inbox. Now, there's a lot going on in that prompt. Everyone has their own things that they want to do. To make sense of these variables, Gemini starts by gathering all kinds of information from search and helpful extensions like maps and Gmail. It uses that data to create a dynamic graph of possible travel options, taking into account all of my priorities and constraints. The end result is a personalized vacation plan presented in Gemini's new dynamic UI. Now based on my flight information, Gemini knows that I needed two and a half day, itinerary. You can see how Gemini uses spatial data to make decisions. Our flight lands in the late afternoon, so Gemini skips a big activity that day and finds a highly rated seafood restaurant close to our hotel. Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens.
Now on Sunday, we have a jam packed day. I like these recommendations, but my family likes to sleep in. So I tap to change the start time. And just like that, Gemini adjusted my intunary for the rest of the trip. It moved our walking tour to the next day and added lunch options near the street art museum to make the most of our Sunday afternoon. This looks great. It would have taken me hours of work checking multiple sources, figuring out schedules, and Gemini did this in a fraction of the time. This new trip planning experience will be rolling out to Gemini Advanced this summer, just in time to help you plan your own Labor Day weekend. Alright, we saved the best for last. You heard Sundar say earlier that starting today, Gemini Advanced subscribers get access to Gemini 1.5 pro with 1 million tokens. That is the longest context window of any chatbot in the world. It unlocks incredible new potential in AI, so you can tackle complex problems that were previously unimaginable. You can upload a PDF up to 1,500 pages long or multiple files to get insights across a project. And soon you can upload as much as 30,000 lines of code or even an hour long video. Gemini Advanced is the only chatbot that lets you process this amount of information. Now just imagine how useful this will be for students. Let's say you've spent months on your thesis and you could really use a fresh perspective. You can upload your entire thesis, your sources, your notes, your research, and soon interview audio recordings and videos too. So Gemini has all of this context to give you actionable advice. It can dissect your main points, identify improvements, and even roleplay as your professor. So you can feel confident in your work. And check out what Gemini Advanced can do with your spreadsheets, with the new data analysis feature launching in the coming weeks. Maybe you have a side hustle selling handcrafted products. But you're a better artist than accountant, and it's really hard to understand which products are worth your time. Simply upload all of your spreadsheets and ask Gemini to visualize your earnings and help you understand your profit. Gemini goes to work, calculating your returns, and pulling its analysis together into a single chart so you can easily understand which products are really paying off. Now behind the scenes, Gemini writes custom Python code to crunch these numbers. And of course your files are not used to train our models. Oh, and just one more thing. Later this year, we'll be doubling the long context window to two million tokens. We absolutely can't wait for you to try all of this for yourself. Gemini is continuing to evolve and improve at a breakthrough pace. We're making Gemini more multimodal, more agentive, and more intelligent, with the capacity to process the most information of any chatbot in the world. And as you heard earlier, we're also expanding Gemini Advanced to over 35 supported languages available today. But of course, what makes Gemini so compelling is how easy it is to do just about anything you want with a simple prompt. Let's take a look. Enter a prompt here. Okay, it can be that hard. How about generate an image of a cat playing guitar? Is that how it works? Am I doing AI? Yeah, just does whatever you type. What a last minute gift ideas you can make with arts and crafts. Plan a workout routine to get bigger calves. Help me think of titles my tell all men more. What's something smart I can say about Renoir? Generate another image of a cat playing guitar. If a girl calls me a snack, how do I reply? Yeah, that's how it works. You're doing AI. Make this email somewhere professional before it hits. What's a good excuse to cancel dinner with my friends? They're literally sitting right here. You know I can do other stuff, right?","The purpose of the Gemini Advanced subscription service announced at the event is to provide users with access to the latest AI innovations from Google, specifically through the Gemini app. It offers enhanced features such as a longer context window for processing information, the ability to upload large documents and files (including PDFs, spreadsheets, and audio recordings), and advanced capabilities for personalized assistance, trip planning, and data analysis. Gemini Advanced aims to make the AI experience more intelligent, multimodal, and capable of handling complex tasks, thereby improving productivity and providing actionable insights for users."
What advancements in AI technology were highlighted in relation to Google Search during the event?,"Applause And relying this is the sheer scale of our network, which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago we created search to help people make sense of the waves of information moving online. With each platform shift we have delivered breakthroughs to help answer your questions better. On mobile we unlocked new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it. You can always return them later. Of course search in the Gemini era will take this to a whole new level. Combining our infrastructure strengths, the latest AI capabilities are high bar for information quality, our decades of experience connecting you to the richness of the web. The result is a product that does the work for you. Google search is generative AI at the scale of human curiosity. And it's our most exciting chapter of search yet. To tell you more here's Liz. Thanks Sundar. With each of these platform shifts we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI search will do more for you than you ever imagined. So whatever is on your mind and whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real time information with over a trillion facts about people, places and things. Second, our in parallel ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly. Complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub questions and get an AI overview in seconds. To make this possible, we're introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates studios in Boston and show you details on their inter offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there.
To tell you more, here's Liz. Thanks, Sundar. With each of these platform shifts, we haven't just adapted. We've expanded what's possible with Google search. And now with generative AI, search will do more for you than you ever imagined. So whatever is on your mind, whatever you need to get done. Just ask. And Google will do the googling for you. All the advancements you'll see today are made possible by a new Gemini model customized for Google search. What really sets this apart is our three unique strengths. First, our real-time information with over a trillion facts about people, places, and things. Second, our unparalleled ranking and quality systems trusted for decades to get you the very best of the web. And third, the power of Gemini, which unlocks new agent of capabilities, right in search. By bringing these three things all together, we're able to dramatically expand what's possible with Google search. Yet again, this is search in the Gemini era. So let's dig in. You've heard today about AI overviews and how helpful people are finding them. With AI overviews, Google does the work for you. Instead of piecing together all the information yourself, you can ask your question. And as you see here, you can get an answer instantly, complete with a range of perspectives and links to dive deeper. As Sundar shared, AI overviews will begin rolling out to everyone in the US starting today, with more countries soon. And by the end of the year, AI overviews will come to over a billion people in Google search. But this is just the first step. We're making AI overviews even more helpful for your most complex questions. The types that are really more like ten questions in one. You can ask your entire question with all its sub-questions and get an AI overview in seconds. You make this possible for introducing multi-step reasoning in Google search. So Google can do the researching for you. For example, let's say you've been trying to get into yoga and Pilates. Finding the right studio can take a lot of research. There's so many factors you need to consider. Soon you'll be able to ask search to find the best yoga Pilates to use in Boston and show you details on their inter-offers and the walking time from Beacon Hill. As you can see here, Google gets to work for you, finding the most relevant information and bringing it together into your AI overview. You get some studios with great ratings and their introductory offers. And you can see the distance for each. Like this one, it's just a ten minute walk away. Right below, you see where they're located, laid out visually. And you've got all this from just a single search. Under the hood, our custom Gemini model acts as your AI agent using what we call multi-step reasoning. It breaks your bigger question down into all its parts. And it figures out which problems it needs to solve and in what order. And thanks to our real-time info and ranking expertise, it reasons using the highest quality information out there. So since you're asking about places, it taps into Google's index of information about the real world with over 250 million places and updated in real time, including their ratings, reviews, business hours, and more. Research that might have taken you minutes or even hours Google can now do on your behalf in just seconds. Next, let me show you another way multi-step reasoning in Google search can make your life that much easier. Take planning, for example. Planning of trips and meal plans can be fun. But doing the work of actually figuring it all out, no thank you. With Gemini in search, Google does the planning with you. Planning is really hard for AI to get right. It's the type of problem that takes advance reasoning and logic. After all, if you're meal planning, you probably don't want mac and cheese for breakfast, lunch, and dinner. Okay, my tics might, but say you're looking for a bit more variety. Now, you can ask search to create a three-day meal plan for a group that's easy to prepare. And here you get a plan with a wide range of recipes from across the web. This one for overnight oats looks particularly interesting. And you can easily head over to the website to learn how to prepare them. If you want to get more veggies in, you can simply ask search to swap in a vegetarian dish. And just like that, search customizes your meal plan.","During the event, several advancements in AI technology related to Google Search were highlighted:

1. **Generative AI Search**: Google introduced generative AI capabilities that enhance the search experience, allowing it to do more for users than ever before. This includes the ability to answer complex questions and provide comprehensive information.

2. **AI Overviews**: A new feature called AI overviews was presented, which allows users to receive instant answers to their questions, complete with a range of perspectives and links for deeper exploration. This feature is designed to simplify the process of gathering information.

3. **Multi-Step Reasoning**: Google Search will utilize multi-step reasoning to break down complex questions into manageable parts. This allows the AI to conduct research on behalf of the user, providing tailored answers based on various factors.

4. **Real-Time Information**: The search capabilities are enhanced by access to real-time information, including over a trillion facts about people, places, and things. This ensures that the information provided is current and relevant.

5. **Custom Gemini Model**: The advancements are powered by a new Gemini model specifically customized for Google Search, which combines real-time information, trusted ranking systems, and AI capabilities to improve the search experience.

6. **Enhanced Planning Features**: The AI can assist with planning tasks, such as meal planning or trip planning, by generating customized plans that consider user preferences and dietary restrictions.

These advancements aim to make Google Search more intuitive and efficient, effectively allowing the AI to handle more complex queries and provide users with high-quality information quickly."